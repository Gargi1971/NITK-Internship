{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install medmnist","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:15:02.411558Z","iopub.execute_input":"2024-04-23T04:15:02.411856Z","iopub.status.idle":"2024-04-23T04:15:18.835560Z","shell.execute_reply.started":"2024-04-23T04:15:02.411828Z","shell.execute_reply":"2024-04-23T04:15:18.834557Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting medmnist\n  Downloading medmnist-3.0.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from medmnist) (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from medmnist) (2.1.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from medmnist) (1.2.2)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from medmnist) (0.22.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from medmnist) (4.66.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from medmnist) (9.5.0)\nCollecting fire (from medmnist)\n  Downloading fire-0.6.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m904.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from medmnist) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from medmnist) (0.16.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->medmnist) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->medmnist) (2.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->medmnist) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->medmnist) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->medmnist) (2023.4)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (1.11.4)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (3.2.1)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (2023.12.9)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (21.3)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->medmnist) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->medmnist) (3.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->medmnist) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->medmnist) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->medmnist) (1.12)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->medmnist) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->medmnist) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->medmnist) (2.31.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->medmnist) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->medmnist) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->medmnist) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->medmnist) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->medmnist) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->medmnist) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->medmnist) (1.3.0)\nDownloading medmnist-3.0.1-py3-none-any.whl (25 kB)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=3d7c9be34f24f52bf6fe2e20695f5f65d1655392d6bcf2ad0c08b3dc35eb0622\n  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\nSuccessfully built fire\nInstalling collected packages: fire, medmnist\nSuccessfully installed fire-0.6.0 medmnist-3.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import medmnist\nprint(medmnist.__version__)\n!python -m medmnist available","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:16:03.684105Z","iopub.execute_input":"2024-04-23T04:16:03.685014Z","iopub.status.idle":"2024-04-23T04:16:13.402356Z","shell.execute_reply.started":"2024-04-23T04:16:03.684971Z","shell.execute_reply":"2024-04-23T04:16:13.401177Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"3.0.1\nMedMNIST v3.0.1 @ https://github.com/MedMNIST/MedMNIST/\nAll available datasets:\n\tpathmnist       | PathMNIST       | Size: 28 (default), 64, 128, 224.\n\tchestmnist      | ChestMNIST      | Size: 28 (default), 64, 128, 224.\n\tdermamnist      | DermaMNIST      | Size: 28 (default), 64, 128, 224.\n\toctmnist        | OCTMNIST        | Size: 28 (default), 64, 128, 224.\n\tpneumoniamnist  | PneumoniaMNIST  | Size: 28 (default), 64, 128, 224.\n\tretinamnist     | RetinaMNIST     | Size: 28 (default), 64, 128, 224.\n\tbreastmnist     | BreastMNIST     | Size: 28 (default), 64, 128, 224.\n\tbloodmnist      | BloodMNIST      | Size: 28 (default), 64, 128, 224.\n\ttissuemnist     | TissueMNIST     | Size: 28 (default), 64, 128, 224.\n\torganamnist     | OrganAMNIST     | Size: 28 (default), 64, 128, 224.\n\torgancmnist     | OrganCMNIST     | Size: 28 (default), 64, 128, 224.\n\torgansmnist     | OrganSMNIST     | Size: 28 (default), 64, 128, 224.\n\torganmnist3d    | OrganMNIST3D    | Size: 28 (default), 64.\n\tnodulemnist3d   | NoduleMNIST3D   | Size: 28 (default), 64.\n\tadrenalmnist3d  | AdrenalMNIST3D  | Size: 28 (default), 64.\n\tfracturemnist3d | FractureMNIST3D | Size: 28 (default), 64.\n\tvesselmnist3d   | VesselMNIST3D   | Size: 28 (default), 64.\n\tsynapsemnist3d  | SynapseMNIST3D  | Size: 28 (default), 64.\n","output_type":"stream"}]},{"cell_type":"code","source":"from medmnist import OrganMNIST3D\ntrain_dataset = OrganMNIST3D(download=True, split= 'train')\ntest_dataset = OrganMNIST3D(download=True, split= 'test')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:16:21.383847Z","iopub.execute_input":"2024-04-23T04:16:21.384767Z","iopub.status.idle":"2024-04-23T04:16:27.463993Z","shell.execute_reply.started":"2024-04-23T04:16:21.384727Z","shell.execute_reply":"2024-04-23T04:16:27.462995Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading https://zenodo.org/records/10519652/files/organmnist3d.npz?download=1 to /root/.medmnist/organmnist3d.npz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 32657349/32657349 [00:02<00:00, 14935065.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using downloaded and verified file: /root/.medmnist/organmnist3d.npz\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv3d(\n            in_planes, planes, kernel_size=(3, 3, 3), stride=stride, padding=(1, 1, 1), bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=(3, 3, 3),\n                               stride=1, padding=(1, 1, 1), bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv3d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm3d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=(3, 3, 3),\n                               stride=stride, padding=(1, 1, 1), bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(planes, self.expansion *\n                               planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv3d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm3d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, in_channels=1, num_classes=2):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=(3, 3, 3),\n                               stride=1, padding=(1, 1, 1), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet18(in_channels, num_classes):\n    return ResNet(BasicBlock, [2, 2, 2, 2], in_channels=in_channels, num_classes=num_classes)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:16:40.714040Z","iopub.execute_input":"2024-04-23T04:16:40.714843Z","iopub.status.idle":"2024-04-23T04:16:40.742593Z","shell.execute_reply.started":"2024-04-23T04:16:40.714809Z","shell.execute_reply":"2024-04-23T04:16:40.741514Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import precision_score, recall_score\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResNet18(in_channels=1, num_classes=11).to(torch.float64).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to train mode\n    running_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n    for i, (inputs, labels) in enumerate (train_loader):\n        inputs, labels = inputs.to(device), labels.to(device).view(-1)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        reg = sum(torch.sum(p**2) for p in model.parameters() if p.requires_grad)\n        lambda_ = 0.0001\n        loss = criterion(outputs, labels) + (lambda_ / 2) * reg\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n\n        _, predicted = torch.max(outputs, 1)\n        total_correct += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n    epoch_loss = running_loss / len(train_dataset)\n    print(f\"Training Loss: {epoch_loss:.4f}\")\n\n    training_accuracy = total_correct / total_samples\n    print(f\"Training Accuracy: {training_accuracy:.4f}\")\n\n    # Validation loop\n    model.eval()  # Set the model to evaluation mode\n    correct = 0\n    total = 0\n    predicted_labels = []\n    true_labels = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device).view(-1)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            predicted_labels.extend(predicted.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n    val_acc = correct / total\n    print(f\"Validation Accuracy: {val_acc:.4f}\")\n\n    precision = precision_score(true_labels, predicted_labels, average='weighted')\n    recall = recall_score(true_labels, predicted_labels, average='weighted')\n    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T04:16:47.483830Z","iopub.execute_input":"2024-04-23T04:16:47.484579Z","iopub.status.idle":"2024-04-23T12:07:07.363484Z","shell.execute_reply.started":"2024-04-23T04:16:47.484547Z","shell.execute_reply":"2024-04-23T12:07:07.361622Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Epoch [1/100], Step [1/31], Loss: 2.8100\nEpoch [1/100], Step [2/31], Loss: 3.6070\nEpoch [1/100], Step [3/31], Loss: 3.2076\nEpoch [1/100], Step [4/31], Loss: 3.1458\nEpoch [1/100], Step [5/31], Loss: 2.2505\nEpoch [1/100], Step [6/31], Loss: 2.2619\nEpoch [1/100], Step [7/31], Loss: 2.1963\nEpoch [1/100], Step [8/31], Loss: 2.9960\nEpoch [1/100], Step [9/31], Loss: 2.1459\nEpoch [1/100], Step [10/31], Loss: 2.1149\nEpoch [1/100], Step [11/31], Loss: 2.6054\nEpoch [1/100], Step [12/31], Loss: 2.0966\nEpoch [1/100], Step [13/31], Loss: 1.9917\nEpoch [1/100], Step [14/31], Loss: 2.2189\nEpoch [1/100], Step [15/31], Loss: 2.1087\nEpoch [1/100], Step [16/31], Loss: 1.9558\nEpoch [1/100], Step [17/31], Loss: 1.9202\nEpoch [1/100], Step [18/31], Loss: 2.0233\nEpoch [1/100], Step [19/31], Loss: 1.9167\nEpoch [1/100], Step [20/31], Loss: 2.1838\nEpoch [1/100], Step [21/31], Loss: 1.5837\nEpoch [1/100], Step [22/31], Loss: 1.9222\nEpoch [1/100], Step [23/31], Loss: 1.7945\nEpoch [1/100], Step [24/31], Loss: 1.9623\nEpoch [1/100], Step [25/31], Loss: 1.5530\nEpoch [1/100], Step [26/31], Loss: 1.7991\nEpoch [1/100], Step [27/31], Loss: 1.8457\nEpoch [1/100], Step [28/31], Loss: 1.6758\nEpoch [1/100], Step [29/31], Loss: 1.7709\nEpoch [1/100], Step [30/31], Loss: 1.7729\nEpoch [1/100], Step [31/31], Loss: 1.3784\nTraining Loss: 2.1721\nTraining Accuracy: 0.3512\nValidation Accuracy: 0.2770\nPrecision: 0.2544, Recall: 0.2770\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/100], Step [1/31], Loss: 1.7218\nEpoch [2/100], Step [2/31], Loss: 1.5675\nEpoch [2/100], Step [3/31], Loss: 1.4012\nEpoch [2/100], Step [4/31], Loss: 1.8041\nEpoch [2/100], Step [5/31], Loss: 1.5233\nEpoch [2/100], Step [6/31], Loss: 1.5590\nEpoch [2/100], Step [7/31], Loss: 1.5105\nEpoch [2/100], Step [8/31], Loss: 1.5775\nEpoch [2/100], Step [9/31], Loss: 1.3386\nEpoch [2/100], Step [10/31], Loss: 1.4058\nEpoch [2/100], Step [11/31], Loss: 1.5686\nEpoch [2/100], Step [12/31], Loss: 1.6748\nEpoch [2/100], Step [13/31], Loss: 1.3712\nEpoch [2/100], Step [14/31], Loss: 1.3157\nEpoch [2/100], Step [15/31], Loss: 1.7295\nEpoch [2/100], Step [16/31], Loss: 2.3807\nEpoch [2/100], Step [17/31], Loss: 1.4317\nEpoch [2/100], Step [18/31], Loss: 1.1741\nEpoch [2/100], Step [19/31], Loss: 1.2451\nEpoch [2/100], Step [20/31], Loss: 1.3562\nEpoch [2/100], Step [21/31], Loss: 1.4888\nEpoch [2/100], Step [22/31], Loss: 1.0929\nEpoch [2/100], Step [23/31], Loss: 1.2704\nEpoch [2/100], Step [24/31], Loss: 1.7535\nEpoch [2/100], Step [25/31], Loss: 1.2580\nEpoch [2/100], Step [26/31], Loss: 1.3873\nEpoch [2/100], Step [27/31], Loss: 1.0418\nEpoch [2/100], Step [28/31], Loss: 0.9863\nEpoch [2/100], Step [29/31], Loss: 1.4464\nEpoch [2/100], Step [30/31], Loss: 1.8911\nEpoch [2/100], Step [31/31], Loss: 2.0046\nTraining Loss: 1.4818\nTraining Accuracy: 0.5901\nEpoch [3/100], Step [1/31], Loss: 1.1610\nEpoch [3/100], Step [2/31], Loss: 1.3946\nEpoch [3/100], Step [3/31], Loss: 1.7509\nEpoch [3/100], Step [4/31], Loss: 1.1912\nEpoch [3/100], Step [5/31], Loss: 1.0887\nEpoch [3/100], Step [6/31], Loss: 1.4631\nEpoch [3/100], Step [7/31], Loss: 1.2377\nEpoch [3/100], Step [8/31], Loss: 1.6140\nEpoch [3/100], Step [9/31], Loss: 1.5330\nEpoch [3/100], Step [10/31], Loss: 1.0306\nEpoch [3/100], Step [11/31], Loss: 1.1568\nEpoch [3/100], Step [12/31], Loss: 1.4832\nEpoch [3/100], Step [13/31], Loss: 1.2068\nEpoch [3/100], Step [14/31], Loss: 1.2485\nEpoch [3/100], Step [15/31], Loss: 0.9199\nEpoch [3/100], Step [16/31], Loss: 1.4783\nEpoch [3/100], Step [17/31], Loss: 1.1277\nEpoch [3/100], Step [18/31], Loss: 1.4374\nEpoch [3/100], Step [19/31], Loss: 0.9898\nEpoch [3/100], Step [20/31], Loss: 1.3305\nEpoch [3/100], Step [21/31], Loss: 0.9626\nEpoch [3/100], Step [22/31], Loss: 1.2415\nEpoch [3/100], Step [23/31], Loss: 1.1681\nEpoch [3/100], Step [24/31], Loss: 0.9585\nEpoch [3/100], Step [25/31], Loss: 0.9103\nEpoch [3/100], Step [26/31], Loss: 1.0526\nEpoch [3/100], Step [27/31], Loss: 1.4253\nEpoch [3/100], Step [28/31], Loss: 0.7283\nEpoch [3/100], Step [29/31], Loss: 0.9082\nEpoch [3/100], Step [30/31], Loss: 1.1814\nEpoch [3/100], Step [31/31], Loss: 0.7763\nTraining Loss: 1.2077\nTraining Accuracy: 0.6890\nValidation Accuracy: 0.6770\nPrecision: 0.6813, Recall: 0.6770\nEpoch [4/100], Step [1/31], Loss: 0.8644\nEpoch [4/100], Step [2/31], Loss: 1.0176\nEpoch [4/100], Step [3/31], Loss: 1.0356\nEpoch [4/100], Step [4/31], Loss: 0.9175\nEpoch [4/100], Step [5/31], Loss: 1.2380\nEpoch [4/100], Step [6/31], Loss: 0.7915\nEpoch [4/100], Step [7/31], Loss: 0.9943\nEpoch [4/100], Step [8/31], Loss: 1.1526\nEpoch [4/100], Step [9/31], Loss: 1.1959\nEpoch [4/100], Step [10/31], Loss: 0.8424\nEpoch [4/100], Step [11/31], Loss: 0.8011\nEpoch [4/100], Step [12/31], Loss: 1.0375\nEpoch [4/100], Step [13/31], Loss: 0.7826\nEpoch [4/100], Step [14/31], Loss: 0.8298\nEpoch [4/100], Step [15/31], Loss: 0.6692\nEpoch [4/100], Step [16/31], Loss: 0.9798\nEpoch [4/100], Step [17/31], Loss: 1.1583\nEpoch [4/100], Step [18/31], Loss: 1.0457\nEpoch [4/100], Step [19/31], Loss: 1.5227\nEpoch [4/100], Step [20/31], Loss: 0.9666\nEpoch [4/100], Step [21/31], Loss: 0.9871\nEpoch [4/100], Step [22/31], Loss: 0.8699\nEpoch [4/100], Step [23/31], Loss: 1.1652\nEpoch [4/100], Step [24/31], Loss: 0.9157\nEpoch [4/100], Step [25/31], Loss: 0.7642\nEpoch [4/100], Step [26/31], Loss: 1.2841\nEpoch [4/100], Step [27/31], Loss: 0.8264\nEpoch [4/100], Step [28/31], Loss: 1.1381\nEpoch [4/100], Step [29/31], Loss: 0.9462\nEpoch [4/100], Step [30/31], Loss: 0.7803\nEpoch [4/100], Step [31/31], Loss: 0.6696\nTraining Loss: 0.9805\nTraining Accuracy: 0.7775\nValidation Accuracy: 0.6754\nPrecision: 0.7601, Recall: 0.6754\nEpoch [5/100], Step [1/31], Loss: 0.9585\nEpoch [5/100], Step [2/31], Loss: 1.0437\nEpoch [5/100], Step [3/31], Loss: 1.1244\nEpoch [5/100], Step [4/31], Loss: 0.6983\nEpoch [5/100], Step [5/31], Loss: 0.6258\nEpoch [5/100], Step [6/31], Loss: 0.8569\nEpoch [5/100], Step [7/31], Loss: 0.7128\nEpoch [5/100], Step [8/31], Loss: 0.7217\nEpoch [5/100], Step [9/31], Loss: 0.6447\nEpoch [5/100], Step [10/31], Loss: 0.9163\nEpoch [5/100], Step [11/31], Loss: 0.9603\nEpoch [5/100], Step [12/31], Loss: 0.6738\nEpoch [5/100], Step [13/31], Loss: 0.7202\nEpoch [5/100], Step [14/31], Loss: 0.7264\nEpoch [5/100], Step [15/31], Loss: 0.8097\nEpoch [5/100], Step [16/31], Loss: 0.8621\nEpoch [5/100], Step [17/31], Loss: 0.9218\nEpoch [5/100], Step [18/31], Loss: 0.8439\nEpoch [5/100], Step [19/31], Loss: 0.7132\nEpoch [5/100], Step [20/31], Loss: 0.9188\nEpoch [5/100], Step [21/31], Loss: 0.5524\nEpoch [5/100], Step [22/31], Loss: 0.6367\nEpoch [5/100], Step [23/31], Loss: 0.8658\nEpoch [5/100], Step [24/31], Loss: 0.6011\nEpoch [5/100], Step [25/31], Loss: 1.0053\nEpoch [5/100], Step [26/31], Loss: 0.9027\nEpoch [5/100], Step [27/31], Loss: 1.0268\nEpoch [5/100], Step [28/31], Loss: 0.7280\nEpoch [5/100], Step [29/31], Loss: 0.5548\nEpoch [5/100], Step [30/31], Loss: 1.0693\nEpoch [5/100], Step [31/31], Loss: 0.9718\nTraining Loss: 0.8150\nTraining Accuracy: 0.8527\nValidation Accuracy: 0.6754\nPrecision: 0.7396, Recall: 0.6754\nEpoch [6/100], Step [1/31], Loss: 0.9008\nEpoch [6/100], Step [2/31], Loss: 1.0483\nEpoch [6/100], Step [3/31], Loss: 0.8315\nEpoch [6/100], Step [4/31], Loss: 0.8942\nEpoch [6/100], Step [5/31], Loss: 0.9367\nEpoch [6/100], Step [6/31], Loss: 0.9773\nEpoch [6/100], Step [7/31], Loss: 0.9775\nEpoch [6/100], Step [8/31], Loss: 0.6765\nEpoch [6/100], Step [9/31], Loss: 0.6854\nEpoch [6/100], Step [10/31], Loss: 0.7197\nEpoch [6/100], Step [11/31], Loss: 0.7386\nEpoch [6/100], Step [12/31], Loss: 0.8366\nEpoch [6/100], Step [13/31], Loss: 0.8983\nEpoch [6/100], Step [14/31], Loss: 0.7130\nEpoch [6/100], Step [15/31], Loss: 1.0840\nEpoch [6/100], Step [16/31], Loss: 0.9089\nEpoch [6/100], Step [17/31], Loss: 0.9328\nEpoch [6/100], Step [18/31], Loss: 0.6841\nEpoch [6/100], Step [19/31], Loss: 0.8928\nEpoch [6/100], Step [20/31], Loss: 0.7424\nEpoch [6/100], Step [21/31], Loss: 0.6619\nEpoch [6/100], Step [22/31], Loss: 0.5963\nEpoch [6/100], Step [23/31], Loss: 0.6215\nEpoch [6/100], Step [24/31], Loss: 0.6596\nEpoch [6/100], Step [25/31], Loss: 0.7739\nEpoch [6/100], Step [26/31], Loss: 1.0542\nEpoch [6/100], Step [27/31], Loss: 1.0834\nEpoch [6/100], Step [28/31], Loss: 1.1241\nEpoch [6/100], Step [29/31], Loss: 0.5856\nEpoch [6/100], Step [30/31], Loss: 1.0212\nEpoch [6/100], Step [31/31], Loss: 1.0635\nTraining Loss: 0.8445\nTraining Accuracy: 0.8496\nValidation Accuracy: 0.8197\nPrecision: 0.8334, Recall: 0.8197\nEpoch [7/100], Step [1/31], Loss: 0.9418\nEpoch [7/100], Step [2/31], Loss: 0.5742\nEpoch [7/100], Step [3/31], Loss: 0.7017\nEpoch [7/100], Step [4/31], Loss: 0.7830\nEpoch [7/100], Step [5/31], Loss: 0.7887\nEpoch [7/100], Step [6/31], Loss: 0.8758\nEpoch [7/100], Step [7/31], Loss: 0.6450\nEpoch [7/100], Step [8/31], Loss: 0.6399\nEpoch [7/100], Step [9/31], Loss: 0.5356\nEpoch [7/100], Step [10/31], Loss: 0.6523\nEpoch [7/100], Step [11/31], Loss: 0.6015\nEpoch [7/100], Step [12/31], Loss: 0.7127\nEpoch [7/100], Step [13/31], Loss: 0.6950\nEpoch [7/100], Step [14/31], Loss: 0.8527\nEpoch [7/100], Step [15/31], Loss: 0.6705\nEpoch [7/100], Step [16/31], Loss: 0.7921\nEpoch [7/100], Step [17/31], Loss: 0.6946\nEpoch [7/100], Step [18/31], Loss: 0.8055\nEpoch [7/100], Step [19/31], Loss: 0.7593\nEpoch [7/100], Step [20/31], Loss: 0.4843\nEpoch [7/100], Step [21/31], Loss: 0.7090\nEpoch [7/100], Step [22/31], Loss: 1.0256\nEpoch [7/100], Step [23/31], Loss: 0.6097\nEpoch [7/100], Step [24/31], Loss: 0.6271\nEpoch [7/100], Step [25/31], Loss: 0.7199\nEpoch [7/100], Step [26/31], Loss: 0.6755\nEpoch [7/100], Step [27/31], Loss: 0.7583\nEpoch [7/100], Step [28/31], Loss: 0.7504\nEpoch [7/100], Step [29/31], Loss: 0.5429\nEpoch [7/100], Step [30/31], Loss: 0.7799\nEpoch [7/100], Step [31/31], Loss: 0.7724\nTraining Loss: 0.7141\nTraining Accuracy: 0.8836\nValidation Accuracy: 0.8344\nPrecision: 0.8391, Recall: 0.8344\nEpoch [8/100], Step [1/31], Loss: 0.4948\nEpoch [8/100], Step [2/31], Loss: 0.6772\nEpoch [8/100], Step [3/31], Loss: 0.5859\nEpoch [8/100], Step [4/31], Loss: 0.8423\nEpoch [8/100], Step [5/31], Loss: 0.8615\nEpoch [8/100], Step [6/31], Loss: 0.5745\nEpoch [8/100], Step [7/31], Loss: 0.6960\nEpoch [8/100], Step [8/31], Loss: 0.5208\nEpoch [8/100], Step [9/31], Loss: 0.6401\nEpoch [8/100], Step [10/31], Loss: 0.6708\nEpoch [8/100], Step [11/31], Loss: 0.7386\nEpoch [8/100], Step [12/31], Loss: 0.6547\nEpoch [8/100], Step [13/31], Loss: 0.5579\nEpoch [8/100], Step [14/31], Loss: 0.6040\nEpoch [8/100], Step [15/31], Loss: 0.5737\nEpoch [8/100], Step [16/31], Loss: 0.6564\nEpoch [8/100], Step [17/31], Loss: 0.7594\nEpoch [8/100], Step [18/31], Loss: 0.5760\nEpoch [8/100], Step [19/31], Loss: 0.5549\nEpoch [8/100], Step [20/31], Loss: 0.9148\nEpoch [8/100], Step [21/31], Loss: 0.5175\nEpoch [8/100], Step [22/31], Loss: 0.5854\nEpoch [8/100], Step [23/31], Loss: 0.7014\nEpoch [8/100], Step [24/31], Loss: 0.5261\nEpoch [8/100], Step [25/31], Loss: 0.4771\nEpoch [8/100], Step [26/31], Loss: 0.5627\nEpoch [8/100], Step [27/31], Loss: 0.5919\nEpoch [8/100], Step [28/31], Loss: 0.6359\nEpoch [8/100], Step [29/31], Loss: 0.5587\nEpoch [8/100], Step [30/31], Loss: 0.7723\nEpoch [8/100], Step [31/31], Loss: 1.2196\nTraining Loss: 0.6427\nTraining Accuracy: 0.9207\nValidation Accuracy: 0.7590\nPrecision: 0.8058, Recall: 0.7590\nEpoch [9/100], Step [1/31], Loss: 0.4922\nEpoch [9/100], Step [2/31], Loss: 0.6075\nEpoch [9/100], Step [3/31], Loss: 0.6859\nEpoch [9/100], Step [4/31], Loss: 0.6302\nEpoch [9/100], Step [5/31], Loss: 1.1852\nEpoch [9/100], Step [6/31], Loss: 0.9779\nEpoch [9/100], Step [7/31], Loss: 0.6856\nEpoch [9/100], Step [8/31], Loss: 0.6996\nEpoch [9/100], Step [9/31], Loss: 0.5936\nEpoch [9/100], Step [10/31], Loss: 0.5322\nEpoch [9/100], Step [11/31], Loss: 0.8901\nEpoch [9/100], Step [12/31], Loss: 0.6908\nEpoch [9/100], Step [13/31], Loss: 0.7834\nEpoch [9/100], Step [14/31], Loss: 0.6953\nEpoch [9/100], Step [15/31], Loss: 0.9545\nEpoch [9/100], Step [16/31], Loss: 0.5525\nEpoch [9/100], Step [17/31], Loss: 0.6364\nEpoch [9/100], Step [18/31], Loss: 0.7553\nEpoch [9/100], Step [19/31], Loss: 0.7120\nEpoch [9/100], Step [20/31], Loss: 0.8517\nEpoch [9/100], Step [21/31], Loss: 1.0974\nEpoch [9/100], Step [22/31], Loss: 0.5440\nEpoch [9/100], Step [23/31], Loss: 0.6470\nEpoch [9/100], Step [24/31], Loss: 0.5621\nEpoch [9/100], Step [25/31], Loss: 0.6367\nEpoch [9/100], Step [26/31], Loss: 1.2243\nEpoch [9/100], Step [27/31], Loss: 0.4633\nEpoch [9/100], Step [28/31], Loss: 0.7213\nEpoch [9/100], Step [29/31], Loss: 0.6055\nEpoch [9/100], Step [30/31], Loss: 0.6406\nEpoch [9/100], Step [31/31], Loss: 0.9946\nTraining Loss: 0.7282\nTraining Accuracy: 0.9001\nValidation Accuracy: 0.8262\nPrecision: 0.8387, Recall: 0.8262\nEpoch [10/100], Step [1/31], Loss: 0.6279\nEpoch [10/100], Step [2/31], Loss: 0.5409\nEpoch [10/100], Step [3/31], Loss: 0.5793\nEpoch [10/100], Step [4/31], Loss: 0.6326\nEpoch [10/100], Step [5/31], Loss: 0.4823\nEpoch [10/100], Step [6/31], Loss: 0.5828\nEpoch [10/100], Step [7/31], Loss: 0.6435\nEpoch [10/100], Step [8/31], Loss: 0.5892\nEpoch [10/100], Step [9/31], Loss: 0.5447\nEpoch [10/100], Step [10/31], Loss: 0.7554\nEpoch [10/100], Step [11/31], Loss: 0.5993\nEpoch [10/100], Step [12/31], Loss: 0.6210\nEpoch [10/100], Step [13/31], Loss: 0.6628\nEpoch [10/100], Step [14/31], Loss: 0.6481\nEpoch [10/100], Step [15/31], Loss: 0.6420\nEpoch [10/100], Step [16/31], Loss: 0.7016\nEpoch [10/100], Step [17/31], Loss: 0.5197\nEpoch [10/100], Step [18/31], Loss: 0.5430\nEpoch [10/100], Step [19/31], Loss: 0.6572\nEpoch [10/100], Step [20/31], Loss: 0.7151\nEpoch [10/100], Step [21/31], Loss: 0.6824\nEpoch [10/100], Step [22/31], Loss: 0.5267\nEpoch [10/100], Step [23/31], Loss: 0.4918\nEpoch [10/100], Step [24/31], Loss: 0.5576\nEpoch [10/100], Step [25/31], Loss: 0.5386\nEpoch [10/100], Step [26/31], Loss: 0.5064\nEpoch [10/100], Step [27/31], Loss: 0.4695\nEpoch [10/100], Step [28/31], Loss: 0.6395\nEpoch [10/100], Step [29/31], Loss: 0.6408\nEpoch [10/100], Step [30/31], Loss: 0.7682\nEpoch [10/100], Step [31/31], Loss: 0.5681\nTraining Loss: 0.6033\nTraining Accuracy: 0.9197\nValidation Accuracy: 0.8689\nPrecision: 0.8870, Recall: 0.8689\nEpoch [11/100], Step [1/31], Loss: 0.5900\nEpoch [11/100], Step [2/31], Loss: 0.4994\nEpoch [11/100], Step [3/31], Loss: 0.8458\nEpoch [11/100], Step [4/31], Loss: 0.5009\nEpoch [11/100], Step [5/31], Loss: 0.4472\nEpoch [11/100], Step [6/31], Loss: 0.5408\nEpoch [11/100], Step [7/31], Loss: 0.4193\nEpoch [11/100], Step [8/31], Loss: 0.4594\nEpoch [11/100], Step [9/31], Loss: 0.5340\nEpoch [11/100], Step [10/31], Loss: 0.5241\nEpoch [11/100], Step [11/31], Loss: 0.6309\nEpoch [11/100], Step [12/31], Loss: 0.5056\nEpoch [11/100], Step [13/31], Loss: 0.7088\nEpoch [11/100], Step [14/31], Loss: 0.5047\nEpoch [11/100], Step [15/31], Loss: 0.4689\nEpoch [11/100], Step [16/31], Loss: 0.5719\nEpoch [11/100], Step [17/31], Loss: 0.4792\nEpoch [11/100], Step [18/31], Loss: 0.4601\nEpoch [11/100], Step [19/31], Loss: 0.5936\nEpoch [11/100], Step [20/31], Loss: 0.7699\nEpoch [11/100], Step [21/31], Loss: 0.8522\nEpoch [11/100], Step [22/31], Loss: 0.5378\nEpoch [11/100], Step [23/31], Loss: 0.5544\nEpoch [11/100], Step [24/31], Loss: 0.5335\nEpoch [11/100], Step [25/31], Loss: 0.5678\nEpoch [11/100], Step [26/31], Loss: 0.5114\nEpoch [11/100], Step [27/31], Loss: 0.5098\nEpoch [11/100], Step [28/31], Loss: 0.5966\nEpoch [11/100], Step [29/31], Loss: 0.7672\nEpoch [11/100], Step [30/31], Loss: 0.5535\nEpoch [11/100], Step [31/31], Loss: 0.7188\nTraining Loss: 0.5697\nTraining Accuracy: 0.9434\nValidation Accuracy: 0.8623\nPrecision: 0.8739, Recall: 0.8623\nEpoch [12/100], Step [1/31], Loss: 0.4838\nEpoch [12/100], Step [2/31], Loss: 0.7594\nEpoch [12/100], Step [3/31], Loss: 0.5923\nEpoch [12/100], Step [4/31], Loss: 0.5089\nEpoch [12/100], Step [5/31], Loss: 0.5446\nEpoch [12/100], Step [6/31], Loss: 0.5612\nEpoch [12/100], Step [7/31], Loss: 0.4764\nEpoch [12/100], Step [8/31], Loss: 0.6035\nEpoch [12/100], Step [9/31], Loss: 0.4532\nEpoch [12/100], Step [10/31], Loss: 0.5471\nEpoch [12/100], Step [11/31], Loss: 0.7274\nEpoch [12/100], Step [12/31], Loss: 0.6683\nEpoch [12/100], Step [13/31], Loss: 0.4141\nEpoch [12/100], Step [14/31], Loss: 0.7222\nEpoch [12/100], Step [15/31], Loss: 0.6336\nEpoch [12/100], Step [16/31], Loss: 0.7767\nEpoch [12/100], Step [17/31], Loss: 0.6271\nEpoch [12/100], Step [18/31], Loss: 0.5942\nEpoch [12/100], Step [19/31], Loss: 0.4432\nEpoch [12/100], Step [20/31], Loss: 0.4654\nEpoch [12/100], Step [21/31], Loss: 0.5156\nEpoch [12/100], Step [22/31], Loss: 0.6594\nEpoch [12/100], Step [23/31], Loss: 0.4255\nEpoch [12/100], Step [24/31], Loss: 0.6154\nEpoch [12/100], Step [25/31], Loss: 0.4760\nEpoch [12/100], Step [26/31], Loss: 0.4873\nEpoch [12/100], Step [27/31], Loss: 0.6075\nEpoch [12/100], Step [28/31], Loss: 0.5275\nEpoch [12/100], Step [29/31], Loss: 0.6234\nEpoch [12/100], Step [30/31], Loss: 0.7726\nEpoch [12/100], Step [31/31], Loss: 0.7704\nTraining Loss: 0.5793\nTraining Accuracy: 0.9320\nValidation Accuracy: 0.8295\nPrecision: 0.8714, Recall: 0.8295\nEpoch [13/100], Step [1/31], Loss: 0.6391\nEpoch [13/100], Step [2/31], Loss: 0.5061\nEpoch [13/100], Step [3/31], Loss: 0.5183\nEpoch [13/100], Step [4/31], Loss: 0.4925\nEpoch [13/100], Step [5/31], Loss: 0.5729\nEpoch [13/100], Step [6/31], Loss: 0.4722\nEpoch [13/100], Step [7/31], Loss: 0.4295\nEpoch [13/100], Step [8/31], Loss: 0.4844\nEpoch [13/100], Step [9/31], Loss: 0.5254\nEpoch [13/100], Step [10/31], Loss: 0.5457\nEpoch [13/100], Step [11/31], Loss: 0.4278\nEpoch [13/100], Step [12/31], Loss: 0.6230\nEpoch [13/100], Step [13/31], Loss: 0.4522\nEpoch [13/100], Step [14/31], Loss: 0.5472\nEpoch [13/100], Step [15/31], Loss: 0.5177\nEpoch [13/100], Step [16/31], Loss: 0.5044\nEpoch [13/100], Step [17/31], Loss: 0.5864\nEpoch [13/100], Step [18/31], Loss: 0.5742\nEpoch [13/100], Step [19/31], Loss: 0.4268\nEpoch [13/100], Step [20/31], Loss: 0.4691\nEpoch [13/100], Step [21/31], Loss: 0.5382\nEpoch [13/100], Step [22/31], Loss: 0.4203\nEpoch [13/100], Step [23/31], Loss: 0.5165\nEpoch [13/100], Step [24/31], Loss: 0.4322\nEpoch [13/100], Step [25/31], Loss: 0.4510\nEpoch [13/100], Step [26/31], Loss: 0.4644\nEpoch [13/100], Step [27/31], Loss: 0.5426\nEpoch [13/100], Step [28/31], Loss: 0.5723\nEpoch [13/100], Step [29/31], Loss: 0.5104\nEpoch [13/100], Step [30/31], Loss: 0.7031\nEpoch [13/100], Step [31/31], Loss: 0.6982\nTraining Loss: 0.5176\nTraining Accuracy: 0.9609\nValidation Accuracy: 0.9131\nPrecision: 0.9181, Recall: 0.9131\nEpoch [14/100], Step [1/31], Loss: 0.5828\nEpoch [14/100], Step [2/31], Loss: 0.4695\nEpoch [14/100], Step [3/31], Loss: 0.6729\nEpoch [14/100], Step [4/31], Loss: 0.4466\nEpoch [14/100], Step [5/31], Loss: 0.5943\nEpoch [14/100], Step [6/31], Loss: 0.4883\nEpoch [14/100], Step [7/31], Loss: 0.5228\nEpoch [14/100], Step [8/31], Loss: 0.5043\nEpoch [14/100], Step [9/31], Loss: 0.6863\nEpoch [14/100], Step [10/31], Loss: 0.6086\nEpoch [14/100], Step [11/31], Loss: 0.4762\nEpoch [14/100], Step [12/31], Loss: 0.6504\nEpoch [14/100], Step [13/31], Loss: 0.6135\nEpoch [14/100], Step [14/31], Loss: 0.5182\nEpoch [14/100], Step [15/31], Loss: 0.5032\nEpoch [14/100], Step [16/31], Loss: 0.5387\nEpoch [14/100], Step [17/31], Loss: 0.4927\nEpoch [14/100], Step [18/31], Loss: 0.5430\nEpoch [14/100], Step [19/31], Loss: 0.4185\nEpoch [14/100], Step [20/31], Loss: 0.6227\nEpoch [14/100], Step [21/31], Loss: 0.7603\nEpoch [14/100], Step [22/31], Loss: 0.6310\nEpoch [14/100], Step [23/31], Loss: 0.4218\nEpoch [14/100], Step [24/31], Loss: 0.4894\nEpoch [14/100], Step [25/31], Loss: 0.4338\nEpoch [14/100], Step [26/31], Loss: 0.5721\nEpoch [14/100], Step [27/31], Loss: 0.5480\nEpoch [14/100], Step [28/31], Loss: 0.4618\nEpoch [14/100], Step [29/31], Loss: 0.4988\nEpoch [14/100], Step [30/31], Loss: 0.4684\nEpoch [14/100], Step [31/31], Loss: 0.4389\nTraining Loss: 0.5401\nTraining Accuracy: 0.9485\nValidation Accuracy: 0.8443\nPrecision: 0.8553, Recall: 0.8443\nEpoch [15/100], Step [1/31], Loss: 0.4188\nEpoch [15/100], Step [2/31], Loss: 0.4475\nEpoch [15/100], Step [3/31], Loss: 0.4562\nEpoch [15/100], Step [4/31], Loss: 0.5467\nEpoch [15/100], Step [5/31], Loss: 0.4289\nEpoch [15/100], Step [6/31], Loss: 0.4360\nEpoch [15/100], Step [7/31], Loss: 0.4454\nEpoch [15/100], Step [8/31], Loss: 0.4113\nEpoch [15/100], Step [9/31], Loss: 0.4319\nEpoch [15/100], Step [10/31], Loss: 0.3931\nEpoch [15/100], Step [11/31], Loss: 0.3971\nEpoch [15/100], Step [12/31], Loss: 0.5868\nEpoch [15/100], Step [13/31], Loss: 0.3893\nEpoch [15/100], Step [14/31], Loss: 0.7706\nEpoch [15/100], Step [15/31], Loss: 0.4172\nEpoch [15/100], Step [16/31], Loss: 0.4269\nEpoch [15/100], Step [17/31], Loss: 0.4333\nEpoch [15/100], Step [18/31], Loss: 0.5614\nEpoch [15/100], Step [19/31], Loss: 0.4267\nEpoch [15/100], Step [20/31], Loss: 0.4097\nEpoch [15/100], Step [21/31], Loss: 0.5119\nEpoch [15/100], Step [22/31], Loss: 0.5037\nEpoch [15/100], Step [23/31], Loss: 0.5482\nEpoch [15/100], Step [24/31], Loss: 0.5005\nEpoch [15/100], Step [25/31], Loss: 0.4132\nEpoch [15/100], Step [26/31], Loss: 0.4969\nEpoch [15/100], Step [27/31], Loss: 0.4457\nEpoch [15/100], Step [28/31], Loss: 0.5469\nEpoch [15/100], Step [29/31], Loss: 0.3995\nEpoch [15/100], Step [30/31], Loss: 0.5355\nEpoch [15/100], Step [31/31], Loss: 0.8527\nTraining Loss: 0.4755\nTraining Accuracy: 0.9701\nValidation Accuracy: 0.7557\nPrecision: 0.8091, Recall: 0.7557\nEpoch [16/100], Step [1/31], Loss: 0.3936\nEpoch [16/100], Step [2/31], Loss: 0.5351\nEpoch [16/100], Step [3/31], Loss: 0.5053\nEpoch [16/100], Step [4/31], Loss: 0.4809\nEpoch [16/100], Step [5/31], Loss: 0.6341\nEpoch [16/100], Step [6/31], Loss: 0.4571\nEpoch [16/100], Step [7/31], Loss: 0.9526\nEpoch [16/100], Step [8/31], Loss: 0.4527\nEpoch [16/100], Step [9/31], Loss: 0.7242\nEpoch [16/100], Step [10/31], Loss: 0.5159\nEpoch [16/100], Step [11/31], Loss: 0.4420\nEpoch [16/100], Step [12/31], Loss: 0.4221\nEpoch [16/100], Step [13/31], Loss: 0.7157\nEpoch [16/100], Step [14/31], Loss: 0.5838\nEpoch [16/100], Step [15/31], Loss: 0.5914\nEpoch [16/100], Step [16/31], Loss: 0.6411\nEpoch [16/100], Step [17/31], Loss: 0.5355\nEpoch [16/100], Step [18/31], Loss: 0.3972\nEpoch [16/100], Step [19/31], Loss: 0.8777\nEpoch [16/100], Step [20/31], Loss: 0.7537\nEpoch [16/100], Step [21/31], Loss: 0.6377\nEpoch [16/100], Step [22/31], Loss: 0.7159\nEpoch [16/100], Step [23/31], Loss: 0.4507\nEpoch [16/100], Step [24/31], Loss: 0.4313\nEpoch [16/100], Step [25/31], Loss: 0.4200\nEpoch [16/100], Step [26/31], Loss: 0.4752\nEpoch [16/100], Step [27/31], Loss: 0.9686\nEpoch [16/100], Step [28/31], Loss: 0.4301\nEpoch [16/100], Step [29/31], Loss: 0.5969\nEpoch [16/100], Step [30/31], Loss: 0.4460\nEpoch [16/100], Step [31/31], Loss: 0.5418\nTraining Loss: 0.5725\nTraining Accuracy: 0.9351\nValidation Accuracy: 0.7623\nPrecision: 0.8220, Recall: 0.7623\nEpoch [17/100], Step [1/31], Loss: 0.4937\nEpoch [17/100], Step [2/31], Loss: 0.5654\nEpoch [17/100], Step [3/31], Loss: 0.4541\nEpoch [17/100], Step [4/31], Loss: 0.4243\nEpoch [17/100], Step [5/31], Loss: 0.4120\nEpoch [17/100], Step [6/31], Loss: 0.4479\nEpoch [17/100], Step [8/31], Loss: 0.4271\nEpoch [17/100], Step [9/31], Loss: 0.4378\nEpoch [17/100], Step [10/31], Loss: 0.4113\nEpoch [17/100], Step [11/31], Loss: 0.4593\nEpoch [17/100], Step [12/31], Loss: 0.4559\nEpoch [17/100], Step [13/31], Loss: 0.3848\nEpoch [17/100], Step [14/31], Loss: 0.4166\nEpoch [17/100], Step [15/31], Loss: 0.4456\nEpoch [17/100], Step [16/31], Loss: 0.6509\nEpoch [17/100], Step [17/31], Loss: 0.6058\nEpoch [17/100], Step [18/31], Loss: 0.4025\nEpoch [17/100], Step [19/31], Loss: 0.4160\nEpoch [17/100], Step [20/31], Loss: 0.4964\nEpoch [17/100], Step [21/31], Loss: 0.5930\nEpoch [17/100], Step [22/31], Loss: 0.4285\nEpoch [17/100], Step [23/31], Loss: 0.4612\nEpoch [17/100], Step [24/31], Loss: 0.7268\nEpoch [17/100], Step [25/31], Loss: 0.3916\nEpoch [17/100], Step [26/31], Loss: 0.4280\nEpoch [17/100], Step [27/31], Loss: 0.6244\nEpoch [17/100], Step [28/31], Loss: 0.4463\nEpoch [17/100], Step [29/31], Loss: 0.9507\nEpoch [17/100], Step [30/31], Loss: 0.5663\nEpoch [17/100], Step [31/31], Loss: 0.4325\nTraining Loss: 0.4986\nTraining Accuracy: 0.9629\nValidation Accuracy: 0.8869\nPrecision: 0.9007, Recall: 0.8869\nEpoch [18/100], Step [1/31], Loss: 0.4143\nEpoch [18/100], Step [2/31], Loss: 0.4433\nEpoch [18/100], Step [3/31], Loss: 0.4498\nEpoch [18/100], Step [4/31], Loss: 0.4897\nEpoch [18/100], Step [5/31], Loss: 0.5178\nEpoch [18/100], Step [6/31], Loss: 0.5035\nEpoch [18/100], Step [7/31], Loss: 0.4009\nEpoch [18/100], Step [8/31], Loss: 0.4715\nEpoch [18/100], Step [9/31], Loss: 0.4012\nEpoch [18/100], Step [10/31], Loss: 0.5277\nEpoch [18/100], Step [11/31], Loss: 0.4329\nEpoch [18/100], Step [12/31], Loss: 0.5249\nEpoch [18/100], Step [13/31], Loss: 0.4257\nEpoch [18/100], Step [14/31], Loss: 0.4356\nEpoch [18/100], Step [15/31], Loss: 0.5052\nEpoch [18/100], Step [16/31], Loss: 0.4029\nEpoch [18/100], Step [17/31], Loss: 0.4413\nEpoch [18/100], Step [18/31], Loss: 0.4658\nEpoch [18/100], Step [19/31], Loss: 0.4135\nEpoch [18/100], Step [20/31], Loss: 0.3858\nEpoch [18/100], Step [21/31], Loss: 0.4495\nEpoch [18/100], Step [22/31], Loss: 0.4046\nEpoch [18/100], Step [23/31], Loss: 0.5020\nEpoch [18/100], Step [24/31], Loss: 0.4626\nEpoch [18/100], Step [25/31], Loss: 0.5085\nEpoch [18/100], Step [26/31], Loss: 0.4870\nEpoch [18/100], Step [27/31], Loss: 0.5682\nEpoch [18/100], Step [28/31], Loss: 0.4189\nEpoch [18/100], Step [29/31], Loss: 0.4174\nEpoch [18/100], Step [30/31], Loss: 0.4567\nEpoch [18/100], Step [31/31], Loss: 1.1858\nTraining Loss: 0.4659\nTraining Accuracy: 0.9753\nValidation Accuracy: 0.8623\nPrecision: 0.8741, Recall: 0.8623\nEpoch [19/100], Step [1/31], Loss: 0.4697\nEpoch [19/100], Step [2/31], Loss: 0.9448\nEpoch [19/100], Step [3/31], Loss: 0.5867\nEpoch [19/100], Step [4/31], Loss: 0.6449\nEpoch [19/100], Step [5/31], Loss: 0.5919\nEpoch [19/100], Step [6/31], Loss: 0.3998\nEpoch [19/100], Step [7/31], Loss: 0.5233\nEpoch [19/100], Step [8/31], Loss: 0.5295\nEpoch [19/100], Step [9/31], Loss: 0.4964\nEpoch [19/100], Step [10/31], Loss: 0.4453\nEpoch [19/100], Step [11/31], Loss: 0.4757\nEpoch [19/100], Step [12/31], Loss: 0.4700\nEpoch [19/100], Step [13/31], Loss: 0.5003\nEpoch [19/100], Step [14/31], Loss: 0.4593\nEpoch [19/100], Step [15/31], Loss: 0.5041\nEpoch [19/100], Step [16/31], Loss: 0.5797\nEpoch [19/100], Step [17/31], Loss: 0.4553\nEpoch [19/100], Step [18/31], Loss: 0.4614\nEpoch [19/100], Step [19/31], Loss: 0.5444\nEpoch [19/100], Step [20/31], Loss: 0.5055\nEpoch [19/100], Step [21/31], Loss: 0.3966\nEpoch [19/100], Step [22/31], Loss: 0.4961\nEpoch [19/100], Step [23/31], Loss: 0.3948\nEpoch [19/100], Step [24/31], Loss: 0.7218\nEpoch [19/100], Step [25/31], Loss: 0.4119\nEpoch [19/100], Step [26/31], Loss: 0.4059\nEpoch [19/100], Step [27/31], Loss: 0.6885\nEpoch [19/100], Step [28/31], Loss: 0.4998\nEpoch [19/100], Step [29/31], Loss: 0.5336\nEpoch [19/100], Step [30/31], Loss: 0.6075\nEpoch [19/100], Step [31/31], Loss: 0.4708\nTraining Loss: 0.5242\nTraining Accuracy: 0.9526\nValidation Accuracy: 0.8197\nPrecision: 0.8687, Recall: 0.8197\nEpoch [20/100], Step [1/31], Loss: 0.5027\nEpoch [20/100], Step [2/31], Loss: 0.4099\nEpoch [20/100], Step [3/31], Loss: 0.3934\nEpoch [20/100], Step [4/31], Loss: 0.4384\nEpoch [20/100], Step [5/31], Loss: 0.4299\nEpoch [20/100], Step [6/31], Loss: 0.4938\nEpoch [20/100], Step [7/31], Loss: 0.4331\nEpoch [20/100], Step [8/31], Loss: 0.4089\nEpoch [20/100], Step [9/31], Loss: 0.4333\nEpoch [20/100], Step [10/31], Loss: 0.5218\nEpoch [20/100], Step [11/31], Loss: 0.3869\nEpoch [20/100], Step [12/31], Loss: 0.4418\nEpoch [20/100], Step [13/31], Loss: 0.4497\nEpoch [20/100], Step [14/31], Loss: 0.5144\nEpoch [20/100], Step [15/31], Loss: 0.4205\nEpoch [20/100], Step [16/31], Loss: 0.4845\nEpoch [20/100], Step [17/31], Loss: 0.5091\nEpoch [20/100], Step [18/31], Loss: 0.4062\nEpoch [20/100], Step [19/31], Loss: 0.4028\nEpoch [20/100], Step [20/31], Loss: 0.4146\nEpoch [20/100], Step [21/31], Loss: 0.6326\nEpoch [20/100], Step [22/31], Loss: 0.3854\nEpoch [20/100], Step [23/31], Loss: 0.4161\nEpoch [20/100], Step [24/31], Loss: 0.4223\nEpoch [20/100], Step [25/31], Loss: 0.4177\nEpoch [20/100], Step [26/31], Loss: 0.4233\nEpoch [20/100], Step [27/31], Loss: 0.5583\nEpoch [20/100], Step [28/31], Loss: 0.4300\nEpoch [20/100], Step [29/31], Loss: 0.5304\nEpoch [20/100], Step [30/31], Loss: 0.5836\nEpoch [20/100], Step [31/31], Loss: 1.0017\nTraining Loss: 0.4627\nTraining Accuracy: 0.9722\nValidation Accuracy: 0.8295\nPrecision: 0.8725, Recall: 0.8295\nEpoch [21/100], Step [1/31], Loss: 0.5653\nEpoch [21/100], Step [2/31], Loss: 0.4302\nEpoch [21/100], Step [3/31], Loss: 0.5244\nEpoch [21/100], Step [4/31], Loss: 0.6218\nEpoch [21/100], Step [5/31], Loss: 0.4814\nEpoch [21/100], Step [6/31], Loss: 0.4959\nEpoch [21/100], Step [7/31], Loss: 0.4612\nEpoch [21/100], Step [8/31], Loss: 0.4109\nEpoch [21/100], Step [9/31], Loss: 0.3944\nEpoch [21/100], Step [10/31], Loss: 0.4077\nEpoch [21/100], Step [11/31], Loss: 0.4238\nEpoch [21/100], Step [12/31], Loss: 0.4626\nEpoch [21/100], Step [13/31], Loss: 0.4716\nEpoch [21/100], Step [14/31], Loss: 0.4234\nEpoch [21/100], Step [15/31], Loss: 0.7041\nEpoch [21/100], Step [16/31], Loss: 0.3804\nEpoch [21/100], Step [17/31], Loss: 0.4051\nEpoch [21/100], Step [18/31], Loss: 0.3840\nEpoch [21/100], Step [19/31], Loss: 0.5840\nEpoch [21/100], Step [20/31], Loss: 0.4863\nEpoch [21/100], Step [21/31], Loss: 0.4610\nEpoch [21/100], Step [22/31], Loss: 0.3899\nEpoch [21/100], Step [23/31], Loss: 0.4590\nEpoch [21/100], Step [24/31], Loss: 0.4570\nEpoch [21/100], Step [25/31], Loss: 0.4969\nEpoch [21/100], Step [26/31], Loss: 0.4299\nEpoch [21/100], Step [27/31], Loss: 0.4859\nEpoch [21/100], Step [28/31], Loss: 0.4152\nEpoch [21/100], Step [29/31], Loss: 0.5389\nEpoch [21/100], Step [30/31], Loss: 0.4076\nEpoch [21/100], Step [31/31], Loss: 0.4681\nTraining Loss: 0.4687\nTraining Accuracy: 0.9650\nValidation Accuracy: 0.8672\nPrecision: 0.8810, Recall: 0.8672\nEpoch [22/100], Step [1/31], Loss: 0.5260\nEpoch [22/100], Step [2/31], Loss: 0.4629\nEpoch [22/100], Step [3/31], Loss: 0.4227\nEpoch [22/100], Step [4/31], Loss: 0.4065\nEpoch [22/100], Step [5/31], Loss: 0.4014\nEpoch [22/100], Step [6/31], Loss: 0.4636\nEpoch [22/100], Step [7/31], Loss: 0.4416\nEpoch [22/100], Step [8/31], Loss: 0.4424\nEpoch [22/100], Step [9/31], Loss: 0.5111\nEpoch [22/100], Step [10/31], Loss: 0.3846\nEpoch [22/100], Step [11/31], Loss: 0.3915\nEpoch [22/100], Step [12/31], Loss: 0.4106\nEpoch [22/100], Step [13/31], Loss: 0.4064\nEpoch [22/100], Step [14/31], Loss: 0.4515\nEpoch [22/100], Step [15/31], Loss: 0.3943\nEpoch [22/100], Step [16/31], Loss: 0.3779\nEpoch [22/100], Step [17/31], Loss: 0.4778\nEpoch [22/100], Step [18/31], Loss: 0.3734\nEpoch [22/100], Step [19/31], Loss: 0.4314\nEpoch [22/100], Step [20/31], Loss: 0.3775\nEpoch [22/100], Step [21/31], Loss: 0.4215\nEpoch [22/100], Step [22/31], Loss: 0.4782\nEpoch [22/100], Step [23/31], Loss: 0.3990\nEpoch [22/100], Step [24/31], Loss: 0.3640\nEpoch [22/100], Step [25/31], Loss: 0.3756\nEpoch [22/100], Step [26/31], Loss: 0.3701\nEpoch [22/100], Step [27/31], Loss: 0.4389\nEpoch [22/100], Step [28/31], Loss: 0.3862\nEpoch [22/100], Step [29/31], Loss: 0.3907\nEpoch [22/100], Step [30/31], Loss: 0.4532\nEpoch [22/100], Step [31/31], Loss: 0.4004\nTraining Loss: 0.4209\nTraining Accuracy: 0.9804\nValidation Accuracy: 0.9000\nPrecision: 0.9027, Recall: 0.9000\nEpoch [23/100], Step [1/31], Loss: 0.4559\nEpoch [23/100], Step [2/31], Loss: 0.3655\nEpoch [23/100], Step [3/31], Loss: 0.3659\nEpoch [23/100], Step [4/31], Loss: 0.4334\nEpoch [23/100], Step [5/31], Loss: 0.3933\nEpoch [23/100], Step [6/31], Loss: 0.3858\nEpoch [23/100], Step [7/31], Loss: 0.3768\nEpoch [23/100], Step [8/31], Loss: 0.3667\nEpoch [23/100], Step [9/31], Loss: 0.3641\nEpoch [23/100], Step [10/31], Loss: 0.4420\nEpoch [23/100], Step [11/31], Loss: 0.3984\nEpoch [23/100], Step [12/31], Loss: 0.3800\nEpoch [23/100], Step [13/31], Loss: 0.3822\nEpoch [23/100], Step [14/31], Loss: 0.3784\nEpoch [23/100], Step [15/31], Loss: 0.3918\nEpoch [23/100], Step [16/31], Loss: 0.4896\nEpoch [23/100], Step [17/31], Loss: 0.3819\nEpoch [23/100], Step [18/31], Loss: 0.3720\nEpoch [23/100], Step [19/31], Loss: 0.3876\nEpoch [23/100], Step [20/31], Loss: 0.4620\nEpoch [23/100], Step [21/31], Loss: 0.4967\nEpoch [23/100], Step [22/31], Loss: 0.3659\nEpoch [23/100], Step [23/31], Loss: 0.3582\nEpoch [23/100], Step [24/31], Loss: 0.4438\nEpoch [23/100], Step [25/31], Loss: 0.4079\nEpoch [23/100], Step [26/31], Loss: 0.3747\nEpoch [23/100], Step [27/31], Loss: 0.4020\nEpoch [23/100], Step [28/31], Loss: 0.4154\nEpoch [23/100], Step [29/31], Loss: 0.3701\nEpoch [23/100], Step [30/31], Loss: 0.4471\nEpoch [23/100], Step [31/31], Loss: 0.4295\nTraining Loss: 0.4021\nTraining Accuracy: 0.9835\nValidation Accuracy: 0.8623\nPrecision: 0.8834, Recall: 0.8623\nEpoch [24/100], Step [1/31], Loss: 0.3607\nEpoch [24/100], Step [2/31], Loss: 0.3798\nEpoch [24/100], Step [3/31], Loss: 0.3604\nEpoch [24/100], Step [4/31], Loss: 0.3558\nEpoch [24/100], Step [5/31], Loss: 0.4583\nEpoch [24/100], Step [6/31], Loss: 0.3692\nEpoch [24/100], Step [7/31], Loss: 0.4651\nEpoch [24/100], Step [8/31], Loss: 0.3706\nEpoch [24/100], Step [9/31], Loss: 0.4054\nEpoch [24/100], Step [10/31], Loss: 0.3770\nEpoch [24/100], Step [11/31], Loss: 0.3520\nEpoch [24/100], Step [12/31], Loss: 0.3557\nEpoch [24/100], Step [13/31], Loss: 0.3698\nEpoch [24/100], Step [14/31], Loss: 0.3630\nEpoch [24/100], Step [15/31], Loss: 0.4779\nEpoch [24/100], Step [16/31], Loss: 0.5072\nEpoch [24/100], Step [17/31], Loss: 0.3748\nEpoch [24/100], Step [18/31], Loss: 0.3544\nEpoch [24/100], Step [19/31], Loss: 0.3994\nEpoch [24/100], Step [20/31], Loss: 0.3668\nEpoch [24/100], Step [21/31], Loss: 0.4355\nEpoch [24/100], Step [22/31], Loss: 0.4146\nEpoch [24/100], Step [23/31], Loss: 0.5459\nEpoch [24/100], Step [24/31], Loss: 0.3517\nEpoch [24/100], Step [25/31], Loss: 0.3526\nEpoch [24/100], Step [26/31], Loss: 0.3836\nEpoch [24/100], Step [27/31], Loss: 0.3627\nEpoch [24/100], Step [28/31], Loss: 0.3967\nEpoch [24/100], Step [29/31], Loss: 0.4353\nEpoch [24/100], Step [30/31], Loss: 0.3556\nEpoch [24/100], Step [31/31], Loss: 0.7246\nTraining Loss: 0.3990\nTraining Accuracy: 0.9825\nValidation Accuracy: 0.8098\nPrecision: 0.8733, Recall: 0.8098\nEpoch [25/100], Step [1/31], Loss: 0.3647\nEpoch [25/100], Step [2/31], Loss: 0.3714\nEpoch [25/100], Step [3/31], Loss: 0.4495\nEpoch [25/100], Step [4/31], Loss: 0.3946\nEpoch [25/100], Step [5/31], Loss: 0.5535\nEpoch [25/100], Step [6/31], Loss: 0.6664\nEpoch [25/100], Step [7/31], Loss: 0.4754\nEpoch [25/100], Step [8/31], Loss: 0.7490\nEpoch [25/100], Step [9/31], Loss: 0.5352\nEpoch [25/100], Step [10/31], Loss: 0.4790\nEpoch [25/100], Step [11/31], Loss: 0.3823\nEpoch [25/100], Step [12/31], Loss: 0.3798\nEpoch [25/100], Step [13/31], Loss: 0.4820\nEpoch [25/100], Step [14/31], Loss: 0.3949\nEpoch [25/100], Step [15/31], Loss: 0.4915\nEpoch [25/100], Step [16/31], Loss: 0.4844\nEpoch [25/100], Step [17/31], Loss: 0.4756\nEpoch [25/100], Step [18/31], Loss: 0.4419\nEpoch [25/100], Step [19/31], Loss: 0.4154\nEpoch [25/100], Step [20/31], Loss: 0.4845\nEpoch [25/100], Step [21/31], Loss: 0.4163\nEpoch [25/100], Step [22/31], Loss: 0.6648\nEpoch [25/100], Step [23/31], Loss: 0.3708\nEpoch [25/100], Step [24/31], Loss: 0.4122\nEpoch [25/100], Step [25/31], Loss: 0.3940\nEpoch [25/100], Step [26/31], Loss: 0.4406\nEpoch [25/100], Step [27/31], Loss: 0.6938\nEpoch [25/100], Step [28/31], Loss: 0.3857\nEpoch [25/100], Step [29/31], Loss: 0.4059\nEpoch [25/100], Step [30/31], Loss: 0.4865\nEpoch [25/100], Step [31/31], Loss: 0.3773\nTraining Loss: 0.4703\nTraining Accuracy: 0.9578\nValidation Accuracy: 0.8475\nPrecision: 0.8960, Recall: 0.8475\nEpoch [26/100], Step [1/31], Loss: 0.4843\nEpoch [26/100], Step [2/31], Loss: 0.4368\nEpoch [26/100], Step [3/31], Loss: 0.3620\nEpoch [26/100], Step [4/31], Loss: 0.3717\nEpoch [26/100], Step [5/31], Loss: 0.3792\nEpoch [26/100], Step [6/31], Loss: 0.3998\nEpoch [26/100], Step [7/31], Loss: 0.3827\nEpoch [26/100], Step [8/31], Loss: 0.3838\nEpoch [26/100], Step [9/31], Loss: 0.4398\nEpoch [26/100], Step [10/31], Loss: 0.4608\nEpoch [26/100], Step [11/31], Loss: 0.3686\nEpoch [26/100], Step [12/31], Loss: 0.4365\nEpoch [26/100], Step [13/31], Loss: 0.4892\nEpoch [26/100], Step [14/31], Loss: 0.3622\nEpoch [26/100], Step [15/31], Loss: 0.3915\nEpoch [26/100], Step [16/31], Loss: 0.3957\nEpoch [26/100], Step [17/31], Loss: 0.3876\nEpoch [26/100], Step [18/31], Loss: 0.4225\nEpoch [26/100], Step [19/31], Loss: 0.3665\nEpoch [26/100], Step [20/31], Loss: 0.4405\nEpoch [26/100], Step [21/31], Loss: 0.3596\nEpoch [26/100], Step [22/31], Loss: 0.3807\nEpoch [26/100], Step [23/31], Loss: 0.3737\nEpoch [26/100], Step [24/31], Loss: 0.5028\nEpoch [26/100], Step [25/31], Loss: 0.4819\nEpoch [26/100], Step [26/31], Loss: 0.3510\nEpoch [26/100], Step [27/31], Loss: 0.3649\nEpoch [26/100], Step [28/31], Loss: 0.4359\nEpoch [26/100], Step [29/31], Loss: 0.3828\nEpoch [26/100], Step [30/31], Loss: 0.4373\nEpoch [26/100], Step [31/31], Loss: 0.3629\nTraining Loss: 0.4072\nTraining Accuracy: 0.9815\nValidation Accuracy: 0.8590\nPrecision: 0.8819, Recall: 0.8590\nEpoch [27/100], Step [1/31], Loss: 0.4421\nEpoch [27/100], Step [2/31], Loss: 0.4808\nEpoch [27/100], Step [3/31], Loss: 0.3795\nEpoch [27/100], Step [4/31], Loss: 0.4200\nEpoch [27/100], Step [5/31], Loss: 0.3615\nEpoch [27/100], Step [6/31], Loss: 0.3977\nEpoch [27/100], Step [7/31], Loss: 0.4134\nEpoch [27/100], Step [8/31], Loss: 0.4048\nEpoch [27/100], Step [9/31], Loss: 0.4039\nEpoch [27/100], Step [10/31], Loss: 0.3788\nEpoch [27/100], Step [11/31], Loss: 0.3694\nEpoch [27/100], Step [12/31], Loss: 0.3645\nEpoch [27/100], Step [13/31], Loss: 0.3762\nEpoch [27/100], Step [14/31], Loss: 0.3462\nEpoch [27/100], Step [15/31], Loss: 0.7131\nEpoch [27/100], Step [16/31], Loss: 0.3685\nEpoch [27/100], Step [17/31], Loss: 0.3676\nEpoch [27/100], Step [18/31], Loss: 0.6260\nEpoch [27/100], Step [19/31], Loss: 0.3631\nEpoch [27/100], Step [20/31], Loss: 0.3489\nEpoch [27/100], Step [21/31], Loss: 0.3743\nEpoch [27/100], Step [22/31], Loss: 0.3972\nEpoch [27/100], Step [23/31], Loss: 0.3673\nEpoch [27/100], Step [24/31], Loss: 0.5267\nEpoch [27/100], Step [25/31], Loss: 0.3560\nEpoch [27/100], Step [26/31], Loss: 0.3610\nEpoch [27/100], Step [27/31], Loss: 0.3923\nEpoch [27/100], Step [28/31], Loss: 0.4101\nEpoch [27/100], Step [29/31], Loss: 0.4164\nEpoch [27/100], Step [30/31], Loss: 0.4023\nEpoch [27/100], Step [31/31], Loss: 0.7833\nTraining Loss: 0.4152\nTraining Accuracy: 0.9804\nValidation Accuracy: 0.9246\nPrecision: 0.9275, Recall: 0.9246\nEpoch [28/100], Step [1/31], Loss: 0.3464\nEpoch [28/100], Step [2/31], Loss: 0.3541\nEpoch [28/100], Step [3/31], Loss: 0.3829\nEpoch [28/100], Step [4/31], Loss: 0.4545\nEpoch [28/100], Step [5/31], Loss: 0.3957\nEpoch [28/100], Step [6/31], Loss: 0.3794\nEpoch [28/100], Step [7/31], Loss: 0.3453\nEpoch [28/100], Step [8/31], Loss: 0.3783\nEpoch [28/100], Step [9/31], Loss: 0.3562\nEpoch [28/100], Step [10/31], Loss: 0.4524\nEpoch [28/100], Step [11/31], Loss: 0.3432\nEpoch [28/100], Step [12/31], Loss: 0.3791\nEpoch [28/100], Step [13/31], Loss: 0.4033\nEpoch [28/100], Step [14/31], Loss: 0.4317\nEpoch [28/100], Step [15/31], Loss: 0.3482\nEpoch [28/100], Step [16/31], Loss: 0.4062\nEpoch [28/100], Step [17/31], Loss: 0.3433\nEpoch [28/100], Step [18/31], Loss: 0.3453\nEpoch [28/100], Step [19/31], Loss: 0.3753\nEpoch [28/100], Step [20/31], Loss: 0.3787\nEpoch [28/100], Step [21/31], Loss: 0.3530\nEpoch [28/100], Step [22/31], Loss: 0.3975\nEpoch [28/100], Step [23/31], Loss: 0.3510\nEpoch [28/100], Step [24/31], Loss: 0.4989\nEpoch [28/100], Step [25/31], Loss: 0.4277\nEpoch [28/100], Step [26/31], Loss: 0.3500\nEpoch [28/100], Step [27/31], Loss: 0.3467\nEpoch [28/100], Step [28/31], Loss: 0.4028\nEpoch [28/100], Step [29/31], Loss: 0.4172\nEpoch [28/100], Step [30/31], Loss: 0.3744\nEpoch [28/100], Step [31/31], Loss: 0.4922\nTraining Loss: 0.3852\nTraining Accuracy: 0.9866\nValidation Accuracy: 0.9098\nPrecision: 0.9132, Recall: 0.9098\nEpoch [29/100], Step [1/31], Loss: 0.3454\nEpoch [29/100], Step [2/31], Loss: 0.3562\nEpoch [29/100], Step [3/31], Loss: 0.3610\nEpoch [29/100], Step [4/31], Loss: 0.4183\nEpoch [29/100], Step [5/31], Loss: 0.4093\nEpoch [29/100], Step [6/31], Loss: 0.3437\nEpoch [29/100], Step [7/31], Loss: 0.3730\nEpoch [29/100], Step [8/31], Loss: 0.4119\nEpoch [29/100], Step [9/31], Loss: 0.3843\nEpoch [29/100], Step [10/31], Loss: 0.3581\nEpoch [29/100], Step [11/31], Loss: 0.4697\nEpoch [29/100], Step [12/31], Loss: 0.3568\nEpoch [29/100], Step [13/31], Loss: 0.3418\nEpoch [29/100], Step [14/31], Loss: 0.3739\nEpoch [29/100], Step [15/31], Loss: 0.3458\nEpoch [29/100], Step [16/31], Loss: 0.4160\nEpoch [29/100], Step [17/31], Loss: 0.3573\nEpoch [29/100], Step [18/31], Loss: 0.3543\nEpoch [29/100], Step [19/31], Loss: 0.3958\nEpoch [29/100], Step [20/31], Loss: 0.4116\nEpoch [29/100], Step [21/31], Loss: 0.3928\nEpoch [29/100], Step [22/31], Loss: 0.3925\nEpoch [29/100], Step [23/31], Loss: 0.3998\nEpoch [29/100], Step [24/31], Loss: 0.3763\nEpoch [29/100], Step [25/31], Loss: 0.3514\nEpoch [29/100], Step [26/31], Loss: 0.5445\nEpoch [29/100], Step [27/31], Loss: 0.3444\nEpoch [29/100], Step [28/31], Loss: 0.3717\nEpoch [29/100], Step [29/31], Loss: 0.3393\nEpoch [29/100], Step [30/31], Loss: 0.4481\nEpoch [29/100], Step [31/31], Loss: 0.4449\nTraining Loss: 0.3855\nTraining Accuracy: 0.9815\nValidation Accuracy: 0.8361\nPrecision: 0.8872, Recall: 0.8361\nEpoch [30/100], Step [1/31], Loss: 0.3486\nEpoch [30/100], Step [2/31], Loss: 0.4711\nEpoch [30/100], Step [3/31], Loss: 0.3802\nEpoch [30/100], Step [4/31], Loss: 0.3439\nEpoch [30/100], Step [5/31], Loss: 0.3850\nEpoch [30/100], Step [6/31], Loss: 0.3924\nEpoch [30/100], Step [7/31], Loss: 0.3822\nEpoch [30/100], Step [8/31], Loss: 0.3651\nEpoch [30/100], Step [9/31], Loss: 0.3517\nEpoch [30/100], Step [10/31], Loss: 0.3472\nEpoch [30/100], Step [11/31], Loss: 0.3455\nEpoch [30/100], Step [12/31], Loss: 0.5529\nEpoch [30/100], Step [13/31], Loss: 0.3579\nEpoch [30/100], Step [14/31], Loss: 0.3440\nEpoch [30/100], Step [15/31], Loss: 0.3360\nEpoch [30/100], Step [16/31], Loss: 0.4928\nEpoch [30/100], Step [17/31], Loss: 0.3618\nEpoch [30/100], Step [18/31], Loss: 0.4169\nEpoch [30/100], Step [19/31], Loss: 0.6673\nEpoch [30/100], Step [20/31], Loss: 0.5145\nEpoch [30/100], Step [21/31], Loss: 0.6657\nEpoch [30/100], Step [22/31], Loss: 0.5581\nEpoch [30/100], Step [23/31], Loss: 0.3673\nEpoch [30/100], Step [24/31], Loss: 0.4387\nEpoch [30/100], Step [25/31], Loss: 0.6784\nEpoch [30/100], Step [26/31], Loss: 0.5061\nEpoch [30/100], Step [27/31], Loss: 0.6088\nEpoch [30/100], Step [28/31], Loss: 0.5493\nEpoch [30/100], Step [29/31], Loss: 0.3567\nEpoch [30/100], Step [30/31], Loss: 0.3609\nEpoch [30/100], Step [31/31], Loss: 0.3773\nTraining Loss: 0.4408\nTraining Accuracy: 0.9619\nValidation Accuracy: 0.8541\nPrecision: 0.8720, Recall: 0.8541\nEpoch [31/100], Step [1/31], Loss: 0.4316\nEpoch [31/100], Step [2/31], Loss: 0.3892\nEpoch [31/100], Step [3/31], Loss: 0.4923\nEpoch [31/100], Step [4/31], Loss: 0.4495\nEpoch [31/100], Step [5/31], Loss: 0.3684\nEpoch [31/100], Step [6/31], Loss: 0.4532\nEpoch [31/100], Step [7/31], Loss: 0.4593\nEpoch [31/100], Step [8/31], Loss: 0.5819\nEpoch [31/100], Step [9/31], Loss: 0.4357\nEpoch [31/100], Step [10/31], Loss: 0.3909\nEpoch [31/100], Step [11/31], Loss: 0.4409\nEpoch [31/100], Step [12/31], Loss: 0.4932\nEpoch [31/100], Step [13/31], Loss: 0.4429\nEpoch [31/100], Step [14/31], Loss: 0.3660\nEpoch [31/100], Step [15/31], Loss: 0.3775\nEpoch [31/100], Step [16/31], Loss: 0.3647\nEpoch [31/100], Step [17/31], Loss: 0.5137\nEpoch [31/100], Step [18/31], Loss: 0.4569\nEpoch [31/100], Step [19/31], Loss: 0.5585\nEpoch [31/100], Step [20/31], Loss: 0.4054\nEpoch [31/100], Step [21/31], Loss: 0.3784\nEpoch [31/100], Step [22/31], Loss: 0.5400\nEpoch [31/100], Step [23/31], Loss: 0.3647\nEpoch [31/100], Step [24/31], Loss: 0.3590\nEpoch [31/100], Step [25/31], Loss: 0.3752\nEpoch [31/100], Step [26/31], Loss: 0.3995\nEpoch [31/100], Step [27/31], Loss: 0.4208\nEpoch [31/100], Step [28/31], Loss: 0.4602\nEpoch [31/100], Step [29/31], Loss: 0.4536\nEpoch [31/100], Step [30/31], Loss: 0.4240\nEpoch [31/100], Step [31/31], Loss: 0.3939\nTraining Loss: 0.4344\nTraining Accuracy: 0.9670\nValidation Accuracy: 0.8213\nPrecision: 0.8843, Recall: 0.8213\nEpoch [32/100], Step [1/31], Loss: 0.3623\nEpoch [32/100], Step [2/31], Loss: 0.3793\nEpoch [32/100], Step [3/31], Loss: 0.4085\nEpoch [32/100], Step [4/31], Loss: 0.3647\nEpoch [32/100], Step [5/31], Loss: 0.4070\nEpoch [32/100], Step [6/31], Loss: 0.4375\nEpoch [32/100], Step [7/31], Loss: 0.3583\nEpoch [32/100], Step [8/31], Loss: 0.3621\nEpoch [32/100], Step [9/31], Loss: 0.3715\nEpoch [32/100], Step [10/31], Loss: 0.4326\nEpoch [32/100], Step [11/31], Loss: 0.3581\nEpoch [32/100], Step [12/31], Loss: 0.3898\nEpoch [32/100], Step [13/31], Loss: 0.5061\nEpoch [32/100], Step [14/31], Loss: 0.3940\nEpoch [32/100], Step [15/31], Loss: 0.4622\nEpoch [32/100], Step [16/31], Loss: 0.3499\nEpoch [32/100], Step [17/31], Loss: 0.4301\nEpoch [32/100], Step [18/31], Loss: 0.4186\nEpoch [32/100], Step [19/31], Loss: 0.3482\nEpoch [32/100], Step [20/31], Loss: 0.3674\nEpoch [32/100], Step [21/31], Loss: 0.3771\nEpoch [32/100], Step [22/31], Loss: 0.4074\nEpoch [32/100], Step [23/31], Loss: 0.5017\nEpoch [32/100], Step [24/31], Loss: 0.3684\nEpoch [32/100], Step [25/31], Loss: 0.3752\nEpoch [32/100], Step [26/31], Loss: 0.3632\nEpoch [32/100], Step [27/31], Loss: 0.3479\nEpoch [32/100], Step [28/31], Loss: 0.3564\nEpoch [32/100], Step [29/31], Loss: 0.3821\nEpoch [32/100], Step [30/31], Loss: 0.3441\nEpoch [32/100], Step [31/31], Loss: 0.4741\nTraining Loss: 0.3920\nTraining Accuracy: 0.9876\nValidation Accuracy: 0.8836\nPrecision: 0.8960, Recall: 0.8836\nEpoch [33/100], Step [1/31], Loss: 0.3993\nEpoch [33/100], Step [2/31], Loss: 0.5038\nEpoch [33/100], Step [3/31], Loss: 0.3488\nEpoch [33/100], Step [4/31], Loss: 0.4298\nEpoch [33/100], Step [5/31], Loss: 0.4151\nEpoch [33/100], Step [6/31], Loss: 0.4235\nEpoch [33/100], Step [7/31], Loss: 0.3799\nEpoch [33/100], Step [8/31], Loss: 0.3811\nEpoch [33/100], Step [9/31], Loss: 0.3494\nEpoch [33/100], Step [10/31], Loss: 0.4221\nEpoch [33/100], Step [11/31], Loss: 0.3443\nEpoch [33/100], Step [12/31], Loss: 0.3492\nEpoch [33/100], Step [13/31], Loss: 0.3869\nEpoch [33/100], Step [14/31], Loss: 0.3657\nEpoch [33/100], Step [15/31], Loss: 0.3528\nEpoch [33/100], Step [16/31], Loss: 0.4968\nEpoch [33/100], Step [17/31], Loss: 0.4213\nEpoch [33/100], Step [18/31], Loss: 0.3745\nEpoch [33/100], Step [19/31], Loss: 0.3509\nEpoch [33/100], Step [20/31], Loss: 0.3475\nEpoch [33/100], Step [21/31], Loss: 0.3390\nEpoch [33/100], Step [22/31], Loss: 0.3590\nEpoch [33/100], Step [23/31], Loss: 0.4189\nEpoch [33/100], Step [24/31], Loss: 0.4481\nEpoch [33/100], Step [25/31], Loss: 0.3443\nEpoch [33/100], Step [26/31], Loss: 0.4613\nEpoch [33/100], Step [27/31], Loss: 0.3383\nEpoch [33/100], Step [28/31], Loss: 0.4979\nEpoch [33/100], Step [29/31], Loss: 0.3644\nEpoch [33/100], Step [30/31], Loss: 0.3636\nEpoch [33/100], Step [31/31], Loss: 0.8514\nTraining Loss: 0.3978\nTraining Accuracy: 0.9825\nValidation Accuracy: 0.8656\nPrecision: 0.8757, Recall: 0.8656\nEpoch [34/100], Step [1/31], Loss: 0.3580\nEpoch [34/100], Step [2/31], Loss: 0.3455\nEpoch [34/100], Step [3/31], Loss: 0.3578\nEpoch [34/100], Step [4/31], Loss: 0.3664\nEpoch [34/100], Step [5/31], Loss: 0.4798\nEpoch [34/100], Step [6/31], Loss: 0.4062\nEpoch [34/100], Step [7/31], Loss: 0.4102\nEpoch [34/100], Step [8/31], Loss: 0.4167\nEpoch [34/100], Step [9/31], Loss: 0.3508\nEpoch [34/100], Step [10/31], Loss: 0.4191\nEpoch [34/100], Step [11/31], Loss: 0.3549\nEpoch [34/100], Step [12/31], Loss: 0.3636\nEpoch [34/100], Step [13/31], Loss: 0.4163\nEpoch [34/100], Step [14/31], Loss: 0.4005\nEpoch [34/100], Step [15/31], Loss: 0.3360\nEpoch [34/100], Step [16/31], Loss: 0.3452\nEpoch [34/100], Step [17/31], Loss: 0.4773\nEpoch [34/100], Step [18/31], Loss: 0.3568\nEpoch [34/100], Step [19/31], Loss: 0.3574\nEpoch [34/100], Step [20/31], Loss: 0.3481\nEpoch [34/100], Step [21/31], Loss: 0.3546\nEpoch [34/100], Step [22/31], Loss: 0.4450\nEpoch [34/100], Step [23/31], Loss: 0.4323\nEpoch [34/100], Step [24/31], Loss: 0.3964\nEpoch [34/100], Step [25/31], Loss: 0.4137\nEpoch [34/100], Step [26/31], Loss: 0.4233\nEpoch [34/100], Step [27/31], Loss: 0.3601\nEpoch [34/100], Step [28/31], Loss: 0.4377\nEpoch [34/100], Step [29/31], Loss: 0.4169\nEpoch [34/100], Step [30/31], Loss: 0.3459\nEpoch [34/100], Step [31/31], Loss: 0.5987\nTraining Loss: 0.3921\nTraining Accuracy: 0.9815\nValidation Accuracy: 0.8902\nPrecision: 0.9044, Recall: 0.8902\nEpoch [35/100], Step [1/31], Loss: 0.3587\nEpoch [35/100], Step [2/31], Loss: 0.3521\nEpoch [35/100], Step [3/31], Loss: 0.4122\nEpoch [35/100], Step [4/31], Loss: 0.4384\nEpoch [35/100], Step [5/31], Loss: 0.3760\nEpoch [35/100], Step [6/31], Loss: 0.3762\nEpoch [35/100], Step [7/31], Loss: 0.4991\nEpoch [35/100], Step [8/31], Loss: 0.3635\nEpoch [35/100], Step [9/31], Loss: 0.3411\nEpoch [35/100], Step [10/31], Loss: 0.3495\nEpoch [35/100], Step [11/31], Loss: 0.3470\nEpoch [35/100], Step [12/31], Loss: 0.4345\nEpoch [35/100], Step [13/31], Loss: 0.3401\nEpoch [35/100], Step [14/31], Loss: 0.3708\nEpoch [35/100], Step [15/31], Loss: 0.3980\nEpoch [35/100], Step [16/31], Loss: 0.3565\nEpoch [35/100], Step [17/31], Loss: 0.3455\nEpoch [35/100], Step [18/31], Loss: 0.3563\nEpoch [35/100], Step [19/31], Loss: 0.4250\nEpoch [35/100], Step [20/31], Loss: 0.3664\nEpoch [35/100], Step [21/31], Loss: 0.6003\nEpoch [35/100], Step [22/31], Loss: 0.4372\nEpoch [35/100], Step [23/31], Loss: 0.3947\nEpoch [35/100], Step [24/31], Loss: 0.5317\nEpoch [35/100], Step [25/31], Loss: 0.3366\nEpoch [35/100], Step [26/31], Loss: 0.3557\nEpoch [35/100], Step [27/31], Loss: 0.3671\nEpoch [35/100], Step [28/31], Loss: 0.3693\nEpoch [35/100], Step [29/31], Loss: 0.3623\nEpoch [35/100], Step [30/31], Loss: 0.3917\nEpoch [35/100], Step [31/31], Loss: 1.1481\nTraining Loss: 0.4004\nTraining Accuracy: 0.9804\nValidation Accuracy: 0.8836\nPrecision: 0.8917, Recall: 0.8836\nEpoch [36/100], Step [1/31], Loss: 0.3994\nEpoch [36/100], Step [2/31], Loss: 0.4559\nEpoch [36/100], Step [3/31], Loss: 0.6891\nEpoch [36/100], Step [4/31], Loss: 0.9312\nEpoch [36/100], Step [5/31], Loss: 0.3969\nEpoch [36/100], Step [6/31], Loss: 0.3517\nEpoch [36/100], Step [7/31], Loss: 0.7044\nEpoch [36/100], Step [8/31], Loss: 0.5074\nEpoch [36/100], Step [9/31], Loss: 0.5516\nEpoch [36/100], Step [10/31], Loss: 0.5600\nEpoch [36/100], Step [11/31], Loss: 0.4347\nEpoch [36/100], Step [12/31], Loss: 0.3810\nEpoch [36/100], Step [13/31], Loss: 0.4391\nEpoch [36/100], Step [14/31], Loss: 0.3952\nEpoch [36/100], Step [15/31], Loss: 0.3767\nEpoch [36/100], Step [16/31], Loss: 0.3434\nEpoch [36/100], Step [17/31], Loss: 0.4275\nEpoch [36/100], Step [18/31], Loss: 0.4262\nEpoch [36/100], Step [19/31], Loss: 0.3802\nEpoch [36/100], Step [20/31], Loss: 0.4131\nEpoch [36/100], Step [21/31], Loss: 0.4570\nEpoch [36/100], Step [22/31], Loss: 0.4026\nEpoch [36/100], Step [23/31], Loss: 0.4047\nEpoch [36/100], Step [24/31], Loss: 0.4211\nEpoch [36/100], Step [25/31], Loss: 0.3942\nEpoch [36/100], Step [26/31], Loss: 0.4436\nEpoch [36/100], Step [27/31], Loss: 0.4075\nEpoch [36/100], Step [28/31], Loss: 0.3482\nEpoch [36/100], Step [29/31], Loss: 0.3983\nEpoch [36/100], Step [30/31], Loss: 0.3787\nEpoch [36/100], Step [31/31], Loss: 0.4132\nTraining Loss: 0.4536\nTraining Accuracy: 0.9578\nValidation Accuracy: 0.8787\nPrecision: 0.8866, Recall: 0.8787\nEpoch [37/100], Step [1/31], Loss: 0.4217\nEpoch [37/100], Step [2/31], Loss: 0.3656\nEpoch [37/100], Step [3/31], Loss: 0.3791\nEpoch [37/100], Step [4/31], Loss: 0.3915\nEpoch [37/100], Step [5/31], Loss: 0.3872\nEpoch [37/100], Step [6/31], Loss: 0.3533\nEpoch [37/100], Step [7/31], Loss: 0.3464\nEpoch [37/100], Step [8/31], Loss: 0.3991\nEpoch [37/100], Step [9/31], Loss: 0.3437\nEpoch [37/100], Step [10/31], Loss: 0.4944\nEpoch [37/100], Step [11/31], Loss: 0.3792\nEpoch [37/100], Step [12/31], Loss: 0.3678\nEpoch [37/100], Step [13/31], Loss: 0.3774\nEpoch [37/100], Step [14/31], Loss: 0.4028\nEpoch [37/100], Step [15/31], Loss: 0.5584\nEpoch [37/100], Step [16/31], Loss: 0.3483\nEpoch [37/100], Step [17/31], Loss: 0.3703\nEpoch [37/100], Step [18/31], Loss: 0.4732\nEpoch [37/100], Step [19/31], Loss: 0.3523\nEpoch [37/100], Step [20/31], Loss: 0.4291\nEpoch [37/100], Step [21/31], Loss: 0.3486\nEpoch [37/100], Step [22/31], Loss: 0.3459\nEpoch [37/100], Step [23/31], Loss: 0.3480\nEpoch [37/100], Step [24/31], Loss: 0.4001\nEpoch [37/100], Step [25/31], Loss: 0.3448\nEpoch [37/100], Step [26/31], Loss: 0.4478\nEpoch [37/100], Step [27/31], Loss: 0.4410\nEpoch [37/100], Step [28/31], Loss: 0.3470\nEpoch [37/100], Step [29/31], Loss: 0.3537\nEpoch [37/100], Step [30/31], Loss: 0.3440\nEpoch [37/100], Step [31/31], Loss: 0.3428\nTraining Loss: 0.3882\nTraining Accuracy: 0.9846\nValidation Accuracy: 0.8934\nPrecision: 0.9012, Recall: 0.8934\nEpoch [38/100], Step [1/31], Loss: 0.3562\nEpoch [38/100], Step [2/31], Loss: 0.3486\nEpoch [38/100], Step [3/31], Loss: 0.3354\nEpoch [38/100], Step [4/31], Loss: 0.3660\nEpoch [38/100], Step [5/31], Loss: 0.3637\nEpoch [38/100], Step [6/31], Loss: 0.4476\nEpoch [38/100], Step [7/31], Loss: 0.3615\nEpoch [38/100], Step [8/31], Loss: 0.3635\nEpoch [38/100], Step [9/31], Loss: 0.4194\nEpoch [38/100], Step [10/31], Loss: 0.3303\nEpoch [38/100], Step [11/31], Loss: 0.3392\nEpoch [38/100], Step [12/31], Loss: 0.3339\nEpoch [38/100], Step [13/31], Loss: 0.3568\nEpoch [38/100], Step [14/31], Loss: 0.3997\nEpoch [38/100], Step [15/31], Loss: 0.4015\nEpoch [38/100], Step [16/31], Loss: 0.3756\nEpoch [38/100], Step [17/31], Loss: 0.3336\nEpoch [38/100], Step [18/31], Loss: 0.3286\nEpoch [38/100], Step [19/31], Loss: 0.4225\nEpoch [38/100], Step [20/31], Loss: 0.3432\nEpoch [38/100], Step [21/31], Loss: 0.3329\nEpoch [38/100], Step [22/31], Loss: 0.3859\nEpoch [38/100], Step [23/31], Loss: 0.3350\nEpoch [38/100], Step [24/31], Loss: 0.3520\nEpoch [38/100], Step [25/31], Loss: 0.3426\nEpoch [38/100], Step [26/31], Loss: 0.3293\nEpoch [38/100], Step [27/31], Loss: 0.3272\nEpoch [38/100], Step [28/31], Loss: 0.3337\nEpoch [38/100], Step [29/31], Loss: 0.3754\nEpoch [38/100], Step [30/31], Loss: 0.4087\nEpoch [38/100], Step [31/31], Loss: 0.3322\nTraining Loss: 0.3613\nTraining Accuracy: 0.9876\nValidation Accuracy: 0.9164\nPrecision: 0.9205, Recall: 0.9164\nEpoch [39/100], Step [1/31], Loss: 0.4255\nEpoch [39/100], Step [2/31], Loss: 0.3621\nEpoch [39/100], Step [3/31], Loss: 0.3231\nEpoch [39/100], Step [4/31], Loss: 0.3247\nEpoch [39/100], Step [5/31], Loss: 0.3288\nEpoch [39/100], Step [6/31], Loss: 0.3251\nEpoch [39/100], Step [7/31], Loss: 0.3773\nEpoch [39/100], Step [8/31], Loss: 0.3367\nEpoch [39/100], Step [9/31], Loss: 0.3307\nEpoch [39/100], Step [10/31], Loss: 0.3469\nEpoch [39/100], Step [11/31], Loss: 0.5272\nEpoch [39/100], Step [12/31], Loss: 0.3261\nEpoch [39/100], Step [13/31], Loss: 0.3402\nEpoch [39/100], Step [14/31], Loss: 0.3190\nEpoch [39/100], Step [15/31], Loss: 0.3552\nEpoch [39/100], Step [16/31], Loss: 0.3346\nEpoch [39/100], Step [17/31], Loss: 0.3368\nEpoch [39/100], Step [18/31], Loss: 0.3344\nEpoch [39/100], Step [19/31], Loss: 0.3270\nEpoch [39/100], Step [20/31], Loss: 0.3218\nEpoch [39/100], Step [21/31], Loss: 0.3260\nEpoch [39/100], Step [22/31], Loss: 0.3427\nEpoch [39/100], Step [23/31], Loss: 0.3227\nEpoch [39/100], Step [24/31], Loss: 0.3805\nEpoch [39/100], Step [25/31], Loss: 0.3543\nEpoch [39/100], Step [26/31], Loss: 0.3207\nEpoch [39/100], Step [27/31], Loss: 0.3246\nEpoch [39/100], Step [28/31], Loss: 0.3890\nEpoch [39/100], Step [29/31], Loss: 0.3708\nEpoch [39/100], Step [30/31], Loss: 0.3708\nEpoch [39/100], Step [31/31], Loss: 0.5084\nTraining Loss: 0.3520\nTraining Accuracy: 0.9887\nValidation Accuracy: 0.9049\nPrecision: 0.9163, Recall: 0.9049\nEpoch [40/100], Step [1/31], Loss: 0.3140\nEpoch [40/100], Step [2/31], Loss: 0.3419\nEpoch [40/100], Step [3/31], Loss: 0.3521\nEpoch [40/100], Step [4/31], Loss: 0.3645\nEpoch [40/100], Step [5/31], Loss: 0.3774\nEpoch [40/100], Step [6/31], Loss: 0.3464\nEpoch [40/100], Step [7/31], Loss: 0.3605\nEpoch [40/100], Step [8/31], Loss: 0.3336\nEpoch [40/100], Step [9/31], Loss: 0.4411\nEpoch [40/100], Step [10/31], Loss: 0.3460\nEpoch [40/100], Step [11/31], Loss: 0.4140\nEpoch [40/100], Step [12/31], Loss: 0.3596\nEpoch [40/100], Step [13/31], Loss: 0.4654\nEpoch [40/100], Step [14/31], Loss: 0.3466\nEpoch [40/100], Step [15/31], Loss: 0.3390\nEpoch [40/100], Step [16/31], Loss: 0.3557\nEpoch [40/100], Step [17/31], Loss: 0.3518\nEpoch [40/100], Step [18/31], Loss: 0.3287\nEpoch [40/100], Step [19/31], Loss: 0.4047\nEpoch [40/100], Step [20/31], Loss: 0.4641\nEpoch [40/100], Step [21/31], Loss: 0.3370\nEpoch [40/100], Step [22/31], Loss: 0.3519\nEpoch [40/100], Step [23/31], Loss: 0.4134\nEpoch [40/100], Step [24/31], Loss: 0.5229\nEpoch [40/100], Step [25/31], Loss: 0.4124\nEpoch [40/100], Step [26/31], Loss: 0.4290\nEpoch [40/100], Step [27/31], Loss: 0.4281\nEpoch [40/100], Step [28/31], Loss: 0.3231\nEpoch [40/100], Step [29/31], Loss: 0.3500\nEpoch [40/100], Step [30/31], Loss: 0.3429\nEpoch [40/100], Step [31/31], Loss: 0.5715\nTraining Loss: 0.3795\nTraining Accuracy: 0.9804\nValidation Accuracy: 0.7934\nPrecision: 0.8137, Recall: 0.7934\nEpoch [41/100], Step [1/31], Loss: 0.3448\nEpoch [41/100], Step [2/31], Loss: 0.3367\nEpoch [41/100], Step [3/31], Loss: 0.4870\nEpoch [41/100], Step [4/31], Loss: 0.8164\nEpoch [41/100], Step [5/31], Loss: 0.7542\nEpoch [41/100], Step [6/31], Loss: 0.4297\nEpoch [41/100], Step [7/31], Loss: 0.4124\nEpoch [41/100], Step [8/31], Loss: 0.6794\nEpoch [41/100], Step [9/31], Loss: 0.5273\nEpoch [41/100], Step [10/31], Loss: 1.0276\nEpoch [41/100], Step [11/31], Loss: 0.4123\nEpoch [41/100], Step [12/31], Loss: 0.3771\nEpoch [41/100], Step [13/31], Loss: 0.4973\nEpoch [41/100], Step [14/31], Loss: 0.3664\nEpoch [41/100], Step [15/31], Loss: 0.6273\nEpoch [41/100], Step [16/31], Loss: 0.3514\nEpoch [41/100], Step [17/31], Loss: 0.4199\nEpoch [41/100], Step [18/31], Loss: 0.8620\nEpoch [41/100], Step [19/31], Loss: 0.4022\nEpoch [41/100], Step [20/31], Loss: 0.6608\nEpoch [41/100], Step [21/31], Loss: 0.6133\nEpoch [41/100], Step [22/31], Loss: 0.3605\nEpoch [41/100], Step [23/31], Loss: 0.4102\nEpoch [41/100], Step [24/31], Loss: 0.3608\nEpoch [41/100], Step [25/31], Loss: 0.4484\nEpoch [41/100], Step [26/31], Loss: 0.4171\nEpoch [41/100], Step [27/31], Loss: 0.5377\nEpoch [41/100], Step [28/31], Loss: 0.6020\nEpoch [41/100], Step [29/31], Loss: 0.7011\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 34\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}