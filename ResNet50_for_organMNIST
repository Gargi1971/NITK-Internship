{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install medmnist","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:35:28.035430Z","iopub.execute_input":"2024-04-23T14:35:28.036360Z","iopub.status.idle":"2024-04-23T14:35:40.727901Z","shell.execute_reply.started":"2024-04-23T14:35:28.036322Z","shell.execute_reply":"2024-04-23T14:35:40.726792Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: medmnist in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from medmnist) (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from medmnist) (2.1.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from medmnist) (1.2.2)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from medmnist) (0.22.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from medmnist) (4.66.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from medmnist) (9.5.0)\nRequirement already satisfied: fire in /opt/conda/lib/python3.10/site-packages (from medmnist) (0.6.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from medmnist) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from medmnist) (0.16.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->medmnist) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->medmnist) (2.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->medmnist) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->medmnist) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->medmnist) (2023.4)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (1.11.4)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (3.2.1)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (2023.12.9)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (21.3)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->medmnist) (0.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->medmnist) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->medmnist) (3.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->medmnist) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->medmnist) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->medmnist) (1.12)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->medmnist) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->medmnist) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->medmnist) (2.31.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->medmnist) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->medmnist) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->medmnist) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->medmnist) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->medmnist) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->medmnist) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->medmnist) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import medmnist\nprint(medmnist.__version__)\n!python -m medmnist available","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:35:40.730116Z","iopub.execute_input":"2024-04-23T14:35:40.730451Z","iopub.status.idle":"2024-04-23T14:35:47.823538Z","shell.execute_reply.started":"2024-04-23T14:35:40.730420Z","shell.execute_reply":"2024-04-23T14:35:47.822123Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"3.0.1\nMedMNIST v3.0.1 @ https://github.com/MedMNIST/MedMNIST/\nAll available datasets:\n\tpathmnist       | PathMNIST       | Size: 28 (default), 64, 128, 224.\n\tchestmnist      | ChestMNIST      | Size: 28 (default), 64, 128, 224.\n\tdermamnist      | DermaMNIST      | Size: 28 (default), 64, 128, 224.\n\toctmnist        | OCTMNIST        | Size: 28 (default), 64, 128, 224.\n\tpneumoniamnist  | PneumoniaMNIST  | Size: 28 (default), 64, 128, 224.\n\tretinamnist     | RetinaMNIST     | Size: 28 (default), 64, 128, 224.\n\tbreastmnist     | BreastMNIST     | Size: 28 (default), 64, 128, 224.\n\tbloodmnist      | BloodMNIST      | Size: 28 (default), 64, 128, 224.\n\ttissuemnist     | TissueMNIST     | Size: 28 (default), 64, 128, 224.\n\torganamnist     | OrganAMNIST     | Size: 28 (default), 64, 128, 224.\n\torgancmnist     | OrganCMNIST     | Size: 28 (default), 64, 128, 224.\n\torgansmnist     | OrganSMNIST     | Size: 28 (default), 64, 128, 224.\n\torganmnist3d    | OrganMNIST3D    | Size: 28 (default), 64.\n\tnodulemnist3d   | NoduleMNIST3D   | Size: 28 (default), 64.\n\tadrenalmnist3d  | AdrenalMNIST3D  | Size: 28 (default), 64.\n\tfracturemnist3d | FractureMNIST3D | Size: 28 (default), 64.\n\tvesselmnist3d   | VesselMNIST3D   | Size: 28 (default), 64.\n\tsynapsemnist3d  | SynapseMNIST3D  | Size: 28 (default), 64.\n","output_type":"stream"}]},{"cell_type":"code","source":"from medmnist import OrganMNIST3D\ntrain_dataset = OrganMNIST3D(download=True, split= 'train')\ntest_dataset = OrganMNIST3D(download=True, split= 'test')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:35:47.825606Z","iopub.execute_input":"2024-04-23T14:35:47.826593Z","iopub.status.idle":"2024-04-23T14:35:49.858030Z","shell.execute_reply.started":"2024-04-23T14:35:47.826542Z","shell.execute_reply":"2024-04-23T14:35:49.856994Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Using downloaded and verified file: /root/.medmnist/organmnist3d.npz\nUsing downloaded and verified file: /root/.medmnist/organmnist3d.npz\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv3d(\n            in_planes, planes, kernel_size=(3, 3, 3), stride=stride, padding=(1, 1, 1), bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=(3, 3, 3),\n                               stride=1, padding=(1, 1, 1), bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv3d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm3d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv3d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(planes)\n        self.conv2 = nn.Conv3d(planes, planes, kernel_size=(3, 3, 3),\n                               stride=stride, padding=(1, 1, 1), bias=False)\n        self.bn2 = nn.BatchNorm3d(planes)\n        self.conv3 = nn.Conv3d(planes, self.expansion *\n                               planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm3d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv3d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm3d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, in_channels=1, num_classes=2):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv3d(in_channels, 64, kernel_size=(3, 3, 3),\n                               stride=1, padding=(1, 1, 1), bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet50(in_channels, num_classes):\n    return ResNet(Bottleneck, [3, 4, 6, 3], in_channels=in_channels, num_classes=num_classes)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:35:49.859468Z","iopub.execute_input":"2024-04-23T14:35:49.859941Z","iopub.status.idle":"2024-04-23T14:35:49.884796Z","shell.execute_reply.started":"2024-04-23T14:35:49.859914Z","shell.execute_reply":"2024-04-23T14:35:49.883845Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:35:49.887091Z","iopub.execute_input":"2024-04-23T14:35:49.887440Z","iopub.status.idle":"2024-04-23T14:35:49.899067Z","shell.execute_reply.started":"2024-04-23T14:35:49.887416Z","shell.execute_reply":"2024-04-23T14:35:49.898096Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import precision_score, recall_score\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResNet50(in_channels=1, num_classes=11).to(torch.float64).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\nnum_epochs = 60\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to train mode\n    running_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n    for i, (inputs, labels) in enumerate (train_loader):\n        inputs, labels = inputs.to(device), labels.to(device).view(-1)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        reg = sum(torch.sum(p**2) for p in model.parameters() if p.requires_grad)\n        lambda_ = 0.0001\n        loss = criterion(outputs, labels) + (lambda_ / 2) * reg\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n\n        _, predicted = torch.max(outputs, 1)\n        total_correct += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n    epoch_loss = running_loss / len(train_dataset)\n    print(f\"Training Loss: {epoch_loss:.4f}\")\n\n    training_accuracy = total_correct / total_samples\n    print(f\"Training Accuracy: {training_accuracy:.4f}\")\n\n    # Validation loop\n    model.eval()  # Set the model to evaluation mode\n    correct = 0\n    total = 0\n    predicted_labels = []\n    true_labels = []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device).view(-1)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            predicted_labels.extend(predicted.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n    val_acc = correct / total\n    print(f\"Validation Accuracy: {val_acc:.4f}\")\n\n    precision = precision_score(true_labels, predicted_labels, average='weighted')\n    recall = recall_score(true_labels, predicted_labels, average='weighted')\n    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:35:49.900379Z","iopub.execute_input":"2024-04-23T14:35:49.900667Z","iopub.status.idle":"2024-04-24T01:00:46.730784Z","shell.execute_reply.started":"2024-04-23T14:35:49.900644Z","shell.execute_reply":"2024-04-24T01:00:46.729510Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch [1/60], Step [1/61], Loss: 4.3342\nEpoch [1/60], Step [2/61], Loss: 6.5399\nEpoch [1/60], Step [3/61], Loss: 4.9612\nEpoch [1/60], Step [4/61], Loss: 4.7994\nEpoch [1/60], Step [5/61], Loss: 6.7830\nEpoch [1/60], Step [6/61], Loss: 6.4520\nEpoch [1/60], Step [7/61], Loss: 3.7874\nEpoch [1/60], Step [8/61], Loss: 4.1296\nEpoch [1/60], Step [9/61], Loss: 4.4160\nEpoch [1/60], Step [10/61], Loss: 4.4353\nEpoch [1/60], Step [11/61], Loss: 4.7293\nEpoch [1/60], Step [12/61], Loss: 4.2695\nEpoch [1/60], Step [13/61], Loss: 6.6122\nEpoch [1/60], Step [14/61], Loss: 3.9874\nEpoch [1/60], Step [15/61], Loss: 4.1692\nEpoch [1/60], Step [16/61], Loss: 4.2200\nEpoch [1/60], Step [17/61], Loss: 3.9805\nEpoch [1/60], Step [18/61], Loss: 4.2720\nEpoch [1/60], Step [19/61], Loss: 3.9940\nEpoch [1/60], Step [20/61], Loss: 4.3074\nEpoch [1/60], Step [21/61], Loss: 3.6619\nEpoch [1/60], Step [22/61], Loss: 3.7286\nEpoch [1/60], Step [23/61], Loss: 3.9250\nEpoch [1/60], Step [24/61], Loss: 4.1647\nEpoch [1/60], Step [25/61], Loss: 4.3839\nEpoch [1/60], Step [26/61], Loss: 3.6988\nEpoch [1/60], Step [27/61], Loss: 3.4745\nEpoch [1/60], Step [28/61], Loss: 3.5358\nEpoch [1/60], Step [29/61], Loss: 3.5719\nEpoch [1/60], Step [30/61], Loss: 3.7912\nEpoch [1/60], Step [31/61], Loss: 3.5444\nEpoch [1/60], Step [32/61], Loss: 3.7231\nEpoch [1/60], Step [33/61], Loss: 3.6985\nEpoch [1/60], Step [34/61], Loss: 3.5969\nEpoch [1/60], Step [35/61], Loss: 3.4845\nEpoch [1/60], Step [36/61], Loss: 3.7094\nEpoch [1/60], Step [37/61], Loss: 3.7571\nEpoch [1/60], Step [38/61], Loss: 3.6332\nEpoch [1/60], Step [39/61], Loss: 3.8414\nEpoch [1/60], Step [40/61], Loss: 3.4587\nEpoch [1/60], Step [41/61], Loss: 3.4190\nEpoch [1/60], Step [42/61], Loss: 3.2498\nEpoch [1/60], Step [43/61], Loss: 3.3881\nEpoch [1/60], Step [44/61], Loss: 3.5623\nEpoch [1/60], Step [45/61], Loss: 3.7399\nEpoch [1/60], Step [46/61], Loss: 5.8340\nEpoch [1/60], Step [47/61], Loss: 3.3901\nEpoch [1/60], Step [48/61], Loss: 3.2875\nEpoch [1/60], Step [49/61], Loss: 3.5975\nEpoch [1/60], Step [50/61], Loss: 2.9732\nEpoch [1/60], Step [51/61], Loss: 3.4268\nEpoch [1/60], Step [52/61], Loss: 3.4623\nEpoch [1/60], Step [53/61], Loss: 3.7179\nEpoch [1/60], Step [54/61], Loss: 3.5779\nEpoch [1/60], Step [55/61], Loss: 4.2584\nEpoch [1/60], Step [56/61], Loss: 3.2109\nEpoch [1/60], Step [57/61], Loss: 3.4094\nEpoch [1/60], Step [58/61], Loss: 3.3813\nEpoch [1/60], Step [59/61], Loss: 3.3569\nEpoch [1/60], Step [60/61], Loss: 3.3923\nEpoch [1/60], Step [61/61], Loss: 3.9332\nTraining Loss: 4.0190\nTraining Accuracy: 0.2915\nValidation Accuracy: 0.3262\nPrecision: 0.2224, Recall: 0.3262\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/60], Step [1/61], Loss: 3.3217\nEpoch [2/60], Step [2/61], Loss: 3.2711\nEpoch [2/60], Step [3/61], Loss: 3.1862\nEpoch [2/60], Step [4/61], Loss: 3.6011\nEpoch [2/60], Step [5/61], Loss: 4.5892\nEpoch [2/60], Step [6/61], Loss: 3.2988\nEpoch [2/60], Step [7/61], Loss: 3.3367\nEpoch [2/60], Step [8/61], Loss: 3.9913\nEpoch [2/60], Step [9/61], Loss: 3.2936\nEpoch [2/60], Step [10/61], Loss: 3.3764\nEpoch [2/60], Step [11/61], Loss: 3.6376\nEpoch [2/60], Step [12/61], Loss: 3.7944\nEpoch [2/60], Step [13/61], Loss: 3.1605\nEpoch [2/60], Step [14/61], Loss: 3.3969\nEpoch [2/60], Step [15/61], Loss: 3.1150\nEpoch [2/60], Step [16/61], Loss: 3.3345\nEpoch [2/60], Step [17/61], Loss: 2.9200\nEpoch [2/60], Step [18/61], Loss: 3.0092\nEpoch [2/60], Step [19/61], Loss: 4.2575\nEpoch [2/60], Step [20/61], Loss: 2.9254\nEpoch [2/60], Step [21/61], Loss: 3.6668\nEpoch [2/60], Step [22/61], Loss: 3.1518\nEpoch [2/60], Step [23/61], Loss: 3.2119\nEpoch [2/60], Step [24/61], Loss: 3.4148\nEpoch [2/60], Step [25/61], Loss: 3.1132\nEpoch [2/60], Step [26/61], Loss: 2.9460\nEpoch [2/60], Step [27/61], Loss: 2.8487\nEpoch [2/60], Step [28/61], Loss: 3.0473\nEpoch [2/60], Step [29/61], Loss: 2.7412\nEpoch [2/60], Step [30/61], Loss: 2.7552\nEpoch [2/60], Step [31/61], Loss: 2.7423\nEpoch [2/60], Step [32/61], Loss: 2.9771\nEpoch [2/60], Step [33/61], Loss: 2.9774\nEpoch [2/60], Step [34/61], Loss: 3.0253\nEpoch [2/60], Step [35/61], Loss: 3.6651\nEpoch [2/60], Step [36/61], Loss: 2.7535\nEpoch [2/60], Step [37/61], Loss: 3.2320\nEpoch [2/60], Step [38/61], Loss: 2.9455\nEpoch [2/60], Step [39/61], Loss: 3.5751\nEpoch [2/60], Step [40/61], Loss: 2.5101\nEpoch [2/60], Step [41/61], Loss: 4.3800\nEpoch [2/60], Step [42/61], Loss: 3.2918\nEpoch [2/60], Step [43/61], Loss: 2.9674\nEpoch [2/60], Step [44/61], Loss: 3.7673\nEpoch [2/60], Step [45/61], Loss: 2.7838\nEpoch [2/60], Step [46/61], Loss: 2.9191\nEpoch [2/60], Step [47/61], Loss: 2.8176\nEpoch [2/60], Step [48/61], Loss: 2.8458\nEpoch [2/60], Step [49/61], Loss: 3.5233\nEpoch [2/60], Step [50/61], Loss: 2.8130\nEpoch [2/60], Step [51/61], Loss: 2.5673\nEpoch [2/60], Step [52/61], Loss: 2.9158\nEpoch [2/60], Step [53/61], Loss: 3.9589\nEpoch [2/60], Step [54/61], Loss: 3.1181\nEpoch [2/60], Step [55/61], Loss: 3.0517\nEpoch [2/60], Step [56/61], Loss: 2.3741\nEpoch [2/60], Step [57/61], Loss: 2.5057\nEpoch [2/60], Step [58/61], Loss: 2.7559\nEpoch [2/60], Step [59/61], Loss: 2.5233\nEpoch [2/60], Step [60/61], Loss: 3.9495\nEpoch [2/60], Step [61/61], Loss: 3.1415\nTraining Loss: 3.1985\nTraining Accuracy: 0.5026\nValidation Accuracy: 0.4820\nPrecision: 0.5278, Recall: 0.4820\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/60], Step [1/61], Loss: 3.2005\nEpoch [3/60], Step [2/61], Loss: 2.7896\nEpoch [3/60], Step [3/61], Loss: 2.3693\nEpoch [3/60], Step [4/61], Loss: 2.8143\nEpoch [3/60], Step [5/61], Loss: 3.2227\nEpoch [3/60], Step [6/61], Loss: 2.5451\nEpoch [3/60], Step [7/61], Loss: 2.4570\nEpoch [3/60], Step [8/61], Loss: 2.7160\nEpoch [3/60], Step [9/61], Loss: 3.1127\nEpoch [3/60], Step [10/61], Loss: 2.5730\nEpoch [3/60], Step [11/61], Loss: 2.9284\nEpoch [3/60], Step [12/61], Loss: 3.4525\nEpoch [3/60], Step [13/61], Loss: 2.7426\nEpoch [3/60], Step [14/61], Loss: 2.6600\nEpoch [3/60], Step [15/61], Loss: 3.2248\nEpoch [3/60], Step [16/61], Loss: 2.8642\nEpoch [3/60], Step [17/61], Loss: 2.5711\nEpoch [3/60], Step [18/61], Loss: 2.5703\nEpoch [3/60], Step [19/61], Loss: 2.5934\nEpoch [3/60], Step [20/61], Loss: 2.6364\nEpoch [3/60], Step [21/61], Loss: 2.4778\nEpoch [3/60], Step [22/61], Loss: 2.7946\nEpoch [3/60], Step [23/61], Loss: 3.4525\nEpoch [3/60], Step [24/61], Loss: 2.2125\nEpoch [3/60], Step [25/61], Loss: 3.4244\nEpoch [3/60], Step [26/61], Loss: 2.6426\nEpoch [3/60], Step [27/61], Loss: 2.9875\nEpoch [3/60], Step [28/61], Loss: 2.7822\nEpoch [3/60], Step [29/61], Loss: 2.4925\nEpoch [3/60], Step [30/61], Loss: 2.1248\nEpoch [3/60], Step [31/61], Loss: 2.5585\nEpoch [3/60], Step [32/61], Loss: 2.6887\nEpoch [3/60], Step [33/61], Loss: 3.2341\nEpoch [3/60], Step [34/61], Loss: 2.3708\nEpoch [3/60], Step [35/61], Loss: 2.3116\nEpoch [3/60], Step [36/61], Loss: 2.8376\nEpoch [3/60], Step [37/61], Loss: 2.6760\nEpoch [3/60], Step [38/61], Loss: 2.3358\nEpoch [3/60], Step [39/61], Loss: 2.7094\nEpoch [3/60], Step [40/61], Loss: 2.2499\nEpoch [3/60], Step [41/61], Loss: 2.4431\nEpoch [3/60], Step [42/61], Loss: 2.1791\nEpoch [3/60], Step [43/61], Loss: 2.6762\nEpoch [3/60], Step [44/61], Loss: 3.0823\nEpoch [3/60], Step [45/61], Loss: 2.4993\nEpoch [3/60], Step [46/61], Loss: 2.3554\nEpoch [3/60], Step [47/61], Loss: 2.4393\nEpoch [3/60], Step [48/61], Loss: 2.2814\nEpoch [3/60], Step [49/61], Loss: 2.8122\nEpoch [3/60], Step [50/61], Loss: 2.5195\nEpoch [3/60], Step [51/61], Loss: 2.4604\nEpoch [3/60], Step [52/61], Loss: 2.7202\nEpoch [3/60], Step [53/61], Loss: 2.7525\nEpoch [3/60], Step [54/61], Loss: 2.7175\nEpoch [3/60], Step [55/61], Loss: 2.3852\nEpoch [3/60], Step [56/61], Loss: 2.3026\nEpoch [3/60], Step [57/61], Loss: 2.4826\nEpoch [3/60], Step [58/61], Loss: 2.2659\nEpoch [3/60], Step [59/61], Loss: 2.4803\nEpoch [3/60], Step [60/61], Loss: 2.3099\nEpoch [3/60], Step [61/61], Loss: 2.4599\nTraining Loss: 2.6573\nTraining Accuracy: 0.6869\nValidation Accuracy: 0.7344\nPrecision: 0.7489, Recall: 0.7344\nEpoch [4/60], Step [1/61], Loss: 2.6670\nEpoch [4/60], Step [2/61], Loss: 2.3165\nEpoch [4/60], Step [3/61], Loss: 2.3628\nEpoch [4/60], Step [4/61], Loss: 2.2672\nEpoch [4/60], Step [5/61], Loss: 2.0892\nEpoch [4/60], Step [6/61], Loss: 1.9591\nEpoch [4/60], Step [7/61], Loss: 2.0779\nEpoch [4/60], Step [8/61], Loss: 2.0568\nEpoch [4/60], Step [9/61], Loss: 2.2869\nEpoch [4/60], Step [10/61], Loss: 2.1033\nEpoch [4/60], Step [11/61], Loss: 2.2035\nEpoch [4/60], Step [12/61], Loss: 2.1353\nEpoch [4/60], Step [13/61], Loss: 2.7150\nEpoch [4/60], Step [14/61], Loss: 2.0736\nEpoch [4/60], Step [15/61], Loss: 2.0310\nEpoch [4/60], Step [16/61], Loss: 2.1690\nEpoch [4/60], Step [17/61], Loss: 2.1314\nEpoch [4/60], Step [18/61], Loss: 2.3102\nEpoch [4/60], Step [19/61], Loss: 2.7247\nEpoch [4/60], Step [20/61], Loss: 2.5246\nEpoch [4/60], Step [21/61], Loss: 2.4088\nEpoch [4/60], Step [22/61], Loss: 2.2899\nEpoch [4/60], Step [23/61], Loss: 2.0525\nEpoch [4/60], Step [24/61], Loss: 2.0813\nEpoch [4/60], Step [25/61], Loss: 2.6729\nEpoch [4/60], Step [26/61], Loss: 2.1816\nEpoch [4/60], Step [27/61], Loss: 3.0832\nEpoch [4/60], Step [28/61], Loss: 2.4101\nEpoch [4/60], Step [29/61], Loss: 2.2948\nEpoch [4/60], Step [30/61], Loss: 2.6169\nEpoch [4/60], Step [31/61], Loss: 2.0763\nEpoch [4/60], Step [32/61], Loss: 2.0137\nEpoch [4/60], Step [33/61], Loss: 2.4295\nEpoch [4/60], Step [34/61], Loss: 2.5277\nEpoch [4/60], Step [35/61], Loss: 2.2441\nEpoch [4/60], Step [36/61], Loss: 2.2996\nEpoch [4/60], Step [37/61], Loss: 2.6277\nEpoch [4/60], Step [38/61], Loss: 2.9308\nEpoch [4/60], Step [39/61], Loss: 2.2786\nEpoch [4/60], Step [40/61], Loss: 2.2226\nEpoch [4/60], Step [41/61], Loss: 2.0560\nEpoch [4/60], Step [42/61], Loss: 2.3111\nEpoch [4/60], Step [43/61], Loss: 2.6189\nEpoch [4/60], Step [44/61], Loss: 2.0252\nEpoch [4/60], Step [45/61], Loss: 1.9737\nEpoch [4/60], Step [46/61], Loss: 2.7396\nEpoch [4/60], Step [47/61], Loss: 2.3863\nEpoch [4/60], Step [48/61], Loss: 2.7866\nEpoch [4/60], Step [49/61], Loss: 2.3781\nEpoch [4/60], Step [50/61], Loss: 2.5974\nEpoch [4/60], Step [51/61], Loss: 2.4830\nEpoch [4/60], Step [52/61], Loss: 2.1861\nEpoch [4/60], Step [53/61], Loss: 2.3279\nEpoch [4/60], Step [54/61], Loss: 2.1496\nEpoch [4/60], Step [55/61], Loss: 2.0281\nEpoch [4/60], Step [56/61], Loss: 2.7412\nEpoch [4/60], Step [57/61], Loss: 2.0058\nEpoch [4/60], Step [58/61], Loss: 2.5084\nEpoch [4/60], Step [59/61], Loss: 2.3728\nEpoch [4/60], Step [60/61], Loss: 2.1195\nEpoch [4/60], Step [61/61], Loss: 2.4466\nTraining Loss: 2.3304\nTraining Accuracy: 0.7745\nValidation Accuracy: 0.7770\nPrecision: 0.7973, Recall: 0.7770\nEpoch [5/60], Step [1/61], Loss: 2.2008\nEpoch [5/60], Step [2/61], Loss: 2.3932\nEpoch [5/60], Step [3/61], Loss: 2.2001\nEpoch [5/60], Step [4/61], Loss: 2.3495\nEpoch [5/60], Step [5/61], Loss: 2.0590\nEpoch [5/60], Step [6/61], Loss: 2.1728\nEpoch [5/60], Step [7/61], Loss: 2.0240\nEpoch [5/60], Step [8/61], Loss: 2.0565\nEpoch [5/60], Step [9/61], Loss: 1.9976\nEpoch [5/60], Step [10/61], Loss: 2.1749\nEpoch [5/60], Step [11/61], Loss: 2.1512\nEpoch [5/60], Step [12/61], Loss: 2.1733\nEpoch [5/60], Step [13/61], Loss: 2.3973\nEpoch [5/60], Step [14/61], Loss: 1.8605\nEpoch [5/60], Step [15/61], Loss: 2.0068\nEpoch [5/60], Step [16/61], Loss: 2.5235\nEpoch [5/60], Step [17/61], Loss: 3.0606\nEpoch [5/60], Step [18/61], Loss: 3.3187\nEpoch [5/60], Step [19/61], Loss: 2.4348\nEpoch [5/60], Step [20/61], Loss: 1.9423\nEpoch [5/60], Step [21/61], Loss: 2.3184\nEpoch [5/60], Step [22/61], Loss: 2.0676\nEpoch [5/60], Step [23/61], Loss: 2.2048\nEpoch [5/60], Step [24/61], Loss: 2.0426\nEpoch [5/60], Step [25/61], Loss: 2.2956\nEpoch [5/60], Step [26/61], Loss: 3.1265\nEpoch [5/60], Step [27/61], Loss: 2.1065\nEpoch [5/60], Step [28/61], Loss: 1.9959\nEpoch [5/60], Step [29/61], Loss: 2.6041\nEpoch [5/60], Step [30/61], Loss: 2.5393\nEpoch [5/60], Step [31/61], Loss: 2.1403\nEpoch [5/60], Step [32/61], Loss: 2.1765\nEpoch [5/60], Step [33/61], Loss: 1.9535\nEpoch [5/60], Step [34/61], Loss: 2.2953\nEpoch [5/60], Step [35/61], Loss: 2.1957\nEpoch [5/60], Step [36/61], Loss: 1.9618\nEpoch [5/60], Step [37/61], Loss: 2.0794\nEpoch [5/60], Step [38/61], Loss: 2.1633\nEpoch [5/60], Step [39/61], Loss: 1.9370\nEpoch [5/60], Step [40/61], Loss: 2.0244\nEpoch [5/60], Step [41/61], Loss: 1.9287\nEpoch [5/60], Step [42/61], Loss: 2.0398\nEpoch [5/60], Step [43/61], Loss: 2.3507\nEpoch [5/60], Step [44/61], Loss: 2.2801\nEpoch [5/60], Step [45/61], Loss: 2.8500\nEpoch [5/60], Step [46/61], Loss: 3.4771\nEpoch [5/60], Step [47/61], Loss: 2.0580\nEpoch [5/60], Step [48/61], Loss: 1.9987\nEpoch [5/60], Step [49/61], Loss: 2.4030\nEpoch [5/60], Step [50/61], Loss: 1.8498\nEpoch [5/60], Step [51/61], Loss: 2.2410\nEpoch [5/60], Step [52/61], Loss: 2.0479\nEpoch [5/60], Step [53/61], Loss: 2.1306\nEpoch [5/60], Step [54/61], Loss: 1.8372\nEpoch [5/60], Step [55/61], Loss: 2.1113\nEpoch [5/60], Step [56/61], Loss: 1.8727\nEpoch [5/60], Step [57/61], Loss: 2.1499\nEpoch [5/60], Step [58/61], Loss: 2.1277\nEpoch [5/60], Step [59/61], Loss: 2.0723\nEpoch [5/60], Step [60/61], Loss: 2.1273\nEpoch [5/60], Step [61/61], Loss: 1.7930\nTraining Loss: 2.2231\nTraining Accuracy: 0.8187\nValidation Accuracy: 0.7295\nPrecision: 0.7927, Recall: 0.7295\nEpoch [6/60], Step [1/61], Loss: 2.2421\nEpoch [6/60], Step [2/61], Loss: 1.9967\nEpoch [6/60], Step [3/61], Loss: 2.7577\nEpoch [6/60], Step [4/61], Loss: 2.3523\nEpoch [6/60], Step [5/61], Loss: 1.8083\nEpoch [6/60], Step [6/61], Loss: 1.9686\nEpoch [6/60], Step [7/61], Loss: 1.9784\nEpoch [6/60], Step [8/61], Loss: 2.2804\nEpoch [6/60], Step [9/61], Loss: 2.2193\nEpoch [6/60], Step [10/61], Loss: 2.8174\nEpoch [6/60], Step [11/61], Loss: 1.7587\nEpoch [6/60], Step [12/61], Loss: 1.8061\nEpoch [6/60], Step [13/61], Loss: 1.9602\nEpoch [6/60], Step [14/61], Loss: 1.7522\nEpoch [6/60], Step [15/61], Loss: 1.9723\nEpoch [6/60], Step [16/61], Loss: 2.3956\nEpoch [6/60], Step [17/61], Loss: 2.1601\nEpoch [6/60], Step [18/61], Loss: 2.1764\nEpoch [6/60], Step [19/61], Loss: 2.1585\nEpoch [6/60], Step [20/61], Loss: 2.1947\nEpoch [6/60], Step [21/61], Loss: 2.0645\nEpoch [6/60], Step [22/61], Loss: 1.9301\nEpoch [6/60], Step [23/61], Loss: 1.8940\nEpoch [6/60], Step [24/61], Loss: 2.2354\nEpoch [6/60], Step [25/61], Loss: 2.1970\nEpoch [6/60], Step [26/61], Loss: 2.4091\nEpoch [6/60], Step [27/61], Loss: 2.1268\nEpoch [6/60], Step [28/61], Loss: 2.2511\nEpoch [6/60], Step [29/61], Loss: 2.7024\nEpoch [6/60], Step [30/61], Loss: 2.2008\nEpoch [6/60], Step [31/61], Loss: 2.1787\nEpoch [6/60], Step [32/61], Loss: 2.6276\nEpoch [6/60], Step [33/61], Loss: 2.1055\nEpoch [6/60], Step [34/61], Loss: 2.6531\nEpoch [6/60], Step [35/61], Loss: 2.1624\nEpoch [6/60], Step [36/61], Loss: 1.7850\nEpoch [6/60], Step [37/61], Loss: 1.9932\nEpoch [6/60], Step [38/61], Loss: 1.7126\nEpoch [6/60], Step [39/61], Loss: 1.9654\nEpoch [6/60], Step [40/61], Loss: 2.2232\nEpoch [6/60], Step [41/61], Loss: 1.8998\nEpoch [6/60], Step [42/61], Loss: 2.0157\nEpoch [6/60], Step [43/61], Loss: 1.8463\nEpoch [6/60], Step [44/61], Loss: 1.8411\nEpoch [6/60], Step [45/61], Loss: 2.4719\nEpoch [6/60], Step [46/61], Loss: 1.9628\nEpoch [6/60], Step [47/61], Loss: 1.9642\nEpoch [6/60], Step [48/61], Loss: 2.2124\nEpoch [6/60], Step [49/61], Loss: 1.7958\nEpoch [6/60], Step [50/61], Loss: 1.8743\nEpoch [6/60], Step [51/61], Loss: 2.4044\nEpoch [6/60], Step [52/61], Loss: 2.4614\nEpoch [6/60], Step [53/61], Loss: 2.3663\nEpoch [6/60], Step [54/61], Loss: 1.9189\nEpoch [6/60], Step [55/61], Loss: 2.5856\nEpoch [6/60], Step [56/61], Loss: 2.5199\nEpoch [6/60], Step [57/61], Loss: 1.7214\nEpoch [6/60], Step [58/61], Loss: 1.7213\nEpoch [6/60], Step [59/61], Loss: 2.1293\nEpoch [6/60], Step [60/61], Loss: 2.2251\nEpoch [6/60], Step [61/61], Loss: 2.8542\nTraining Loss: 2.1433\nTraining Accuracy: 0.8229\nValidation Accuracy: 0.6902\nPrecision: 0.7808, Recall: 0.6902\nEpoch [7/60], Step [1/61], Loss: 2.4510\nEpoch [7/60], Step [2/61], Loss: 2.0526\nEpoch [7/60], Step [3/61], Loss: 1.9399\nEpoch [7/60], Step [4/61], Loss: 2.1914\nEpoch [7/60], Step [5/61], Loss: 1.8389\nEpoch [7/60], Step [6/61], Loss: 1.9889\nEpoch [7/60], Step [7/61], Loss: 1.9610\nEpoch [7/60], Step [8/61], Loss: 1.8447\nEpoch [7/60], Step [9/61], Loss: 1.9382\nEpoch [7/60], Step [10/61], Loss: 2.1441\nEpoch [7/60], Step [11/61], Loss: 1.7959\nEpoch [7/60], Step [12/61], Loss: 2.0978\nEpoch [7/60], Step [13/61], Loss: 2.1074\nEpoch [7/60], Step [14/61], Loss: 2.4472\nEpoch [7/60], Step [15/61], Loss: 2.4084\nEpoch [7/60], Step [16/61], Loss: 1.8341\nEpoch [7/60], Step [17/61], Loss: 2.4052\nEpoch [7/60], Step [18/61], Loss: 2.2022\nEpoch [7/60], Step [19/61], Loss: 2.1454\nEpoch [7/60], Step [20/61], Loss: 1.9986\nEpoch [7/60], Step [21/61], Loss: 2.2333\nEpoch [7/60], Step [22/61], Loss: 1.8333\nEpoch [7/60], Step [23/61], Loss: 2.0084\nEpoch [7/60], Step [24/61], Loss: 1.9555\nEpoch [7/60], Step [25/61], Loss: 2.0292\nEpoch [7/60], Step [26/61], Loss: 2.0828\nEpoch [7/60], Step [27/61], Loss: 2.6068\nEpoch [7/60], Step [28/61], Loss: 1.9925\nEpoch [7/60], Step [29/61], Loss: 2.0953\nEpoch [7/60], Step [30/61], Loss: 2.4351\nEpoch [7/60], Step [31/61], Loss: 1.7606\nEpoch [7/60], Step [32/61], Loss: 2.0627\nEpoch [7/60], Step [33/61], Loss: 2.0817\nEpoch [7/60], Step [34/61], Loss: 1.9074\nEpoch [7/60], Step [35/61], Loss: 2.0105\nEpoch [7/60], Step [36/61], Loss: 1.9102\nEpoch [7/60], Step [37/61], Loss: 2.1553\nEpoch [7/60], Step [38/61], Loss: 1.9204\nEpoch [7/60], Step [39/61], Loss: 2.5938\nEpoch [7/60], Step [40/61], Loss: 1.8016\nEpoch [7/60], Step [41/61], Loss: 1.7774\nEpoch [7/60], Step [42/61], Loss: 1.9054\nEpoch [7/60], Step [43/61], Loss: 2.0614\nEpoch [7/60], Step [44/61], Loss: 1.5957\nEpoch [7/60], Step [45/61], Loss: 1.7952\nEpoch [7/60], Step [46/61], Loss: 2.1697\nEpoch [7/60], Step [47/61], Loss: 2.1658\nEpoch [7/60], Step [48/61], Loss: 1.6872\nEpoch [7/60], Step [49/61], Loss: 2.0574\nEpoch [7/60], Step [50/61], Loss: 2.2333\nEpoch [7/60], Step [51/61], Loss: 2.3508\nEpoch [7/60], Step [52/61], Loss: 2.0645\nEpoch [7/60], Step [53/61], Loss: 1.9295\nEpoch [7/60], Step [54/61], Loss: 1.9611\nEpoch [7/60], Step [55/61], Loss: 2.0760\nEpoch [7/60], Step [56/61], Loss: 2.2326\nEpoch [7/60], Step [57/61], Loss: 1.8632\nEpoch [7/60], Step [58/61], Loss: 2.0965\nEpoch [7/60], Step [59/61], Loss: 2.2060\nEpoch [7/60], Step [60/61], Loss: 1.8304\nEpoch [7/60], Step [61/61], Loss: 1.7581\nTraining Loss: 2.0521\nTraining Accuracy: 0.8249\nValidation Accuracy: 0.8295\nPrecision: 0.8434, Recall: 0.8295\nEpoch [8/60], Step [1/61], Loss: 1.9109\nEpoch [8/60], Step [2/61], Loss: 1.7803\nEpoch [8/60], Step [3/61], Loss: 1.8115\nEpoch [8/60], Step [4/61], Loss: 1.7625\nEpoch [8/60], Step [5/61], Loss: 1.8790\nEpoch [8/60], Step [6/61], Loss: 2.0264\nEpoch [8/60], Step [7/61], Loss: 2.0889\nEpoch [8/60], Step [8/61], Loss: 2.3440\nEpoch [8/60], Step [9/61], Loss: 2.1361\nEpoch [8/60], Step [10/61], Loss: 1.7598\nEpoch [8/60], Step [11/61], Loss: 2.0372\nEpoch [8/60], Step [12/61], Loss: 2.0344\nEpoch [8/60], Step [13/61], Loss: 1.9618\nEpoch [8/60], Step [14/61], Loss: 1.8778\nEpoch [8/60], Step [15/61], Loss: 1.8360\nEpoch [8/60], Step [16/61], Loss: 1.7444\nEpoch [8/60], Step [17/61], Loss: 1.7581\nEpoch [8/60], Step [18/61], Loss: 1.7156\nEpoch [8/60], Step [19/61], Loss: 1.8459\nEpoch [8/60], Step [20/61], Loss: 1.7751\nEpoch [8/60], Step [21/61], Loss: 2.1034\nEpoch [8/60], Step [22/61], Loss: 1.6170\nEpoch [8/60], Step [23/61], Loss: 1.7898\nEpoch [8/60], Step [24/61], Loss: 1.5745\nEpoch [8/60], Step [25/61], Loss: 1.6196\nEpoch [8/60], Step [26/61], Loss: 1.8335\nEpoch [8/60], Step [27/61], Loss: 1.8313\nEpoch [8/60], Step [28/61], Loss: 1.6644\nEpoch [8/60], Step [29/61], Loss: 1.6488\nEpoch [8/60], Step [30/61], Loss: 1.9089\nEpoch [8/60], Step [31/61], Loss: 1.8831\nEpoch [8/60], Step [32/61], Loss: 2.1092\nEpoch [8/60], Step [33/61], Loss: 2.7064\nEpoch [8/60], Step [34/61], Loss: 1.9114\nEpoch [8/60], Step [35/61], Loss: 1.9995\nEpoch [8/60], Step [36/61], Loss: 1.8097\nEpoch [8/60], Step [37/61], Loss: 1.6917\nEpoch [8/60], Step [38/61], Loss: 1.8378\nEpoch [8/60], Step [39/61], Loss: 1.8677\nEpoch [8/60], Step [40/61], Loss: 1.7332\nEpoch [8/60], Step [41/61], Loss: 1.8503\nEpoch [8/60], Step [42/61], Loss: 1.6885\nEpoch [8/60], Step [43/61], Loss: 1.9689\nEpoch [8/60], Step [44/61], Loss: 2.0371\nEpoch [8/60], Step [45/61], Loss: 1.9724\nEpoch [8/60], Step [46/61], Loss: 2.5775\nEpoch [8/60], Step [47/61], Loss: 1.9963\nEpoch [8/60], Step [48/61], Loss: 1.6367\nEpoch [8/60], Step [49/61], Loss: 2.0692\nEpoch [8/60], Step [50/61], Loss: 1.6434\nEpoch [8/60], Step [51/61], Loss: 1.9847\nEpoch [8/60], Step [52/61], Loss: 1.6041\nEpoch [8/60], Step [53/61], Loss: 1.8547\nEpoch [8/60], Step [54/61], Loss: 1.6248\nEpoch [8/60], Step [55/61], Loss: 1.9327\nEpoch [8/60], Step [56/61], Loss: 1.7903\nEpoch [8/60], Step [57/61], Loss: 2.1724\nEpoch [8/60], Step [58/61], Loss: 1.8082\nEpoch [8/60], Step [59/61], Loss: 2.6161\nEpoch [8/60], Step [60/61], Loss: 2.0138\nEpoch [8/60], Step [61/61], Loss: 1.7873\nTraining Loss: 1.8999\nTraining Accuracy: 0.8579\nValidation Accuracy: 0.8180\nPrecision: 0.8398, Recall: 0.8180\nEpoch [9/60], Step [1/61], Loss: 1.5818\nEpoch [9/60], Step [2/61], Loss: 1.7342\nEpoch [9/60], Step [3/61], Loss: 1.6300\nEpoch [9/60], Step [4/61], Loss: 1.9076\nEpoch [9/60], Step [5/61], Loss: 1.8387\nEpoch [9/60], Step [6/61], Loss: 2.2492\nEpoch [9/60], Step [7/61], Loss: 2.4888\nEpoch [9/60], Step [8/61], Loss: 2.0166\nEpoch [9/60], Step [9/61], Loss: 1.8348\nEpoch [9/60], Step [10/61], Loss: 2.1975\nEpoch [9/60], Step [11/61], Loss: 1.8023\nEpoch [9/60], Step [12/61], Loss: 1.9238\nEpoch [9/60], Step [13/61], Loss: 2.0120\nEpoch [9/60], Step [14/61], Loss: 1.7831\nEpoch [9/60], Step [15/61], Loss: 1.7898\nEpoch [9/60], Step [16/61], Loss: 1.8048\nEpoch [9/60], Step [17/61], Loss: 1.8526\nEpoch [9/60], Step [18/61], Loss: 1.8251\nEpoch [9/60], Step [19/61], Loss: 2.0584\nEpoch [9/60], Step [20/61], Loss: 1.7245\nEpoch [9/60], Step [21/61], Loss: 1.6443\nEpoch [9/60], Step [22/61], Loss: 1.6165\nEpoch [9/60], Step [23/61], Loss: 1.6410\nEpoch [9/60], Step [24/61], Loss: 2.1088\nEpoch [9/60], Step [25/61], Loss: 2.1233\nEpoch [9/60], Step [26/61], Loss: 1.8241\nEpoch [9/60], Step [27/61], Loss: 1.6354\nEpoch [9/60], Step [28/61], Loss: 1.5718\nEpoch [9/60], Step [29/61], Loss: 1.7170\nEpoch [9/60], Step [30/61], Loss: 2.1028\nEpoch [9/60], Step [31/61], Loss: 1.7825\nEpoch [9/60], Step [32/61], Loss: 2.1072\nEpoch [9/60], Step [33/61], Loss: 1.9378\nEpoch [9/60], Step [34/61], Loss: 2.2084\nEpoch [9/60], Step [35/61], Loss: 1.9711\nEpoch [9/60], Step [36/61], Loss: 1.9585\nEpoch [9/60], Step [37/61], Loss: 1.7580\nEpoch [9/60], Step [38/61], Loss: 2.2779\nEpoch [9/60], Step [39/61], Loss: 2.9645\nEpoch [9/60], Step [40/61], Loss: 1.9940\nEpoch [9/60], Step [41/61], Loss: 2.7977\nEpoch [9/60], Step [42/61], Loss: 1.7851\nEpoch [9/60], Step [43/61], Loss: 1.5078\nEpoch [9/60], Step [44/61], Loss: 1.7255\nEpoch [9/60], Step [45/61], Loss: 1.9451\nEpoch [9/60], Step [46/61], Loss: 1.7811\nEpoch [9/60], Step [47/61], Loss: 1.9237\nEpoch [9/60], Step [48/61], Loss: 1.9634\nEpoch [9/60], Step [49/61], Loss: 2.1042\nEpoch [9/60], Step [50/61], Loss: 1.8432\nEpoch [9/60], Step [51/61], Loss: 1.6551\nEpoch [9/60], Step [52/61], Loss: 1.5636\nEpoch [9/60], Step [53/61], Loss: 2.4910\nEpoch [9/60], Step [54/61], Loss: 1.8001\nEpoch [9/60], Step [55/61], Loss: 1.6261\nEpoch [9/60], Step [56/61], Loss: 1.8888\nEpoch [9/60], Step [57/61], Loss: 1.6552\nEpoch [9/60], Step [58/61], Loss: 1.6446\nEpoch [9/60], Step [59/61], Loss: 1.9178\nEpoch [9/60], Step [60/61], Loss: 1.6460\nEpoch [9/60], Step [61/61], Loss: 1.9057\nTraining Loss: 1.9044\nTraining Accuracy: 0.8445\nValidation Accuracy: 0.7656\nPrecision: 0.7780, Recall: 0.7656\nEpoch [10/60], Step [1/61], Loss: 2.1454\nEpoch [10/60], Step [2/61], Loss: 1.5444\nEpoch [10/60], Step [3/61], Loss: 1.6955\nEpoch [10/60], Step [4/61], Loss: 1.8097\nEpoch [10/60], Step [5/61], Loss: 1.8455\nEpoch [10/60], Step [6/61], Loss: 1.5309\nEpoch [10/60], Step [7/61], Loss: 1.8639\nEpoch [10/60], Step [8/61], Loss: 1.8137\nEpoch [10/60], Step [9/61], Loss: 1.9579\nEpoch [10/60], Step [10/61], Loss: 1.6791\nEpoch [10/60], Step [11/61], Loss: 1.8449\nEpoch [10/60], Step [12/61], Loss: 1.8006\nEpoch [10/60], Step [13/61], Loss: 1.5125\nEpoch [10/60], Step [14/61], Loss: 1.5081\nEpoch [10/60], Step [15/61], Loss: 1.5694\nEpoch [10/60], Step [16/61], Loss: 1.6951\nEpoch [10/60], Step [17/61], Loss: 1.5562\nEpoch [10/60], Step [18/61], Loss: 1.6009\nEpoch [10/60], Step [19/61], Loss: 1.6540\nEpoch [10/60], Step [20/61], Loss: 1.5129\nEpoch [10/60], Step [21/61], Loss: 1.6899\nEpoch [10/60], Step [22/61], Loss: 1.4902\nEpoch [10/60], Step [23/61], Loss: 1.7897\nEpoch [10/60], Step [24/61], Loss: 1.9497\nEpoch [10/60], Step [25/61], Loss: 1.9589\nEpoch [10/60], Step [26/61], Loss: 2.0975\nEpoch [10/60], Step [27/61], Loss: 1.5888\nEpoch [10/60], Step [28/61], Loss: 1.7963\nEpoch [10/60], Step [29/61], Loss: 1.6097\nEpoch [10/60], Step [30/61], Loss: 1.9343\nEpoch [10/60], Step [31/61], Loss: 1.8356\nEpoch [10/60], Step [32/61], Loss: 1.4939\nEpoch [10/60], Step [33/61], Loss: 2.2992\nEpoch [10/60], Step [34/61], Loss: 1.6383\nEpoch [10/60], Step [35/61], Loss: 1.8646\nEpoch [10/60], Step [36/61], Loss: 1.7118\nEpoch [10/60], Step [37/61], Loss: 2.0017\nEpoch [10/60], Step [38/61], Loss: 1.7419\nEpoch [10/60], Step [39/61], Loss: 1.8375\nEpoch [10/60], Step [40/61], Loss: 1.7237\nEpoch [10/60], Step [41/61], Loss: 1.7667\nEpoch [10/60], Step [42/61], Loss: 1.7307\nEpoch [10/60], Step [43/61], Loss: 1.6581\nEpoch [10/60], Step [44/61], Loss: 1.8272\nEpoch [10/60], Step [45/61], Loss: 2.0737\nEpoch [10/60], Step [46/61], Loss: 2.0082\nEpoch [10/60], Step [47/61], Loss: 1.7857\nEpoch [10/60], Step [48/61], Loss: 1.5608\nEpoch [10/60], Step [49/61], Loss: 2.2476\nEpoch [10/60], Step [50/61], Loss: 2.3090\nEpoch [10/60], Step [51/61], Loss: 1.7939\nEpoch [10/60], Step [52/61], Loss: 1.5649\nEpoch [10/60], Step [53/61], Loss: 1.5163\nEpoch [10/60], Step [54/61], Loss: 1.5239\nEpoch [10/60], Step [55/61], Loss: 1.5567\nEpoch [10/60], Step [56/61], Loss: 1.5308\nEpoch [10/60], Step [57/61], Loss: 1.4500\nEpoch [10/60], Step [58/61], Loss: 1.7099\nEpoch [10/60], Step [59/61], Loss: 1.5181\nEpoch [10/60], Step [60/61], Loss: 1.5005\nEpoch [10/60], Step [61/61], Loss: 1.7239\nTraining Loss: 1.7468\nTraining Accuracy: 0.8857\nValidation Accuracy: 0.8148\nPrecision: 0.8379, Recall: 0.8148\nEpoch [11/60], Step [1/61], Loss: 1.7567\nEpoch [11/60], Step [2/61], Loss: 1.4818\nEpoch [11/60], Step [3/61], Loss: 2.1149\nEpoch [11/60], Step [4/61], Loss: 1.5329\nEpoch [11/60], Step [5/61], Loss: 1.6457\nEpoch [11/60], Step [6/61], Loss: 1.6854\nEpoch [11/60], Step [7/61], Loss: 1.7438\nEpoch [11/60], Step [8/61], Loss: 1.5453\nEpoch [11/60], Step [9/61], Loss: 1.7193\nEpoch [11/60], Step [10/61], Loss: 1.6578\nEpoch [11/60], Step [11/61], Loss: 1.8213\nEpoch [11/60], Step [12/61], Loss: 1.4797\nEpoch [11/60], Step [13/61], Loss: 1.4785\nEpoch [11/60], Step [14/61], Loss: 1.6822\nEpoch [11/60], Step [15/61], Loss: 1.6662\nEpoch [11/60], Step [16/61], Loss: 1.5286\nEpoch [11/60], Step [17/61], Loss: 1.6337\nEpoch [11/60], Step [18/61], Loss: 1.4378\nEpoch [11/60], Step [19/61], Loss: 1.4395\nEpoch [11/60], Step [20/61], Loss: 1.5552\nEpoch [11/60], Step [21/61], Loss: 2.0094\nEpoch [11/60], Step [22/61], Loss: 1.5096\nEpoch [11/60], Step [23/61], Loss: 1.5640\nEpoch [11/60], Step [24/61], Loss: 1.5310\nEpoch [11/60], Step [25/61], Loss: 1.6370\nEpoch [11/60], Step [26/61], Loss: 1.4653\nEpoch [11/60], Step [27/61], Loss: 1.4998\nEpoch [11/60], Step [28/61], Loss: 1.4559\nEpoch [11/60], Step [29/61], Loss: 1.9759\nEpoch [11/60], Step [30/61], Loss: 1.7433\nEpoch [11/60], Step [31/61], Loss: 1.5244\nEpoch [11/60], Step [32/61], Loss: 1.5095\nEpoch [11/60], Step [33/61], Loss: 1.4589\nEpoch [11/60], Step [34/61], Loss: 1.3717\nEpoch [11/60], Step [35/61], Loss: 1.4248\nEpoch [11/60], Step [36/61], Loss: 1.5387\nEpoch [11/60], Step [37/61], Loss: 1.5323\nEpoch [11/60], Step [38/61], Loss: 1.3890\nEpoch [11/60], Step [39/61], Loss: 1.4516\nEpoch [11/60], Step [40/61], Loss: 1.5708\nEpoch [11/60], Step [41/61], Loss: 2.1599\nEpoch [11/60], Step [42/61], Loss: 1.5577\nEpoch [11/60], Step [43/61], Loss: 1.4331\nEpoch [11/60], Step [44/61], Loss: 1.6832\nEpoch [11/60], Step [45/61], Loss: 1.6562\nEpoch [11/60], Step [46/61], Loss: 1.5771\nEpoch [11/60], Step [47/61], Loss: 1.5886\nEpoch [11/60], Step [48/61], Loss: 1.5772\nEpoch [11/60], Step [49/61], Loss: 2.2103\nEpoch [11/60], Step [50/61], Loss: 1.3856\nEpoch [11/60], Step [51/61], Loss: 1.9510\nEpoch [11/60], Step [52/61], Loss: 1.6082\nEpoch [11/60], Step [53/61], Loss: 1.5731\nEpoch [11/60], Step [54/61], Loss: 1.6706\nEpoch [11/60], Step [55/61], Loss: 2.0886\nEpoch [11/60], Step [56/61], Loss: 1.7269\nEpoch [11/60], Step [57/61], Loss: 1.5551\nEpoch [11/60], Step [58/61], Loss: 1.8103\nEpoch [11/60], Step [59/61], Loss: 1.4702\nEpoch [11/60], Step [60/61], Loss: 1.5920\nEpoch [11/60], Step [61/61], Loss: 1.8308\nTraining Loss: 1.6297\nTraining Accuracy: 0.9042\nValidation Accuracy: 0.8328\nPrecision: 0.8456, Recall: 0.8328\nEpoch [12/60], Step [1/61], Loss: 1.4295\nEpoch [12/60], Step [2/61], Loss: 1.4645\nEpoch [12/60], Step [3/61], Loss: 1.4356\nEpoch [12/60], Step [4/61], Loss: 1.6637\nEpoch [12/60], Step [5/61], Loss: 1.4501\nEpoch [12/60], Step [6/61], Loss: 2.6644\nEpoch [12/60], Step [7/61], Loss: 1.6985\nEpoch [12/60], Step [8/61], Loss: 1.4523\nEpoch [12/60], Step [9/61], Loss: 1.6604\nEpoch [12/60], Step [10/61], Loss: 1.4902\nEpoch [12/60], Step [11/61], Loss: 1.6548\nEpoch [12/60], Step [12/61], Loss: 1.6285\nEpoch [12/60], Step [13/61], Loss: 1.4236\nEpoch [12/60], Step [14/61], Loss: 1.8863\nEpoch [12/60], Step [15/61], Loss: 1.4026\nEpoch [12/60], Step [16/61], Loss: 1.6816\nEpoch [12/60], Step [17/61], Loss: 2.0028\nEpoch [12/60], Step [18/61], Loss: 1.6944\nEpoch [12/60], Step [19/61], Loss: 1.5460\nEpoch [12/60], Step [20/61], Loss: 1.5282\nEpoch [12/60], Step [21/61], Loss: 1.4385\nEpoch [12/60], Step [22/61], Loss: 1.6069\nEpoch [12/60], Step [23/61], Loss: 1.5923\nEpoch [12/60], Step [24/61], Loss: 1.3484\nEpoch [12/60], Step [25/61], Loss: 1.6314\nEpoch [12/60], Step [26/61], Loss: 1.4407\nEpoch [12/60], Step [27/61], Loss: 1.5413\nEpoch [12/60], Step [28/61], Loss: 1.4100\nEpoch [12/60], Step [29/61], Loss: 1.4502\nEpoch [12/60], Step [30/61], Loss: 1.4948\nEpoch [12/60], Step [31/61], Loss: 1.4967\nEpoch [12/60], Step [32/61], Loss: 1.6329\nEpoch [12/60], Step [33/61], Loss: 1.7477\nEpoch [12/60], Step [34/61], Loss: 1.5127\nEpoch [12/60], Step [35/61], Loss: 1.7712\nEpoch [12/60], Step [36/61], Loss: 1.4041\nEpoch [12/60], Step [37/61], Loss: 1.3155\nEpoch [12/60], Step [38/61], Loss: 1.4273\nEpoch [12/60], Step [39/61], Loss: 1.9421\nEpoch [12/60], Step [40/61], Loss: 1.5303\nEpoch [12/60], Step [41/61], Loss: 1.3235\nEpoch [12/60], Step [42/61], Loss: 1.5618\nEpoch [12/60], Step [43/61], Loss: 1.4549\nEpoch [12/60], Step [44/61], Loss: 1.4018\nEpoch [12/60], Step [45/61], Loss: 1.4168\nEpoch [12/60], Step [46/61], Loss: 1.5308\nEpoch [12/60], Step [47/61], Loss: 1.6441\nEpoch [12/60], Step [48/61], Loss: 1.3488\nEpoch [12/60], Step [49/61], Loss: 1.3391\nEpoch [12/60], Step [50/61], Loss: 1.6554\nEpoch [12/60], Step [51/61], Loss: 1.8601\nEpoch [12/60], Step [52/61], Loss: 1.6926\nEpoch [12/60], Step [53/61], Loss: 1.4205\nEpoch [12/60], Step [54/61], Loss: 1.4933\nEpoch [12/60], Step [55/61], Loss: 1.4903\nEpoch [12/60], Step [56/61], Loss: 1.6272\nEpoch [12/60], Step [57/61], Loss: 1.4121\nEpoch [12/60], Step [58/61], Loss: 1.4406\nEpoch [12/60], Step [59/61], Loss: 1.4796\nEpoch [12/60], Step [60/61], Loss: 1.4852\nEpoch [12/60], Step [61/61], Loss: 2.3174\nTraining Loss: 1.5698\nTraining Accuracy: 0.9125\nValidation Accuracy: 0.8393\nPrecision: 0.8464, Recall: 0.8393\nEpoch [13/60], Step [1/61], Loss: 1.4624\nEpoch [13/60], Step [2/61], Loss: 1.7291\nEpoch [13/60], Step [3/61], Loss: 2.0600\nEpoch [13/60], Step [4/61], Loss: 1.4400\nEpoch [13/60], Step [5/61], Loss: 1.6112\nEpoch [13/60], Step [6/61], Loss: 1.3436\nEpoch [13/60], Step [7/61], Loss: 1.6726\nEpoch [13/60], Step [8/61], Loss: 1.4108\nEpoch [13/60], Step [9/61], Loss: 2.1634\nEpoch [13/60], Step [10/61], Loss: 1.6998\nEpoch [13/60], Step [11/61], Loss: 1.8118\nEpoch [13/60], Step [12/61], Loss: 1.5004\nEpoch [13/60], Step [13/61], Loss: 1.5236\nEpoch [13/60], Step [14/61], Loss: 1.5012\nEpoch [13/60], Step [15/61], Loss: 1.4242\nEpoch [13/60], Step [16/61], Loss: 1.4093\nEpoch [13/60], Step [17/61], Loss: 1.5188\nEpoch [13/60], Step [18/61], Loss: 1.4712\nEpoch [13/60], Step [19/61], Loss: 1.3141\nEpoch [13/60], Step [20/61], Loss: 1.4841\nEpoch [13/60], Step [21/61], Loss: 1.4384\nEpoch [13/60], Step [22/61], Loss: 1.3668\nEpoch [13/60], Step [23/61], Loss: 1.5365\nEpoch [13/60], Step [24/61], Loss: 1.3693\nEpoch [13/60], Step [25/61], Loss: 1.3394\nEpoch [13/60], Step [26/61], Loss: 1.4747\nEpoch [13/60], Step [27/61], Loss: 1.3713\nEpoch [13/60], Step [28/61], Loss: 1.9332\nEpoch [13/60], Step [29/61], Loss: 1.9208\nEpoch [13/60], Step [30/61], Loss: 1.7090\nEpoch [13/60], Step [31/61], Loss: 1.4386\nEpoch [13/60], Step [32/61], Loss: 1.3999\nEpoch [13/60], Step [33/61], Loss: 1.4416\nEpoch [13/60], Step [34/61], Loss: 2.1546\nEpoch [13/60], Step [35/61], Loss: 1.7190\nEpoch [13/60], Step [36/61], Loss: 1.3765\nEpoch [13/60], Step [37/61], Loss: 1.7470\nEpoch [13/60], Step [38/61], Loss: 1.3532\nEpoch [13/60], Step [39/61], Loss: 1.5507\nEpoch [13/60], Step [40/61], Loss: 1.3790\nEpoch [13/60], Step [41/61], Loss: 1.7538\nEpoch [13/60], Step [42/61], Loss: 1.4972\nEpoch [13/60], Step [43/61], Loss: 1.6611\nEpoch [13/60], Step [44/61], Loss: 1.3358\nEpoch [13/60], Step [45/61], Loss: 1.4711\nEpoch [13/60], Step [46/61], Loss: 1.3782\nEpoch [13/60], Step [47/61], Loss: 1.4067\nEpoch [13/60], Step [48/61], Loss: 1.3230\nEpoch [13/60], Step [49/61], Loss: 1.6605\nEpoch [13/60], Step [50/61], Loss: 1.6780\nEpoch [13/60], Step [51/61], Loss: 1.3290\nEpoch [13/60], Step [52/61], Loss: 1.6912\nEpoch [13/60], Step [53/61], Loss: 1.7569\nEpoch [13/60], Step [54/61], Loss: 1.2873\nEpoch [13/60], Step [55/61], Loss: 1.7833\nEpoch [13/60], Step [56/61], Loss: 1.7113\nEpoch [13/60], Step [57/61], Loss: 1.4737\nEpoch [13/60], Step [58/61], Loss: 1.5683\nEpoch [13/60], Step [59/61], Loss: 1.5045\nEpoch [13/60], Step [60/61], Loss: 1.6917\nEpoch [13/60], Step [61/61], Loss: 1.4622\nTraining Loss: 1.5578\nTraining Accuracy: 0.8970\nValidation Accuracy: 0.8770\nPrecision: 0.8838, Recall: 0.8770\nEpoch [14/60], Step [1/61], Loss: 1.4647\nEpoch [14/60], Step [2/61], Loss: 1.5297\nEpoch [14/60], Step [3/61], Loss: 1.5948\nEpoch [14/60], Step [4/61], Loss: 1.3575\nEpoch [14/60], Step [5/61], Loss: 1.6414\nEpoch [14/60], Step [6/61], Loss: 1.3201\nEpoch [14/60], Step [7/61], Loss: 1.5853\nEpoch [14/60], Step [8/61], Loss: 1.4642\nEpoch [14/60], Step [9/61], Loss: 1.3414\nEpoch [14/60], Step [10/61], Loss: 1.3087\nEpoch [14/60], Step [11/61], Loss: 1.2752\nEpoch [14/60], Step [12/61], Loss: 1.2794\nEpoch [14/60], Step [13/61], Loss: 1.3431\nEpoch [14/60], Step [14/61], Loss: 1.2642\nEpoch [14/60], Step [15/61], Loss: 1.4243\nEpoch [14/60], Step [16/61], Loss: 1.2809\nEpoch [14/60], Step [17/61], Loss: 1.9153\nEpoch [14/60], Step [18/61], Loss: 1.3822\nEpoch [14/60], Step [19/61], Loss: 1.3438\nEpoch [14/60], Step [20/61], Loss: 1.4604\nEpoch [14/60], Step [21/61], Loss: 1.2600\nEpoch [14/60], Step [22/61], Loss: 1.7716\nEpoch [14/60], Step [23/61], Loss: 1.3542\nEpoch [14/60], Step [24/61], Loss: 1.3227\nEpoch [14/60], Step [25/61], Loss: 1.2957\nEpoch [14/60], Step [26/61], Loss: 1.2477\nEpoch [14/60], Step [27/61], Loss: 1.3483\nEpoch [14/60], Step [28/61], Loss: 1.3630\nEpoch [14/60], Step [29/61], Loss: 1.3803\nEpoch [14/60], Step [30/61], Loss: 1.4669\nEpoch [14/60], Step [31/61], Loss: 1.3035\nEpoch [14/60], Step [32/61], Loss: 1.5428\nEpoch [14/60], Step [33/61], Loss: 1.2597\nEpoch [14/60], Step [34/61], Loss: 1.3704\nEpoch [14/60], Step [35/61], Loss: 1.5053\nEpoch [14/60], Step [36/61], Loss: 1.4790\nEpoch [14/60], Step [37/61], Loss: 1.6497\nEpoch [14/60], Step [38/61], Loss: 1.4389\nEpoch [14/60], Step [39/61], Loss: 1.2899\nEpoch [14/60], Step [40/61], Loss: 1.2570\nEpoch [14/60], Step [41/61], Loss: 1.3543\nEpoch [14/60], Step [42/61], Loss: 1.2844\nEpoch [14/60], Step [43/61], Loss: 1.6729\nEpoch [14/60], Step [44/61], Loss: 1.2415\nEpoch [14/60], Step [45/61], Loss: 1.4045\nEpoch [14/60], Step [46/61], Loss: 1.2719\nEpoch [14/60], Step [47/61], Loss: 1.5025\nEpoch [14/60], Step [48/61], Loss: 1.2364\nEpoch [14/60], Step [49/61], Loss: 1.3737\nEpoch [14/60], Step [50/61], Loss: 1.3126\nEpoch [14/60], Step [51/61], Loss: 1.4998\nEpoch [14/60], Step [52/61], Loss: 1.3828\nEpoch [14/60], Step [53/61], Loss: 1.2552\nEpoch [14/60], Step [54/61], Loss: 1.3282\nEpoch [14/60], Step [55/61], Loss: 1.5080\nEpoch [14/60], Step [56/61], Loss: 1.2325\nEpoch [14/60], Step [57/61], Loss: 1.6762\nEpoch [14/60], Step [58/61], Loss: 1.2120\nEpoch [14/60], Step [59/61], Loss: 1.5199\nEpoch [14/60], Step [60/61], Loss: 1.4072\nEpoch [14/60], Step [61/61], Loss: 1.6392\nTraining Loss: 1.4053\nTraining Accuracy: 0.9392\nValidation Accuracy: 0.8459\nPrecision: 0.8630, Recall: 0.8459\nEpoch [15/60], Step [1/61], Loss: 1.4479\nEpoch [15/60], Step [2/61], Loss: 1.7233\nEpoch [15/60], Step [3/61], Loss: 1.2337\nEpoch [15/60], Step [4/61], Loss: 1.2759\nEpoch [15/60], Step [5/61], Loss: 1.2171\nEpoch [15/60], Step [6/61], Loss: 1.4315\nEpoch [15/60], Step [7/61], Loss: 1.3113\nEpoch [15/60], Step [8/61], Loss: 1.3524\nEpoch [15/60], Step [9/61], Loss: 1.5083\nEpoch [15/60], Step [10/61], Loss: 1.4336\nEpoch [15/60], Step [11/61], Loss: 1.8360\nEpoch [15/60], Step [12/61], Loss: 1.3414\nEpoch [15/60], Step [13/61], Loss: 1.3260\nEpoch [15/60], Step [14/61], Loss: 1.2579\nEpoch [15/60], Step [15/61], Loss: 1.2465\nEpoch [15/60], Step [16/61], Loss: 1.3234\nEpoch [15/60], Step [17/61], Loss: 1.2211\nEpoch [15/60], Step [18/61], Loss: 1.3487\nEpoch [15/60], Step [19/61], Loss: 1.2638\nEpoch [15/60], Step [20/61], Loss: 1.5375\nEpoch [15/60], Step [21/61], Loss: 1.5208\nEpoch [15/60], Step [22/61], Loss: 1.4827\nEpoch [15/60], Step [23/61], Loss: 1.7119\nEpoch [15/60], Step [24/61], Loss: 1.2607\nEpoch [15/60], Step [25/61], Loss: 1.2400\nEpoch [15/60], Step [26/61], Loss: 1.2869\nEpoch [15/60], Step [27/61], Loss: 1.2858\nEpoch [15/60], Step [28/61], Loss: 1.2271\nEpoch [15/60], Step [29/61], Loss: 1.3253\nEpoch [15/60], Step [30/61], Loss: 1.7208\nEpoch [15/60], Step [31/61], Loss: 1.4440\nEpoch [15/60], Step [32/61], Loss: 1.7527\nEpoch [15/60], Step [33/61], Loss: 1.7136\nEpoch [15/60], Step [34/61], Loss: 1.2902\nEpoch [15/60], Step [35/61], Loss: 1.4327\nEpoch [15/60], Step [36/61], Loss: 1.2358\nEpoch [15/60], Step [37/61], Loss: 2.0616\nEpoch [15/60], Step [38/61], Loss: 1.6381\nEpoch [15/60], Step [39/61], Loss: 1.2879\nEpoch [15/60], Step [40/61], Loss: 1.5472\nEpoch [15/60], Step [41/61], Loss: 1.2633\nEpoch [15/60], Step [42/61], Loss: 1.3007\nEpoch [15/60], Step [43/61], Loss: 1.2036\nEpoch [15/60], Step [44/61], Loss: 1.6819\nEpoch [15/60], Step [45/61], Loss: 1.4205\nEpoch [15/60], Step [46/61], Loss: 1.3149\nEpoch [15/60], Step [47/61], Loss: 1.4252\nEpoch [15/60], Step [48/61], Loss: 1.5191\nEpoch [15/60], Step [49/61], Loss: 1.7234\nEpoch [15/60], Step [50/61], Loss: 1.6182\nEpoch [15/60], Step [51/61], Loss: 1.3132\nEpoch [15/60], Step [52/61], Loss: 1.4056\nEpoch [15/60], Step [53/61], Loss: 1.7183\nEpoch [15/60], Step [54/61], Loss: 1.2445\nEpoch [15/60], Step [55/61], Loss: 1.3359\nEpoch [15/60], Step [56/61], Loss: 1.3393\nEpoch [15/60], Step [57/61], Loss: 1.2654\nEpoch [15/60], Step [58/61], Loss: 1.7607\nEpoch [15/60], Step [59/61], Loss: 1.3517\nEpoch [15/60], Step [60/61], Loss: 1.2180\nEpoch [15/60], Step [61/61], Loss: 1.3045\nTraining Loss: 1.4241\nTraining Accuracy: 0.9073\nValidation Accuracy: 0.8049\nPrecision: 0.8279, Recall: 0.8049\nEpoch [16/60], Step [1/61], Loss: 1.3366\nEpoch [16/60], Step [2/61], Loss: 1.2095\nEpoch [16/60], Step [3/61], Loss: 1.2199\nEpoch [16/60], Step [4/61], Loss: 2.0636\nEpoch [16/60], Step [5/61], Loss: 1.2528\nEpoch [16/60], Step [6/61], Loss: 1.2434\nEpoch [16/60], Step [7/61], Loss: 1.4365\nEpoch [16/60], Step [8/61], Loss: 1.8779\nEpoch [16/60], Step [9/61], Loss: 1.3206\nEpoch [16/60], Step [10/61], Loss: 1.4898\nEpoch [16/60], Step [11/61], Loss: 1.2465\nEpoch [16/60], Step [12/61], Loss: 1.4026\nEpoch [16/60], Step [13/61], Loss: 1.3284\nEpoch [16/60], Step [14/61], Loss: 1.4676\nEpoch [16/60], Step [15/61], Loss: 1.2908\nEpoch [16/60], Step [16/61], Loss: 1.1692\nEpoch [16/60], Step [17/61], Loss: 1.3713\nEpoch [16/60], Step [18/61], Loss: 1.1726\nEpoch [16/60], Step [19/61], Loss: 1.4482\nEpoch [16/60], Step [20/61], Loss: 1.2443\nEpoch [16/60], Step [21/61], Loss: 1.2639\nEpoch [16/60], Step [22/61], Loss: 1.9720\nEpoch [16/60], Step [23/61], Loss: 1.3899\nEpoch [16/60], Step [24/61], Loss: 1.2973\nEpoch [16/60], Step [25/61], Loss: 1.3150\nEpoch [16/60], Step [26/61], Loss: 1.1993\nEpoch [16/60], Step [27/61], Loss: 1.8462\nEpoch [16/60], Step [28/61], Loss: 1.1589\nEpoch [16/60], Step [29/61], Loss: 1.4006\nEpoch [16/60], Step [30/61], Loss: 1.7031\nEpoch [16/60], Step [31/61], Loss: 1.4146\nEpoch [16/60], Step [32/61], Loss: 1.5648\nEpoch [16/60], Step [33/61], Loss: 1.2038\nEpoch [16/60], Step [34/61], Loss: 1.4021\nEpoch [16/60], Step [35/61], Loss: 1.4699\nEpoch [16/60], Step [36/61], Loss: 1.1856\nEpoch [16/60], Step [37/61], Loss: 1.7602\nEpoch [16/60], Step [38/61], Loss: 1.3225\nEpoch [16/60], Step [39/61], Loss: 1.3309\nEpoch [16/60], Step [40/61], Loss: 1.3323\nEpoch [16/60], Step [41/61], Loss: 1.2627\nEpoch [16/60], Step [42/61], Loss: 1.6009\nEpoch [16/60], Step [43/61], Loss: 1.4122\nEpoch [16/60], Step [44/61], Loss: 1.2585\nEpoch [16/60], Step [45/61], Loss: 1.3260\nEpoch [16/60], Step [46/61], Loss: 1.2352\nEpoch [16/60], Step [47/61], Loss: 1.7067\nEpoch [16/60], Step [48/61], Loss: 1.3859\nEpoch [16/60], Step [49/61], Loss: 1.2175\nEpoch [16/60], Step [50/61], Loss: 1.2549\nEpoch [16/60], Step [51/61], Loss: 1.2514\nEpoch [16/60], Step [52/61], Loss: 1.2252\nEpoch [16/60], Step [53/61], Loss: 1.1811\nEpoch [16/60], Step [54/61], Loss: 1.3041\nEpoch [16/60], Step [55/61], Loss: 1.2101\nEpoch [16/60], Step [56/61], Loss: 1.2046\nEpoch [16/60], Step [57/61], Loss: 1.3827\nEpoch [16/60], Step [58/61], Loss: 1.3266\nEpoch [16/60], Step [59/61], Loss: 1.4220\nEpoch [16/60], Step [60/61], Loss: 1.3521\nEpoch [16/60], Step [61/61], Loss: 1.3015\nTraining Loss: 1.3766\nTraining Accuracy: 0.9217\nValidation Accuracy: 0.8311\nPrecision: 0.8548, Recall: 0.8311\nEpoch [17/60], Step [1/61], Loss: 1.5649\nEpoch [17/60], Step [2/61], Loss: 1.1681\nEpoch [17/60], Step [3/61], Loss: 1.2123\nEpoch [17/60], Step [4/61], Loss: 1.2777\nEpoch [17/60], Step [5/61], Loss: 1.7163\nEpoch [17/60], Step [6/61], Loss: 1.4046\nEpoch [17/60], Step [7/61], Loss: 1.1526\nEpoch [17/60], Step [8/61], Loss: 1.4775\nEpoch [17/60], Step [9/61], Loss: 1.1519\nEpoch [17/60], Step [10/61], Loss: 1.4612\nEpoch [17/60], Step [11/61], Loss: 1.1303\nEpoch [17/60], Step [12/61], Loss: 1.3734\nEpoch [17/60], Step [13/61], Loss: 1.2270\nEpoch [17/60], Step [14/61], Loss: 1.3008\nEpoch [17/60], Step [15/61], Loss: 1.1698\nEpoch [17/60], Step [16/61], Loss: 1.1910\nEpoch [17/60], Step [17/61], Loss: 1.5178\nEpoch [17/60], Step [18/61], Loss: 2.4259\nEpoch [17/60], Step [19/61], Loss: 1.2134\nEpoch [17/60], Step [20/61], Loss: 1.5077\nEpoch [17/60], Step [21/61], Loss: 1.2848\nEpoch [17/60], Step [22/61], Loss: 1.4673\nEpoch [17/60], Step [23/61], Loss: 1.2167\nEpoch [17/60], Step [24/61], Loss: 1.3922\nEpoch [17/60], Step [25/61], Loss: 1.1585\nEpoch [17/60], Step [26/61], Loss: 1.1794\nEpoch [17/60], Step [27/61], Loss: 1.2158\nEpoch [17/60], Step [28/61], Loss: 1.2836\nEpoch [17/60], Step [29/61], Loss: 1.5731\nEpoch [17/60], Step [30/61], Loss: 1.6589\nEpoch [17/60], Step [31/61], Loss: 1.2584\nEpoch [17/60], Step [32/61], Loss: 1.1551\nEpoch [17/60], Step [33/61], Loss: 2.2693\nEpoch [17/60], Step [34/61], Loss: 1.2909\nEpoch [17/60], Step [35/61], Loss: 1.1500\nEpoch [17/60], Step [36/61], Loss: 1.1888\nEpoch [17/60], Step [37/61], Loss: 1.1722\nEpoch [17/60], Step [38/61], Loss: 1.5110\nEpoch [17/60], Step [39/61], Loss: 1.5447\nEpoch [17/60], Step [40/61], Loss: 1.2433\nEpoch [17/60], Step [41/61], Loss: 1.7249\nEpoch [17/60], Step [42/61], Loss: 1.3310\nEpoch [17/60], Step [43/61], Loss: 1.1423\nEpoch [17/60], Step [44/61], Loss: 1.1742\nEpoch [17/60], Step [45/61], Loss: 1.3039\nEpoch [17/60], Step [46/61], Loss: 1.2076\nEpoch [17/60], Step [47/61], Loss: 1.2773\nEpoch [17/60], Step [48/61], Loss: 1.3049\nEpoch [17/60], Step [49/61], Loss: 1.3188\nEpoch [17/60], Step [50/61], Loss: 2.1506\nEpoch [17/60], Step [51/61], Loss: 1.1743\nEpoch [17/60], Step [52/61], Loss: 1.1325\nEpoch [17/60], Step [53/61], Loss: 1.4819\nEpoch [17/60], Step [54/61], Loss: 1.1467\nEpoch [17/60], Step [55/61], Loss: 1.2260\nEpoch [17/60], Step [56/61], Loss: 1.4386\nEpoch [17/60], Step [57/61], Loss: 1.2031\nEpoch [17/60], Step [58/61], Loss: 1.1743\nEpoch [17/60], Step [59/61], Loss: 1.1758\nEpoch [17/60], Step [60/61], Loss: 1.2455\nEpoch [17/60], Step [61/61], Loss: 1.8070\nTraining Loss: 1.3583\nTraining Accuracy: 0.9279\nValidation Accuracy: 0.7443\nPrecision: 0.8173, Recall: 0.7443\nEpoch [18/60], Step [1/61], Loss: 1.2357\nEpoch [18/60], Step [2/61], Loss: 1.2095\nEpoch [18/60], Step [3/61], Loss: 1.4541\nEpoch [18/60], Step [4/61], Loss: 1.2332\nEpoch [18/60], Step [5/61], Loss: 1.2478\nEpoch [18/60], Step [6/61], Loss: 1.1503\nEpoch [18/60], Step [7/61], Loss: 1.1504\nEpoch [18/60], Step [8/61], Loss: 1.2416\nEpoch [18/60], Step [9/61], Loss: 1.3058\nEpoch [18/60], Step [10/61], Loss: 1.2229\nEpoch [18/60], Step [11/61], Loss: 1.3004\nEpoch [18/60], Step [12/61], Loss: 1.2120\nEpoch [18/60], Step [13/61], Loss: 1.4874\nEpoch [18/60], Step [14/61], Loss: 1.3047\nEpoch [18/60], Step [15/61], Loss: 1.1075\nEpoch [18/60], Step [16/61], Loss: 1.4024\nEpoch [18/60], Step [17/61], Loss: 1.2069\nEpoch [18/60], Step [18/61], Loss: 1.2128\nEpoch [18/60], Step [19/61], Loss: 1.2368\nEpoch [18/60], Step [20/61], Loss: 1.2361\nEpoch [18/60], Step [21/61], Loss: 1.4268\nEpoch [18/60], Step [22/61], Loss: 1.1591\nEpoch [18/60], Step [23/61], Loss: 1.1703\nEpoch [18/60], Step [24/61], Loss: 1.3192\nEpoch [18/60], Step [25/61], Loss: 1.1352\nEpoch [18/60], Step [26/61], Loss: 1.2191\nEpoch [18/60], Step [27/61], Loss: 1.3112\nEpoch [18/60], Step [28/61], Loss: 1.3613\nEpoch [18/60], Step [29/61], Loss: 1.2454\nEpoch [18/60], Step [30/61], Loss: 1.5746\nEpoch [18/60], Step [31/61], Loss: 1.0873\nEpoch [18/60], Step [32/61], Loss: 1.2328\nEpoch [18/60], Step [33/61], Loss: 1.1579\nEpoch [18/60], Step [34/61], Loss: 1.1698\nEpoch [18/60], Step [35/61], Loss: 1.6391\nEpoch [18/60], Step [36/61], Loss: 1.2092\nEpoch [18/60], Step [37/61], Loss: 1.7818\nEpoch [18/60], Step [38/61], Loss: 1.1304\nEpoch [18/60], Step [39/61], Loss: 1.2629\nEpoch [18/60], Step [40/61], Loss: 1.1119\nEpoch [18/60], Step [41/61], Loss: 1.2761\nEpoch [18/60], Step [42/61], Loss: 1.1616\nEpoch [18/60], Step [43/61], Loss: 1.2680\nEpoch [18/60], Step [44/61], Loss: 1.1347\nEpoch [18/60], Step [45/61], Loss: 1.2900\nEpoch [18/60], Step [46/61], Loss: 1.2441\nEpoch [18/60], Step [47/61], Loss: 1.2712\nEpoch [18/60], Step [48/61], Loss: 1.4077\nEpoch [18/60], Step [49/61], Loss: 1.0937\nEpoch [18/60], Step [50/61], Loss: 1.1091\nEpoch [18/60], Step [51/61], Loss: 1.4687\nEpoch [18/60], Step [52/61], Loss: 1.1084\nEpoch [18/60], Step [53/61], Loss: 1.3760\nEpoch [18/60], Step [54/61], Loss: 1.4729\nEpoch [18/60], Step [55/61], Loss: 1.2903\nEpoch [18/60], Step [56/61], Loss: 1.2028\nEpoch [18/60], Step [57/61], Loss: 1.6455\nEpoch [18/60], Step [58/61], Loss: 1.1903\nEpoch [18/60], Step [59/61], Loss: 1.3818\nEpoch [18/60], Step [60/61], Loss: 1.2944\nEpoch [18/60], Step [61/61], Loss: 1.1727\nTraining Loss: 1.2747\nTraining Accuracy: 0.9258\nValidation Accuracy: 0.7836\nPrecision: 0.8185, Recall: 0.7836\nEpoch [19/60], Step [1/61], Loss: 1.1179\nEpoch [19/60], Step [2/61], Loss: 1.5271\nEpoch [19/60], Step [3/61], Loss: 1.1805\nEpoch [19/60], Step [4/61], Loss: 1.7470\nEpoch [19/60], Step [5/61], Loss: 1.2484\nEpoch [19/60], Step [6/61], Loss: 1.7484\nEpoch [19/60], Step [7/61], Loss: 1.3330\nEpoch [19/60], Step [8/61], Loss: 1.1840\nEpoch [19/60], Step [9/61], Loss: 1.1237\nEpoch [19/60], Step [10/61], Loss: 1.2814\nEpoch [19/60], Step [11/61], Loss: 1.1153\nEpoch [19/60], Step [12/61], Loss: 1.2394\nEpoch [19/60], Step [13/61], Loss: 1.2744\nEpoch [19/60], Step [14/61], Loss: 1.1637\nEpoch [19/60], Step [15/61], Loss: 1.1746\nEpoch [19/60], Step [16/61], Loss: 1.2990\nEpoch [19/60], Step [17/61], Loss: 1.1438\nEpoch [19/60], Step [18/61], Loss: 1.5411\nEpoch [19/60], Step [19/61], Loss: 1.2560\nEpoch [19/60], Step [20/61], Loss: 1.3103\nEpoch [19/60], Step [21/61], Loss: 1.1017\nEpoch [19/60], Step [22/61], Loss: 1.2583\nEpoch [19/60], Step [23/61], Loss: 1.3888\nEpoch [19/60], Step [24/61], Loss: 1.3509\nEpoch [19/60], Step [25/61], Loss: 1.2026\nEpoch [19/60], Step [26/61], Loss: 1.2673\nEpoch [19/60], Step [27/61], Loss: 1.5078\nEpoch [19/60], Step [28/61], Loss: 1.2322\nEpoch [19/60], Step [29/61], Loss: 1.2765\nEpoch [19/60], Step [30/61], Loss: 1.1972\nEpoch [19/60], Step [31/61], Loss: 1.0750\nEpoch [19/60], Step [32/61], Loss: 1.1520\nEpoch [19/60], Step [33/61], Loss: 2.1524\nEpoch [19/60], Step [34/61], Loss: 1.1968\nEpoch [19/60], Step [35/61], Loss: 1.1084\nEpoch [19/60], Step [36/61], Loss: 1.1583\nEpoch [19/60], Step [37/61], Loss: 1.0798\nEpoch [19/60], Step [38/61], Loss: 1.2611\nEpoch [19/60], Step [39/61], Loss: 1.2370\nEpoch [19/60], Step [40/61], Loss: 1.1309\nEpoch [19/60], Step [41/61], Loss: 1.1711\nEpoch [19/60], Step [42/61], Loss: 1.1221\nEpoch [19/60], Step [43/61], Loss: 1.0705\nEpoch [19/60], Step [44/61], Loss: 1.1329\nEpoch [19/60], Step [45/61], Loss: 1.1869\nEpoch [19/60], Step [46/61], Loss: 1.0994\nEpoch [19/60], Step [47/61], Loss: 1.1737\nEpoch [19/60], Step [48/61], Loss: 1.0627\nEpoch [19/60], Step [49/61], Loss: 1.1494\nEpoch [19/60], Step [50/61], Loss: 1.1069\nEpoch [19/60], Step [51/61], Loss: 1.3367\nEpoch [19/60], Step [52/61], Loss: 1.1335\nEpoch [19/60], Step [53/61], Loss: 1.0843\nEpoch [19/60], Step [54/61], Loss: 1.0991\nEpoch [19/60], Step [55/61], Loss: 1.0670\nEpoch [19/60], Step [56/61], Loss: 1.0441\nEpoch [19/60], Step [57/61], Loss: 1.1071\nEpoch [19/60], Step [58/61], Loss: 1.3013\nEpoch [19/60], Step [59/61], Loss: 1.3446\nEpoch [19/60], Step [60/61], Loss: 1.2944\nEpoch [19/60], Step [61/61], Loss: 1.0749\nTraining Loss: 1.2387\nTraining Accuracy: 0.9372\nValidation Accuracy: 0.8738\nPrecision: 0.8907, Recall: 0.8738\nEpoch [20/60], Step [1/61], Loss: 1.0720\nEpoch [20/60], Step [2/61], Loss: 1.4775\nEpoch [20/60], Step [3/61], Loss: 1.5182\nEpoch [20/60], Step [4/61], Loss: 1.0613\nEpoch [20/60], Step [5/61], Loss: 1.1300\nEpoch [20/60], Step [6/61], Loss: 1.2144\nEpoch [20/60], Step [7/61], Loss: 1.4099\nEpoch [20/60], Step [8/61], Loss: 1.1084\nEpoch [20/60], Step [9/61], Loss: 1.1615\nEpoch [20/60], Step [10/61], Loss: 1.1718\nEpoch [20/60], Step [11/61], Loss: 1.3382\nEpoch [20/60], Step [12/61], Loss: 1.0865\nEpoch [20/60], Step [13/61], Loss: 1.0656\nEpoch [20/60], Step [14/61], Loss: 1.0947\nEpoch [20/60], Step [15/61], Loss: 1.0392\nEpoch [20/60], Step [16/61], Loss: 1.3296\nEpoch [20/60], Step [17/61], Loss: 1.4690\nEpoch [20/60], Step [18/61], Loss: 1.1498\nEpoch [20/60], Step [19/61], Loss: 1.4506\nEpoch [20/60], Step [20/61], Loss: 1.0477\nEpoch [20/60], Step [21/61], Loss: 1.1715\nEpoch [20/60], Step [22/61], Loss: 1.1610\nEpoch [20/60], Step [23/61], Loss: 1.1385\nEpoch [20/60], Step [24/61], Loss: 1.1736\nEpoch [20/60], Step [25/61], Loss: 1.0615\nEpoch [20/60], Step [26/61], Loss: 1.0303\nEpoch [20/60], Step [27/61], Loss: 1.0952\nEpoch [20/60], Step [28/61], Loss: 1.0759\nEpoch [20/60], Step [29/61], Loss: 1.0783\nEpoch [20/60], Step [30/61], Loss: 1.1441\nEpoch [20/60], Step [31/61], Loss: 1.1753\nEpoch [20/60], Step [32/61], Loss: 1.1210\nEpoch [20/60], Step [33/61], Loss: 1.1411\nEpoch [20/60], Step [34/61], Loss: 1.0763\nEpoch [20/60], Step [35/61], Loss: 1.2085\nEpoch [20/60], Step [36/61], Loss: 1.0908\nEpoch [20/60], Step [37/61], Loss: 1.1893\nEpoch [20/60], Step [38/61], Loss: 1.0576\nEpoch [20/60], Step [39/61], Loss: 1.2364\nEpoch [20/60], Step [40/61], Loss: 1.3280\nEpoch [20/60], Step [41/61], Loss: 1.0753\nEpoch [20/60], Step [42/61], Loss: 1.1669\nEpoch [20/60], Step [43/61], Loss: 1.0537\nEpoch [20/60], Step [44/61], Loss: 1.1393\nEpoch [20/60], Step [45/61], Loss: 1.3663\nEpoch [20/60], Step [46/61], Loss: 1.0351\nEpoch [20/60], Step [47/61], Loss: 1.1086\nEpoch [20/60], Step [48/61], Loss: 1.3276\nEpoch [20/60], Step [49/61], Loss: 1.1577\nEpoch [20/60], Step [50/61], Loss: 1.1774\nEpoch [20/60], Step [51/61], Loss: 1.6453\nEpoch [20/60], Step [52/61], Loss: 1.0472\nEpoch [20/60], Step [53/61], Loss: 1.1205\nEpoch [20/60], Step [54/61], Loss: 1.0184\nEpoch [20/60], Step [55/61], Loss: 1.3924\nEpoch [20/60], Step [56/61], Loss: 1.0365\nEpoch [20/60], Step [57/61], Loss: 1.0527\nEpoch [20/60], Step [58/61], Loss: 1.4440\nEpoch [20/60], Step [59/61], Loss: 1.3330\nEpoch [20/60], Step [60/61], Loss: 1.2811\nEpoch [20/60], Step [61/61], Loss: 1.0545\nTraining Loss: 1.1840\nTraining Accuracy: 0.9454\nValidation Accuracy: 0.8820\nPrecision: 0.8872, Recall: 0.8820\nEpoch [21/60], Step [1/61], Loss: 1.0736\nEpoch [21/60], Step [2/61], Loss: 1.2639\nEpoch [21/60], Step [3/61], Loss: 1.3304\nEpoch [21/60], Step [4/61], Loss: 1.3669\nEpoch [21/60], Step [5/61], Loss: 1.1438\nEpoch [21/60], Step [6/61], Loss: 1.1005\nEpoch [21/60], Step [7/61], Loss: 1.2933\nEpoch [21/60], Step [8/61], Loss: 1.0090\nEpoch [21/60], Step [9/61], Loss: 1.1542\nEpoch [21/60], Step [10/61], Loss: 1.0056\nEpoch [21/60], Step [11/61], Loss: 1.1674\nEpoch [21/60], Step [12/61], Loss: 1.1036\nEpoch [21/60], Step [13/61], Loss: 1.2215\nEpoch [21/60], Step [14/61], Loss: 1.0196\nEpoch [21/60], Step [15/61], Loss: 1.1863\nEpoch [21/60], Step [16/61], Loss: 1.0934\nEpoch [21/60], Step [17/61], Loss: 1.0563\nEpoch [21/60], Step [18/61], Loss: 1.1631\nEpoch [21/60], Step [19/61], Loss: 1.3777\nEpoch [21/60], Step [20/61], Loss: 1.0714\nEpoch [21/60], Step [21/61], Loss: 1.0767\nEpoch [21/60], Step [22/61], Loss: 1.0105\nEpoch [21/60], Step [23/61], Loss: 1.0499\nEpoch [21/60], Step [24/61], Loss: 1.0146\nEpoch [21/60], Step [25/61], Loss: 1.2954\nEpoch [21/60], Step [26/61], Loss: 1.3653\nEpoch [21/60], Step [27/61], Loss: 0.9980\nEpoch [21/60], Step [28/61], Loss: 1.0170\nEpoch [21/60], Step [29/61], Loss: 1.1624\nEpoch [21/60], Step [30/61], Loss: 1.2116\nEpoch [21/60], Step [31/61], Loss: 1.0725\nEpoch [21/60], Step [32/61], Loss: 1.1310\nEpoch [21/60], Step [33/61], Loss: 1.0269\nEpoch [21/60], Step [34/61], Loss: 1.3353\nEpoch [21/60], Step [35/61], Loss: 1.0347\nEpoch [21/60], Step [36/61], Loss: 1.1396\nEpoch [21/60], Step [37/61], Loss: 1.2284\nEpoch [21/60], Step [38/61], Loss: 1.0159\nEpoch [21/60], Step [39/61], Loss: 1.0549\nEpoch [21/60], Step [40/61], Loss: 1.3202\nEpoch [21/60], Step [41/61], Loss: 1.0750\nEpoch [21/60], Step [42/61], Loss: 1.2837\nEpoch [21/60], Step [43/61], Loss: 1.0691\nEpoch [21/60], Step [44/61], Loss: 1.0893\nEpoch [21/60], Step [45/61], Loss: 0.9915\nEpoch [21/60], Step [46/61], Loss: 1.0876\nEpoch [21/60], Step [47/61], Loss: 1.6926\nEpoch [21/60], Step [48/61], Loss: 1.0047\nEpoch [21/60], Step [49/61], Loss: 1.0516\nEpoch [21/60], Step [50/61], Loss: 1.2420\nEpoch [21/60], Step [51/61], Loss: 1.1651\nEpoch [21/60], Step [52/61], Loss: 1.0256\nEpoch [21/60], Step [53/61], Loss: 1.0311\nEpoch [21/60], Step [54/61], Loss: 1.1025\nEpoch [21/60], Step [55/61], Loss: 1.1174\nEpoch [21/60], Step [56/61], Loss: 1.1137\nEpoch [21/60], Step [57/61], Loss: 1.3433\nEpoch [21/60], Step [58/61], Loss: 1.7576\nEpoch [21/60], Step [59/61], Loss: 1.3771\nEpoch [21/60], Step [60/61], Loss: 1.0351\nEpoch [21/60], Step [61/61], Loss: 1.0433\nTraining Loss: 1.1557\nTraining Accuracy: 0.9392\nValidation Accuracy: 0.8623\nPrecision: 0.8691, Recall: 0.8623\nEpoch [22/60], Step [1/61], Loss: 1.1454\nEpoch [22/60], Step [2/61], Loss: 0.9979\nEpoch [22/60], Step [3/61], Loss: 1.2337\nEpoch [22/60], Step [4/61], Loss: 1.0312\nEpoch [22/60], Step [5/61], Loss: 1.1171\nEpoch [22/60], Step [6/61], Loss: 0.9806\nEpoch [22/60], Step [7/61], Loss: 0.9864\nEpoch [22/60], Step [8/61], Loss: 1.0834\nEpoch [22/60], Step [9/61], Loss: 0.9778\nEpoch [22/60], Step [10/61], Loss: 0.9999\nEpoch [22/60], Step [11/61], Loss: 1.0561\nEpoch [22/60], Step [12/61], Loss: 1.0284\nEpoch [22/60], Step [13/61], Loss: 1.1253\nEpoch [22/60], Step [14/61], Loss: 1.0709\nEpoch [22/60], Step [15/61], Loss: 1.0733\nEpoch [22/60], Step [16/61], Loss: 1.0921\nEpoch [22/60], Step [17/61], Loss: 1.0167\nEpoch [22/60], Step [18/61], Loss: 0.9714\nEpoch [22/60], Step [19/61], Loss: 1.1756\nEpoch [22/60], Step [20/61], Loss: 1.0039\nEpoch [22/60], Step [21/61], Loss: 1.0530\nEpoch [22/60], Step [22/61], Loss: 1.0562\nEpoch [22/60], Step [23/61], Loss: 0.9921\nEpoch [22/60], Step [24/61], Loss: 1.1966\nEpoch [22/60], Step [25/61], Loss: 0.9699\nEpoch [22/60], Step [26/61], Loss: 1.0983\nEpoch [22/60], Step [27/61], Loss: 1.0407\nEpoch [22/60], Step [28/61], Loss: 0.9629\nEpoch [22/60], Step [29/61], Loss: 1.2207\nEpoch [22/60], Step [30/61], Loss: 0.9667\nEpoch [22/60], Step [31/61], Loss: 1.0326\nEpoch [22/60], Step [32/61], Loss: 0.9658\nEpoch [22/60], Step [33/61], Loss: 1.3776\nEpoch [22/60], Step [34/61], Loss: 1.0090\nEpoch [22/60], Step [35/61], Loss: 0.9624\nEpoch [22/60], Step [36/61], Loss: 1.0819\nEpoch [22/60], Step [37/61], Loss: 1.0324\nEpoch [22/60], Step [38/61], Loss: 1.0800\nEpoch [22/60], Step [39/61], Loss: 0.9791\nEpoch [22/60], Step [40/61], Loss: 1.0702\nEpoch [22/60], Step [41/61], Loss: 0.9637\nEpoch [22/60], Step [42/61], Loss: 1.6226\nEpoch [22/60], Step [43/61], Loss: 1.2520\nEpoch [22/60], Step [44/61], Loss: 1.1905\nEpoch [22/60], Step [45/61], Loss: 1.1456\nEpoch [22/60], Step [46/61], Loss: 1.0909\nEpoch [22/60], Step [47/61], Loss: 1.1842\nEpoch [22/60], Step [48/61], Loss: 1.0230\nEpoch [22/60], Step [49/61], Loss: 1.1752\nEpoch [22/60], Step [50/61], Loss: 1.0083\nEpoch [22/60], Step [51/61], Loss: 1.0574\nEpoch [22/60], Step [52/61], Loss: 0.9743\nEpoch [22/60], Step [53/61], Loss: 1.0488\nEpoch [22/60], Step [54/61], Loss: 1.4370\nEpoch [22/60], Step [55/61], Loss: 1.2386\nEpoch [22/60], Step [56/61], Loss: 0.9815\nEpoch [22/60], Step [57/61], Loss: 1.0909\nEpoch [22/60], Step [58/61], Loss: 1.1753\nEpoch [22/60], Step [59/61], Loss: 0.9886\nEpoch [22/60], Step [60/61], Loss: 1.0553\nEpoch [22/60], Step [61/61], Loss: 1.4613\nTraining Loss: 1.0879\nTraining Accuracy: 0.9506\nValidation Accuracy: 0.8279\nPrecision: 0.8571, Recall: 0.8279\nEpoch [23/60], Step [1/61], Loss: 0.9751\nEpoch [23/60], Step [2/61], Loss: 0.9662\nEpoch [23/60], Step [3/61], Loss: 1.6434\nEpoch [23/60], Step [4/61], Loss: 1.3582\nEpoch [23/60], Step [5/61], Loss: 1.0103\nEpoch [23/60], Step [6/61], Loss: 0.9821\nEpoch [23/60], Step [7/61], Loss: 1.1045\nEpoch [23/60], Step [8/61], Loss: 1.0797\nEpoch [23/60], Step [9/61], Loss: 0.9489\nEpoch [23/60], Step [10/61], Loss: 1.1564\nEpoch [23/60], Step [11/61], Loss: 1.1074\nEpoch [23/60], Step [12/61], Loss: 1.3242\nEpoch [23/60], Step [13/61], Loss: 1.0337\nEpoch [23/60], Step [14/61], Loss: 1.1696\nEpoch [23/60], Step [15/61], Loss: 1.0716\nEpoch [23/60], Step [16/61], Loss: 1.8015\nEpoch [23/60], Step [17/61], Loss: 1.0816\nEpoch [23/60], Step [18/61], Loss: 2.3168\nEpoch [23/60], Step [19/61], Loss: 1.0497\nEpoch [23/60], Step [20/61], Loss: 1.1478\nEpoch [23/60], Step [21/61], Loss: 1.0184\nEpoch [23/60], Step [22/61], Loss: 0.9582\nEpoch [23/60], Step [23/61], Loss: 0.9803\nEpoch [23/60], Step [24/61], Loss: 0.9877\nEpoch [23/60], Step [25/61], Loss: 1.0458\nEpoch [23/60], Step [26/61], Loss: 1.3576\nEpoch [23/60], Step [27/61], Loss: 1.0501\nEpoch [23/60], Step [28/61], Loss: 1.7532\nEpoch [23/60], Step [29/61], Loss: 1.3084\nEpoch [23/60], Step [30/61], Loss: 1.8818\nEpoch [23/60], Step [31/61], Loss: 1.2739\nEpoch [23/60], Step [32/61], Loss: 1.0248\nEpoch [23/60], Step [33/61], Loss: 1.1926\nEpoch [23/60], Step [34/61], Loss: 0.9840\nEpoch [23/60], Step [35/61], Loss: 1.1071\nEpoch [23/60], Step [36/61], Loss: 1.1241\nEpoch [23/60], Step [37/61], Loss: 1.0348\nEpoch [23/60], Step [38/61], Loss: 1.1306\nEpoch [23/60], Step [39/61], Loss: 0.9438\nEpoch [23/60], Step [40/61], Loss: 1.0393\nEpoch [23/60], Step [41/61], Loss: 1.2765\nEpoch [23/60], Step [42/61], Loss: 1.1763\nEpoch [23/60], Step [43/61], Loss: 1.5225\nEpoch [23/60], Step [44/61], Loss: 1.3074\nEpoch [23/60], Step [45/61], Loss: 0.9952\nEpoch [23/60], Step [46/61], Loss: 1.0710\nEpoch [23/60], Step [47/61], Loss: 0.9746\nEpoch [23/60], Step [48/61], Loss: 0.9627\nEpoch [23/60], Step [49/61], Loss: 1.0606\nEpoch [23/60], Step [50/61], Loss: 1.0883\nEpoch [23/60], Step [51/61], Loss: 0.9629\nEpoch [23/60], Step [52/61], Loss: 1.2576\nEpoch [23/60], Step [53/61], Loss: 0.9600\nEpoch [23/60], Step [54/61], Loss: 1.0534\nEpoch [23/60], Step [55/61], Loss: 1.0954\nEpoch [23/60], Step [56/61], Loss: 1.2571\nEpoch [23/60], Step [57/61], Loss: 1.0386\nEpoch [23/60], Step [58/61], Loss: 0.9713\nEpoch [23/60], Step [59/61], Loss: 1.1184\nEpoch [23/60], Step [60/61], Loss: 1.2358\nEpoch [23/60], Step [61/61], Loss: 1.2425\nTraining Loss: 1.1660\nTraining Accuracy: 0.9300\nValidation Accuracy: 0.8869\nPrecision: 0.8909, Recall: 0.8869\nEpoch [24/60], Step [1/61], Loss: 1.0061\nEpoch [24/60], Step [2/61], Loss: 1.0603\nEpoch [24/60], Step [3/61], Loss: 0.9851\nEpoch [24/60], Step [4/61], Loss: 0.9455\nEpoch [24/60], Step [5/61], Loss: 0.9234\nEpoch [24/60], Step [6/61], Loss: 1.0139\nEpoch [24/60], Step [7/61], Loss: 1.0489\nEpoch [24/60], Step [8/61], Loss: 0.9532\nEpoch [24/60], Step [9/61], Loss: 0.9594\nEpoch [24/60], Step [10/61], Loss: 1.1787\nEpoch [24/60], Step [11/61], Loss: 0.9614\nEpoch [24/60], Step [12/61], Loss: 1.0703\nEpoch [24/60], Step [13/61], Loss: 1.0815\nEpoch [24/60], Step [14/61], Loss: 0.9953\nEpoch [24/60], Step [15/61], Loss: 0.9233\nEpoch [24/60], Step [16/61], Loss: 1.0221\nEpoch [24/60], Step [17/61], Loss: 0.9591\nEpoch [24/60], Step [18/61], Loss: 1.0420\nEpoch [24/60], Step [19/61], Loss: 0.9298\nEpoch [24/60], Step [20/61], Loss: 1.0086\nEpoch [24/60], Step [21/61], Loss: 0.9189\nEpoch [24/60], Step [22/61], Loss: 0.9165\nEpoch [24/60], Step [23/61], Loss: 1.0310\nEpoch [24/60], Step [24/61], Loss: 1.0961\nEpoch [24/60], Step [25/61], Loss: 1.0315\nEpoch [24/60], Step [26/61], Loss: 1.1107\nEpoch [24/60], Step [27/61], Loss: 0.9571\nEpoch [24/60], Step [28/61], Loss: 0.9242\nEpoch [24/60], Step [29/61], Loss: 0.9282\nEpoch [24/60], Step [30/61], Loss: 0.9280\nEpoch [24/60], Step [31/61], Loss: 1.0525\nEpoch [24/60], Step [32/61], Loss: 0.9975\nEpoch [24/60], Step [33/61], Loss: 1.0133\nEpoch [24/60], Step [34/61], Loss: 1.0039\nEpoch [24/60], Step [35/61], Loss: 0.9479\nEpoch [24/60], Step [36/61], Loss: 1.0786\nEpoch [24/60], Step [37/61], Loss: 0.9304\nEpoch [24/60], Step [38/61], Loss: 0.9063\nEpoch [24/60], Step [39/61], Loss: 1.2051\nEpoch [24/60], Step [40/61], Loss: 1.0738\nEpoch [24/60], Step [41/61], Loss: 0.9211\nEpoch [24/60], Step [42/61], Loss: 0.9397\nEpoch [24/60], Step [43/61], Loss: 0.9415\nEpoch [24/60], Step [44/61], Loss: 0.9358\nEpoch [24/60], Step [45/61], Loss: 0.9017\nEpoch [24/60], Step [46/61], Loss: 0.9466\nEpoch [24/60], Step [47/61], Loss: 1.0633\nEpoch [24/60], Step [48/61], Loss: 0.9104\nEpoch [24/60], Step [49/61], Loss: 0.9257\nEpoch [24/60], Step [50/61], Loss: 1.0615\nEpoch [24/60], Step [51/61], Loss: 0.9733\nEpoch [24/60], Step [52/61], Loss: 0.9598\nEpoch [24/60], Step [53/61], Loss: 0.9826\nEpoch [24/60], Step [54/61], Loss: 0.9303\nEpoch [24/60], Step [55/61], Loss: 0.9454\nEpoch [24/60], Step [56/61], Loss: 1.0250\nEpoch [24/60], Step [57/61], Loss: 0.9167\nEpoch [24/60], Step [58/61], Loss: 1.0706\nEpoch [24/60], Step [59/61], Loss: 0.8968\nEpoch [24/60], Step [60/61], Loss: 1.0246\nEpoch [24/60], Step [61/61], Loss: 0.8963\nTraining Loss: 0.9888\nTraining Accuracy: 0.9722\nValidation Accuracy: 0.8967\nPrecision: 0.9025, Recall: 0.8967\nEpoch [25/60], Step [1/61], Loss: 0.9172\nEpoch [25/60], Step [2/61], Loss: 0.9230\nEpoch [25/60], Step [3/61], Loss: 0.9327\nEpoch [25/60], Step [4/61], Loss: 0.8824\nEpoch [25/60], Step [5/61], Loss: 0.8905\nEpoch [25/60], Step [6/61], Loss: 0.8949\nEpoch [25/60], Step [7/61], Loss: 0.8975\nEpoch [25/60], Step [8/61], Loss: 1.0879\nEpoch [25/60], Step [9/61], Loss: 0.9008\nEpoch [25/60], Step [10/61], Loss: 0.9585\nEpoch [25/60], Step [11/61], Loss: 0.9571\nEpoch [25/60], Step [12/61], Loss: 0.9184\nEpoch [25/60], Step [13/61], Loss: 1.1132\nEpoch [25/60], Step [14/61], Loss: 0.8788\nEpoch [25/60], Step [15/61], Loss: 0.8748\nEpoch [25/60], Step [16/61], Loss: 0.9041\nEpoch [25/60], Step [17/61], Loss: 0.8806\nEpoch [25/60], Step [18/61], Loss: 1.0550\nEpoch [25/60], Step [19/61], Loss: 0.9387\nEpoch [25/60], Step [20/61], Loss: 1.1210\nEpoch [25/60], Step [21/61], Loss: 0.9188\nEpoch [25/60], Step [22/61], Loss: 1.0103\nEpoch [25/60], Step [23/61], Loss: 0.9074\nEpoch [25/60], Step [24/61], Loss: 0.8709\nEpoch [25/60], Step [25/61], Loss: 1.0605\nEpoch [25/60], Step [26/61], Loss: 0.8772\nEpoch [25/60], Step [27/61], Loss: 0.9658\nEpoch [25/60], Step [28/61], Loss: 1.0188\nEpoch [25/60], Step [29/61], Loss: 0.8777\nEpoch [25/60], Step [30/61], Loss: 0.9754\nEpoch [25/60], Step [31/61], Loss: 0.9282\nEpoch [25/60], Step [32/61], Loss: 0.9704\nEpoch [25/60], Step [33/61], Loss: 1.0980\nEpoch [25/60], Step [34/61], Loss: 1.1440\nEpoch [25/60], Step [35/61], Loss: 1.2614\nEpoch [25/60], Step [36/61], Loss: 1.1299\nEpoch [25/60], Step [37/61], Loss: 0.8696\nEpoch [25/60], Step [38/61], Loss: 1.0312\nEpoch [25/60], Step [39/61], Loss: 0.8949\nEpoch [25/60], Step [40/61], Loss: 0.9539\nEpoch [25/60], Step [41/61], Loss: 1.1215\nEpoch [25/60], Step [42/61], Loss: 0.8961\nEpoch [25/60], Step [43/61], Loss: 1.0259\nEpoch [25/60], Step [44/61], Loss: 1.2446\nEpoch [25/60], Step [45/61], Loss: 1.0267\nEpoch [25/60], Step [46/61], Loss: 1.2243\nEpoch [25/60], Step [47/61], Loss: 1.2959\nEpoch [25/60], Step [48/61], Loss: 1.3517\nEpoch [25/60], Step [49/61], Loss: 0.8786\nEpoch [25/60], Step [50/61], Loss: 1.0224\nEpoch [25/60], Step [51/61], Loss: 0.9773\nEpoch [25/60], Step [52/61], Loss: 0.9367\nEpoch [25/60], Step [53/61], Loss: 1.0919\nEpoch [25/60], Step [54/61], Loss: 0.9340\nEpoch [25/60], Step [55/61], Loss: 1.1014\nEpoch [25/60], Step [56/61], Loss: 1.1691\nEpoch [25/60], Step [57/61], Loss: 0.8839\nEpoch [25/60], Step [58/61], Loss: 1.1371\nEpoch [25/60], Step [59/61], Loss: 0.8804\nEpoch [25/60], Step [60/61], Loss: 1.0376\nEpoch [25/60], Step [61/61], Loss: 1.1921\nTraining Loss: 1.0010\nTraining Accuracy: 0.9506\nValidation Accuracy: 0.8672\nPrecision: 0.8823, Recall: 0.8672\nEpoch [26/60], Step [1/61], Loss: 0.9927\nEpoch [26/60], Step [2/61], Loss: 0.9614\nEpoch [26/60], Step [3/61], Loss: 0.9401\nEpoch [26/60], Step [4/61], Loss: 1.2670\nEpoch [26/60], Step [5/61], Loss: 0.8918\nEpoch [26/60], Step [6/61], Loss: 0.9192\nEpoch [26/60], Step [7/61], Loss: 1.0773\nEpoch [26/60], Step [8/61], Loss: 1.1997\nEpoch [26/60], Step [9/61], Loss: 0.9058\nEpoch [26/60], Step [10/61], Loss: 0.9112\nEpoch [26/60], Step [11/61], Loss: 0.9629\nEpoch [26/60], Step [12/61], Loss: 1.0524\nEpoch [26/60], Step [13/61], Loss: 0.9354\nEpoch [26/60], Step [14/61], Loss: 0.8954\nEpoch [26/60], Step [15/61], Loss: 1.0735\nEpoch [26/60], Step [16/61], Loss: 0.9524\nEpoch [26/60], Step [17/61], Loss: 0.9969\nEpoch [26/60], Step [18/61], Loss: 1.0472\nEpoch [26/60], Step [19/61], Loss: 0.9857\nEpoch [26/60], Step [20/61], Loss: 0.8741\nEpoch [26/60], Step [21/61], Loss: 1.1363\nEpoch [26/60], Step [22/61], Loss: 0.8911\nEpoch [26/60], Step [23/61], Loss: 0.9216\nEpoch [26/60], Step [24/61], Loss: 1.0008\nEpoch [26/60], Step [25/61], Loss: 0.8670\nEpoch [26/60], Step [26/61], Loss: 0.8929\nEpoch [26/60], Step [27/61], Loss: 0.8679\nEpoch [26/60], Step [28/61], Loss: 1.1557\nEpoch [26/60], Step [29/61], Loss: 0.8699\nEpoch [26/60], Step [30/61], Loss: 0.8785\nEpoch [26/60], Step [31/61], Loss: 0.9516\nEpoch [26/60], Step [32/61], Loss: 0.8907\nEpoch [26/60], Step [33/61], Loss: 1.1693\nEpoch [26/60], Step [34/61], Loss: 0.8794\nEpoch [26/60], Step [35/61], Loss: 0.8969\nEpoch [26/60], Step [36/61], Loss: 0.8563\nEpoch [26/60], Step [37/61], Loss: 0.9959\nEpoch [26/60], Step [38/61], Loss: 1.1873\nEpoch [26/60], Step [39/61], Loss: 0.8580\nEpoch [26/60], Step [40/61], Loss: 1.1389\nEpoch [26/60], Step [41/61], Loss: 0.8954\nEpoch [26/60], Step [42/61], Loss: 1.2044\nEpoch [26/60], Step [43/61], Loss: 0.8615\nEpoch [26/60], Step [44/61], Loss: 1.2356\nEpoch [26/60], Step [45/61], Loss: 0.9064\nEpoch [26/60], Step [46/61], Loss: 1.2655\nEpoch [26/60], Step [47/61], Loss: 1.1407\nEpoch [26/60], Step [48/61], Loss: 0.8727\nEpoch [26/60], Step [49/61], Loss: 0.8478\nEpoch [26/60], Step [50/61], Loss: 1.0255\nEpoch [26/60], Step [51/61], Loss: 0.9774\nEpoch [26/60], Step [52/61], Loss: 0.9719\nEpoch [26/60], Step [53/61], Loss: 1.0878\nEpoch [26/60], Step [54/61], Loss: 1.0816\nEpoch [26/60], Step [55/61], Loss: 0.8466\nEpoch [26/60], Step [56/61], Loss: 1.2506\nEpoch [26/60], Step [57/61], Loss: 1.0322\nEpoch [26/60], Step [58/61], Loss: 0.8771\nEpoch [26/60], Step [59/61], Loss: 0.8898\nEpoch [26/60], Step [60/61], Loss: 0.9255\nEpoch [26/60], Step [61/61], Loss: 1.0311\nTraining Loss: 0.9896\nTraining Accuracy: 0.9506\nValidation Accuracy: 0.8213\nPrecision: 0.8863, Recall: 0.8213\nEpoch [27/60], Step [1/61], Loss: 0.9027\nEpoch [27/60], Step [2/61], Loss: 0.8549\nEpoch [27/60], Step [3/61], Loss: 0.9494\nEpoch [27/60], Step [4/61], Loss: 1.1261\nEpoch [27/60], Step [5/61], Loss: 1.0730\nEpoch [27/60], Step [6/61], Loss: 1.0598\nEpoch [27/60], Step [7/61], Loss: 0.9691\nEpoch [27/60], Step [8/61], Loss: 1.2087\nEpoch [27/60], Step [9/61], Loss: 1.5430\nEpoch [27/60], Step [10/61], Loss: 0.9676\nEpoch [27/60], Step [11/61], Loss: 0.8670\nEpoch [27/60], Step [12/61], Loss: 1.1184\nEpoch [27/60], Step [13/61], Loss: 1.2159\nEpoch [27/60], Step [14/61], Loss: 0.9664\nEpoch [27/60], Step [15/61], Loss: 0.8864\nEpoch [27/60], Step [16/61], Loss: 0.9665\nEpoch [27/60], Step [17/61], Loss: 0.8816\nEpoch [27/60], Step [18/61], Loss: 0.9263\nEpoch [27/60], Step [19/61], Loss: 0.9178\nEpoch [27/60], Step [20/61], Loss: 1.0923\nEpoch [27/60], Step [21/61], Loss: 1.5116\nEpoch [27/60], Step [22/61], Loss: 0.8564\nEpoch [27/60], Step [23/61], Loss: 0.9112\nEpoch [27/60], Step [24/61], Loss: 0.8736\nEpoch [27/60], Step [25/61], Loss: 0.9447\nEpoch [27/60], Step [26/61], Loss: 1.0209\nEpoch [27/60], Step [27/61], Loss: 0.9355\nEpoch [27/60], Step [28/61], Loss: 1.3999\nEpoch [27/60], Step [29/61], Loss: 1.0382\nEpoch [27/60], Step [30/61], Loss: 1.1154\nEpoch [27/60], Step [31/61], Loss: 0.8595\nEpoch [27/60], Step [32/61], Loss: 0.8548\nEpoch [27/60], Step [33/61], Loss: 0.9343\nEpoch [27/60], Step [34/61], Loss: 0.8374\nEpoch [27/60], Step [35/61], Loss: 1.0271\nEpoch [27/60], Step [36/61], Loss: 1.0964\nEpoch [27/60], Step [37/61], Loss: 0.8636\nEpoch [27/60], Step [38/61], Loss: 1.0521\nEpoch [27/60], Step [39/61], Loss: 1.0260\nEpoch [27/60], Step [40/61], Loss: 0.8715\nEpoch [27/60], Step [41/61], Loss: 0.8658\nEpoch [27/60], Step [42/61], Loss: 1.0739\nEpoch [27/60], Step [43/61], Loss: 0.9303\nEpoch [27/60], Step [44/61], Loss: 0.8565\nEpoch [27/60], Step [45/61], Loss: 0.9546\nEpoch [27/60], Step [46/61], Loss: 0.9462\nEpoch [27/60], Step [47/61], Loss: 0.9079\nEpoch [27/60], Step [48/61], Loss: 0.8980\nEpoch [27/60], Step [49/61], Loss: 1.1838\nEpoch [27/60], Step [50/61], Loss: 1.1897\nEpoch [27/60], Step [51/61], Loss: 0.8570\nEpoch [27/60], Step [52/61], Loss: 1.2436\nEpoch [27/60], Step [53/61], Loss: 0.9645\nEpoch [27/60], Step [54/61], Loss: 0.9309\nEpoch [27/60], Step [55/61], Loss: 0.8669\nEpoch [27/60], Step [56/61], Loss: 1.2997\nEpoch [27/60], Step [57/61], Loss: 0.8413\nEpoch [27/60], Step [58/61], Loss: 0.8877\nEpoch [27/60], Step [59/61], Loss: 0.9280\nEpoch [27/60], Step [60/61], Loss: 1.0963\nEpoch [27/60], Step [61/61], Loss: 0.8848\nTraining Loss: 1.0027\nTraining Accuracy: 0.9351\nValidation Accuracy: 0.9033\nPrecision: 0.9096, Recall: 0.9033\nEpoch [28/60], Step [1/61], Loss: 0.8260\nEpoch [28/60], Step [2/61], Loss: 1.0102\nEpoch [28/60], Step [3/61], Loss: 0.9751\nEpoch [28/60], Step [4/61], Loss: 0.8468\nEpoch [28/60], Step [5/61], Loss: 1.0496\nEpoch [28/60], Step [6/61], Loss: 0.8594\nEpoch [28/60], Step [7/61], Loss: 1.0307\nEpoch [28/60], Step [8/61], Loss: 1.1233\nEpoch [28/60], Step [9/61], Loss: 0.8425\nEpoch [28/60], Step [10/61], Loss: 0.8662\nEpoch [28/60], Step [11/61], Loss: 0.8768\nEpoch [28/60], Step [12/61], Loss: 0.9466\nEpoch [28/60], Step [13/61], Loss: 0.8695\nEpoch [28/60], Step [14/61], Loss: 1.3601\nEpoch [28/60], Step [15/61], Loss: 0.8630\nEpoch [28/60], Step [16/61], Loss: 0.8448\nEpoch [28/60], Step [17/61], Loss: 0.8380\nEpoch [28/60], Step [18/61], Loss: 0.8768\nEpoch [28/60], Step [19/61], Loss: 0.9928\nEpoch [28/60], Step [20/61], Loss: 0.9247\nEpoch [28/60], Step [21/61], Loss: 0.9019\nEpoch [28/60], Step [22/61], Loss: 0.8293\nEpoch [28/60], Step [23/61], Loss: 1.1118\nEpoch [28/60], Step [24/61], Loss: 0.9051\nEpoch [28/60], Step [25/61], Loss: 0.9959\nEpoch [28/60], Step [26/61], Loss: 0.8611\nEpoch [28/60], Step [27/61], Loss: 0.8400\nEpoch [28/60], Step [28/61], Loss: 0.8754\nEpoch [28/60], Step [29/61], Loss: 0.8378\nEpoch [28/60], Step [30/61], Loss: 0.8112\nEpoch [28/60], Step [31/61], Loss: 0.9224\nEpoch [28/60], Step [32/61], Loss: 0.9203\nEpoch [28/60], Step [33/61], Loss: 0.9161\nEpoch [28/60], Step [34/61], Loss: 1.0545\nEpoch [28/60], Step [35/61], Loss: 1.0607\nEpoch [28/60], Step [36/61], Loss: 0.8805\nEpoch [28/60], Step [37/61], Loss: 0.8327\nEpoch [28/60], Step [38/61], Loss: 0.8464\nEpoch [28/60], Step [39/61], Loss: 0.8413\nEpoch [28/60], Step [40/61], Loss: 1.1393\nEpoch [28/60], Step [41/61], Loss: 0.8231\nEpoch [28/60], Step [42/61], Loss: 0.8118\nEpoch [28/60], Step [43/61], Loss: 0.9351\nEpoch [28/60], Step [44/61], Loss: 1.1564\nEpoch [28/60], Step [45/61], Loss: 0.9758\nEpoch [28/60], Step [46/61], Loss: 0.8701\nEpoch [28/60], Step [47/61], Loss: 0.8131\nEpoch [28/60], Step [48/61], Loss: 1.1797\nEpoch [28/60], Step [49/61], Loss: 0.8546\nEpoch [28/60], Step [50/61], Loss: 0.8641\nEpoch [28/60], Step [51/61], Loss: 0.8848\nEpoch [28/60], Step [52/61], Loss: 0.8156\nEpoch [28/60], Step [53/61], Loss: 0.8451\nEpoch [28/60], Step [54/61], Loss: 0.9659\nEpoch [28/60], Step [55/61], Loss: 0.9306\nEpoch [28/60], Step [56/61], Loss: 0.9910\nEpoch [28/60], Step [57/61], Loss: 1.0663\nEpoch [28/60], Step [58/61], Loss: 0.9453\nEpoch [28/60], Step [59/61], Loss: 0.9912\nEpoch [28/60], Step [60/61], Loss: 0.8348\nEpoch [28/60], Step [61/61], Loss: 0.8708\nTraining Loss: 0.9287\nTraining Accuracy: 0.9547\nValidation Accuracy: 0.8852\nPrecision: 0.9004, Recall: 0.8852\nEpoch [29/60], Step [1/61], Loss: 0.8890\nEpoch [29/60], Step [2/61], Loss: 0.8824\nEpoch [29/60], Step [3/61], Loss: 0.8269\nEpoch [29/60], Step [4/61], Loss: 0.8731\nEpoch [29/60], Step [5/61], Loss: 0.9238\nEpoch [29/60], Step [6/61], Loss: 0.8328\nEpoch [29/60], Step [7/61], Loss: 0.8049\nEpoch [29/60], Step [8/61], Loss: 0.8751\nEpoch [29/60], Step [9/61], Loss: 1.6633\nEpoch [29/60], Step [10/61], Loss: 0.8148\nEpoch [29/60], Step [11/61], Loss: 0.8077\nEpoch [29/60], Step [12/61], Loss: 1.1164\nEpoch [29/60], Step [13/61], Loss: 1.0700\nEpoch [29/60], Step [14/61], Loss: 0.8196\nEpoch [29/60], Step [15/61], Loss: 0.9369\nEpoch [29/60], Step [16/61], Loss: 0.9470\nEpoch [29/60], Step [17/61], Loss: 0.9103\nEpoch [29/60], Step [18/61], Loss: 0.9559\nEpoch [29/60], Step [19/61], Loss: 0.8293\nEpoch [29/60], Step [20/61], Loss: 1.1339\nEpoch [29/60], Step [21/61], Loss: 0.8357\nEpoch [29/60], Step [22/61], Loss: 0.8617\nEpoch [29/60], Step [23/61], Loss: 0.8201\nEpoch [29/60], Step [24/61], Loss: 0.7973\nEpoch [29/60], Step [25/61], Loss: 0.8476\nEpoch [29/60], Step [26/61], Loss: 0.7941\nEpoch [29/60], Step [27/61], Loss: 1.0417\nEpoch [29/60], Step [28/61], Loss: 0.9055\nEpoch [29/60], Step [29/61], Loss: 0.9706\nEpoch [29/60], Step [30/61], Loss: 1.2464\nEpoch [29/60], Step [31/61], Loss: 0.7983\nEpoch [29/60], Step [32/61], Loss: 1.1555\nEpoch [29/60], Step [33/61], Loss: 0.9454\nEpoch [29/60], Step [34/61], Loss: 0.8099\nEpoch [29/60], Step [35/61], Loss: 0.9155\nEpoch [29/60], Step [36/61], Loss: 0.8292\nEpoch [29/60], Step [37/61], Loss: 0.8185\nEpoch [29/60], Step [38/61], Loss: 0.8733\nEpoch [29/60], Step [39/61], Loss: 0.8547\nEpoch [29/60], Step [40/61], Loss: 0.8590\nEpoch [29/60], Step [41/61], Loss: 0.8448\nEpoch [29/60], Step [42/61], Loss: 1.0432\nEpoch [29/60], Step [43/61], Loss: 1.3177\nEpoch [29/60], Step [44/61], Loss: 0.8802\nEpoch [29/60], Step [45/61], Loss: 0.9189\nEpoch [29/60], Step [46/61], Loss: 0.8737\nEpoch [29/60], Step [47/61], Loss: 0.8145\nEpoch [29/60], Step [48/61], Loss: 0.8212\nEpoch [29/60], Step [49/61], Loss: 0.8876\nEpoch [29/60], Step [50/61], Loss: 1.1202\nEpoch [29/60], Step [51/61], Loss: 1.2744\nEpoch [29/60], Step [52/61], Loss: 1.3287\nEpoch [29/60], Step [53/61], Loss: 0.8752\nEpoch [29/60], Step [54/61], Loss: 1.0143\nEpoch [29/60], Step [55/61], Loss: 0.9837\nEpoch [29/60], Step [56/61], Loss: 0.8019\nEpoch [29/60], Step [57/61], Loss: 0.9005\nEpoch [29/60], Step [58/61], Loss: 0.8435\nEpoch [29/60], Step [59/61], Loss: 0.8716\nEpoch [29/60], Step [60/61], Loss: 1.5473\nEpoch [29/60], Step [61/61], Loss: 1.0248\nTraining Loss: 0.9485\nTraining Accuracy: 0.9495\nValidation Accuracy: 0.8213\nPrecision: 0.8791, Recall: 0.8213\nEpoch [30/60], Step [1/61], Loss: 1.0151\nEpoch [30/60], Step [2/61], Loss: 0.9256\nEpoch [30/60], Step [3/61], Loss: 0.9679\nEpoch [30/60], Step [4/61], Loss: 0.9128\nEpoch [30/60], Step [5/61], Loss: 1.0694\nEpoch [30/60], Step [6/61], Loss: 0.8886\nEpoch [30/60], Step [7/61], Loss: 0.8980\nEpoch [30/60], Step [8/61], Loss: 0.9297\nEpoch [30/60], Step [9/61], Loss: 0.8712\nEpoch [30/60], Step [10/61], Loss: 0.8569\nEpoch [30/60], Step [11/61], Loss: 0.8588\nEpoch [30/60], Step [12/61], Loss: 0.8757\nEpoch [30/60], Step [13/61], Loss: 0.8282\nEpoch [30/60], Step [14/61], Loss: 1.6219\nEpoch [30/60], Step [15/61], Loss: 0.7835\nEpoch [30/60], Step [16/61], Loss: 0.8008\nEpoch [30/60], Step [17/61], Loss: 0.8819\nEpoch [30/60], Step [18/61], Loss: 0.8065\nEpoch [30/60], Step [19/61], Loss: 0.8698\nEpoch [30/60], Step [20/61], Loss: 1.3119\nEpoch [30/60], Step [21/61], Loss: 0.9766\nEpoch [30/60], Step [22/61], Loss: 0.9127\nEpoch [30/60], Step [23/61], Loss: 0.9106\nEpoch [30/60], Step [24/61], Loss: 0.9339\nEpoch [30/60], Step [25/61], Loss: 0.8992\nEpoch [30/60], Step [26/61], Loss: 0.8026\nEpoch [30/60], Step [27/61], Loss: 1.0886\nEpoch [30/60], Step [28/61], Loss: 0.8289\nEpoch [30/60], Step [29/61], Loss: 0.8514\nEpoch [30/60], Step [30/61], Loss: 1.1370\nEpoch [30/60], Step [31/61], Loss: 0.9254\nEpoch [30/60], Step [32/61], Loss: 1.0034\nEpoch [30/60], Step [33/61], Loss: 0.8686\nEpoch [30/60], Step [34/61], Loss: 0.8194\nEpoch [30/60], Step [35/61], Loss: 0.8036\nEpoch [30/60], Step [36/61], Loss: 0.7909\nEpoch [30/60], Step [37/61], Loss: 1.0408\nEpoch [30/60], Step [38/61], Loss: 0.9178\nEpoch [30/60], Step [39/61], Loss: 1.1992\nEpoch [30/60], Step [40/61], Loss: 0.8518\nEpoch [30/60], Step [41/61], Loss: 0.7871\nEpoch [30/60], Step [42/61], Loss: 0.8678\nEpoch [30/60], Step [43/61], Loss: 0.7821\nEpoch [30/60], Step [44/61], Loss: 0.8308\nEpoch [30/60], Step [45/61], Loss: 0.9518\nEpoch [30/60], Step [46/61], Loss: 0.7858\nEpoch [30/60], Step [47/61], Loss: 1.3216\nEpoch [30/60], Step [48/61], Loss: 0.9949\nEpoch [30/60], Step [49/61], Loss: 0.8473\nEpoch [30/60], Step [50/61], Loss: 0.7975\nEpoch [30/60], Step [51/61], Loss: 0.7750\nEpoch [30/60], Step [52/61], Loss: 1.1297\nEpoch [30/60], Step [53/61], Loss: 1.0352\nEpoch [30/60], Step [54/61], Loss: 0.8431\nEpoch [30/60], Step [55/61], Loss: 0.8303\nEpoch [30/60], Step [56/61], Loss: 1.0617\nEpoch [30/60], Step [57/61], Loss: 0.9160\nEpoch [30/60], Step [58/61], Loss: 0.8206\nEpoch [30/60], Step [59/61], Loss: 0.8082\nEpoch [30/60], Step [60/61], Loss: 0.7782\nEpoch [30/60], Step [61/61], Loss: 0.7855\nTraining Loss: 0.9234\nTraining Accuracy: 0.9495\nValidation Accuracy: 0.8574\nPrecision: 0.8791, Recall: 0.8574\nEpoch [31/60], Step [1/61], Loss: 0.8422\nEpoch [31/60], Step [2/61], Loss: 1.0512\nEpoch [31/60], Step [3/61], Loss: 0.8393\nEpoch [31/60], Step [4/61], Loss: 0.8836\nEpoch [31/60], Step [5/61], Loss: 0.7770\nEpoch [31/60], Step [6/61], Loss: 0.9335\nEpoch [31/60], Step [7/61], Loss: 0.7807\nEpoch [31/60], Step [8/61], Loss: 0.8733\nEpoch [31/60], Step [9/61], Loss: 0.9038\nEpoch [31/60], Step [10/61], Loss: 0.7872\nEpoch [31/60], Step [11/61], Loss: 0.7783\nEpoch [31/60], Step [12/61], Loss: 0.7803\nEpoch [31/60], Step [13/61], Loss: 0.7836\nEpoch [31/60], Step [14/61], Loss: 0.8445\nEpoch [31/60], Step [15/61], Loss: 0.8431\nEpoch [31/60], Step [16/61], Loss: 0.7939\nEpoch [31/60], Step [17/61], Loss: 0.8051\nEpoch [31/60], Step [18/61], Loss: 0.9243\nEpoch [31/60], Step [19/61], Loss: 0.7564\nEpoch [31/60], Step [20/61], Loss: 0.7945\nEpoch [31/60], Step [21/61], Loss: 0.7750\nEpoch [31/60], Step [22/61], Loss: 0.7647\nEpoch [31/60], Step [23/61], Loss: 0.7679\nEpoch [31/60], Step [24/61], Loss: 0.7572\nEpoch [31/60], Step [25/61], Loss: 0.7663\nEpoch [31/60], Step [26/61], Loss: 0.8655\nEpoch [31/60], Step [27/61], Loss: 0.8234\nEpoch [31/60], Step [28/61], Loss: 0.7755\nEpoch [31/60], Step [29/61], Loss: 0.7906\nEpoch [31/60], Step [30/61], Loss: 0.7781\nEpoch [31/60], Step [31/61], Loss: 0.8371\nEpoch [31/60], Step [32/61], Loss: 0.7791\nEpoch [31/60], Step [33/61], Loss: 1.0229\nEpoch [31/60], Step [34/61], Loss: 1.0651\nEpoch [31/60], Step [35/61], Loss: 0.8715\nEpoch [31/60], Step [36/61], Loss: 0.7665\nEpoch [31/60], Step [37/61], Loss: 0.7773\nEpoch [31/60], Step [38/61], Loss: 0.7856\nEpoch [31/60], Step [39/61], Loss: 0.8289\nEpoch [31/60], Step [40/61], Loss: 0.7590\nEpoch [31/60], Step [41/61], Loss: 0.8703\nEpoch [31/60], Step [42/61], Loss: 0.7747\nEpoch [31/60], Step [43/61], Loss: 0.9561\nEpoch [31/60], Step [44/61], Loss: 0.8901\nEpoch [31/60], Step [45/61], Loss: 0.7590\nEpoch [31/60], Step [46/61], Loss: 0.8067\nEpoch [31/60], Step [47/61], Loss: 0.7869\nEpoch [31/60], Step [48/61], Loss: 0.8013\nEpoch [31/60], Step [49/61], Loss: 0.7727\nEpoch [31/60], Step [50/61], Loss: 0.7468\nEpoch [31/60], Step [51/61], Loss: 0.7531\nEpoch [31/60], Step [52/61], Loss: 0.7664\nEpoch [31/60], Step [53/61], Loss: 0.7394\nEpoch [31/60], Step [54/61], Loss: 0.7836\nEpoch [31/60], Step [55/61], Loss: 1.3413\nEpoch [31/60], Step [56/61], Loss: 0.8167\nEpoch [31/60], Step [57/61], Loss: 0.7577\nEpoch [31/60], Step [58/61], Loss: 0.8391\nEpoch [31/60], Step [59/61], Loss: 0.7461\nEpoch [31/60], Step [60/61], Loss: 0.7636\nEpoch [31/60], Step [61/61], Loss: 1.0667\nTraining Loss: 0.8295\nTraining Accuracy: 0.9763\nValidation Accuracy: 0.8705\nPrecision: 0.8836, Recall: 0.8705\nEpoch [32/60], Step [1/61], Loss: 0.7402\nEpoch [32/60], Step [2/61], Loss: 0.8452\nEpoch [32/60], Step [3/61], Loss: 0.7631\nEpoch [32/60], Step [4/61], Loss: 0.7467\nEpoch [32/60], Step [5/61], Loss: 0.7599\nEpoch [32/60], Step [6/61], Loss: 0.7504\nEpoch [32/60], Step [7/61], Loss: 0.7663\nEpoch [32/60], Step [8/61], Loss: 0.8034\nEpoch [32/60], Step [9/61], Loss: 0.7599\nEpoch [32/60], Step [10/61], Loss: 0.7697\nEpoch [32/60], Step [11/61], Loss: 0.7461\nEpoch [32/60], Step [12/61], Loss: 0.7466\nEpoch [32/60], Step [13/61], Loss: 0.7486\nEpoch [32/60], Step [14/61], Loss: 0.9018\nEpoch [32/60], Step [15/61], Loss: 0.7701\nEpoch [32/60], Step [16/61], Loss: 0.7365\nEpoch [32/60], Step [17/61], Loss: 0.7358\nEpoch [32/60], Step [18/61], Loss: 1.1386\nEpoch [32/60], Step [19/61], Loss: 0.8190\nEpoch [32/60], Step [20/61], Loss: 1.2454\nEpoch [32/60], Step [21/61], Loss: 0.7846\nEpoch [32/60], Step [22/61], Loss: 0.7307\nEpoch [32/60], Step [23/61], Loss: 0.9615\nEpoch [32/60], Step [24/61], Loss: 0.8469\nEpoch [32/60], Step [25/61], Loss: 0.7710\nEpoch [32/60], Step [26/61], Loss: 0.7405\nEpoch [32/60], Step [27/61], Loss: 0.7382\nEpoch [32/60], Step [28/61], Loss: 0.7753\nEpoch [32/60], Step [29/61], Loss: 0.8103\nEpoch [32/60], Step [30/61], Loss: 1.0055\nEpoch [32/60], Step [31/61], Loss: 0.7673\nEpoch [32/60], Step [32/61], Loss: 0.7442\nEpoch [32/60], Step [33/61], Loss: 0.7287\nEpoch [32/60], Step [34/61], Loss: 0.9467\nEpoch [32/60], Step [35/61], Loss: 0.8802\nEpoch [32/60], Step [36/61], Loss: 1.0871\nEpoch [32/60], Step [37/61], Loss: 0.7278\nEpoch [32/60], Step [38/61], Loss: 1.1952\nEpoch [32/60], Step [39/61], Loss: 0.8491\nEpoch [32/60], Step [40/61], Loss: 0.7323\nEpoch [32/60], Step [41/61], Loss: 0.7330\nEpoch [32/60], Step [42/61], Loss: 1.0184\nEpoch [32/60], Step [43/61], Loss: 0.7419\nEpoch [32/60], Step [44/61], Loss: 0.7769\nEpoch [32/60], Step [45/61], Loss: 0.9026\nEpoch [32/60], Step [46/61], Loss: 0.8450\nEpoch [32/60], Step [47/61], Loss: 0.7376\nEpoch [32/60], Step [48/61], Loss: 0.9563\nEpoch [32/60], Step [49/61], Loss: 0.7579\nEpoch [32/60], Step [50/61], Loss: 0.8249\nEpoch [32/60], Step [51/61], Loss: 0.7625\nEpoch [32/60], Step [52/61], Loss: 0.8949\nEpoch [32/60], Step [53/61], Loss: 0.7488\nEpoch [32/60], Step [54/61], Loss: 1.1019\nEpoch [32/60], Step [55/61], Loss: 0.7325\nEpoch [32/60], Step [56/61], Loss: 0.7871\nEpoch [32/60], Step [57/61], Loss: 0.9057\nEpoch [32/60], Step [58/61], Loss: 0.7234\nEpoch [32/60], Step [59/61], Loss: 0.7804\nEpoch [32/60], Step [60/61], Loss: 0.7252\nEpoch [32/60], Step [61/61], Loss: 0.8023\nTraining Loss: 0.8260\nTraining Accuracy: 0.9650\nValidation Accuracy: 0.8984\nPrecision: 0.9097, Recall: 0.8984\nEpoch [33/60], Step [1/61], Loss: 0.7678\nEpoch [33/60], Step [2/61], Loss: 0.8172\nEpoch [33/60], Step [3/61], Loss: 0.8580\nEpoch [33/60], Step [4/61], Loss: 0.7721\nEpoch [33/60], Step [5/61], Loss: 0.7283\nEpoch [33/60], Step [6/61], Loss: 1.2661\nEpoch [33/60], Step [7/61], Loss: 0.7214\nEpoch [33/60], Step [8/61], Loss: 0.7578\nEpoch [33/60], Step [9/61], Loss: 0.8304\nEpoch [33/60], Step [10/61], Loss: 0.8858\nEpoch [33/60], Step [11/61], Loss: 1.0187\nEpoch [33/60], Step [12/61], Loss: 0.7271\nEpoch [33/60], Step [13/61], Loss: 0.7703\nEpoch [33/60], Step [14/61], Loss: 0.7845\nEpoch [33/60], Step [15/61], Loss: 0.8580\nEpoch [33/60], Step [16/61], Loss: 0.7576\nEpoch [33/60], Step [17/61], Loss: 0.7845\nEpoch [33/60], Step [18/61], Loss: 0.8030\nEpoch [33/60], Step [19/61], Loss: 0.7592\nEpoch [33/60], Step [20/61], Loss: 0.8124\nEpoch [33/60], Step [21/61], Loss: 0.8035\nEpoch [33/60], Step [22/61], Loss: 0.7526\nEpoch [33/60], Step [23/61], Loss: 0.8010\nEpoch [33/60], Step [24/61], Loss: 0.7385\nEpoch [33/60], Step [25/61], Loss: 0.7212\nEpoch [33/60], Step [26/61], Loss: 0.7192\nEpoch [33/60], Step [27/61], Loss: 0.7269\nEpoch [33/60], Step [28/61], Loss: 0.7235\nEpoch [33/60], Step [29/61], Loss: 0.7231\nEpoch [33/60], Step [30/61], Loss: 0.7500\nEpoch [33/60], Step [31/61], Loss: 0.7248\nEpoch [33/60], Step [32/61], Loss: 0.7779\nEpoch [33/60], Step [33/61], Loss: 0.8640\nEpoch [33/60], Step [34/61], Loss: 0.8651\nEpoch [33/60], Step [35/61], Loss: 0.7986\nEpoch [33/60], Step [36/61], Loss: 0.8210\nEpoch [33/60], Step [37/61], Loss: 0.8571\nEpoch [33/60], Step [38/61], Loss: 0.7132\nEpoch [33/60], Step [39/61], Loss: 0.7391\nEpoch [33/60], Step [40/61], Loss: 0.7049\nEpoch [33/60], Step [41/61], Loss: 0.8147\nEpoch [33/60], Step [42/61], Loss: 0.8307\nEpoch [33/60], Step [43/61], Loss: 0.8244\nEpoch [33/60], Step [44/61], Loss: 0.7520\nEpoch [33/60], Step [45/61], Loss: 0.8141\nEpoch [33/60], Step [46/61], Loss: 0.7636\nEpoch [33/60], Step [47/61], Loss: 0.7137\nEpoch [33/60], Step [48/61], Loss: 1.3194\nEpoch [33/60], Step [49/61], Loss: 0.7441\nEpoch [33/60], Step [50/61], Loss: 0.8974\nEpoch [33/60], Step [51/61], Loss: 0.9190\nEpoch [33/60], Step [52/61], Loss: 0.7136\nEpoch [33/60], Step [53/61], Loss: 1.2855\nEpoch [33/60], Step [54/61], Loss: 0.8349\nEpoch [33/60], Step [55/61], Loss: 0.8251\nEpoch [33/60], Step [56/61], Loss: 0.8964\nEpoch [33/60], Step [57/61], Loss: 0.8340\nEpoch [33/60], Step [58/61], Loss: 0.7955\nEpoch [33/60], Step [59/61], Loss: 0.8210\nEpoch [33/60], Step [60/61], Loss: 0.7883\nEpoch [33/60], Step [61/61], Loss: 0.7029\nTraining Loss: 0.8153\nTraining Accuracy: 0.9619\nValidation Accuracy: 0.8820\nPrecision: 0.8894, Recall: 0.8820\nEpoch [34/60], Step [1/61], Loss: 0.8815\nEpoch [34/60], Step [2/61], Loss: 0.7186\nEpoch [34/60], Step [3/61], Loss: 0.7964\nEpoch [34/60], Step [4/61], Loss: 0.7128\nEpoch [34/60], Step [5/61], Loss: 0.8040\nEpoch [34/60], Step [6/61], Loss: 0.7527\nEpoch [34/60], Step [7/61], Loss: 0.8049\nEpoch [34/60], Step [8/61], Loss: 1.0166\nEpoch [34/60], Step [9/61], Loss: 0.7183\nEpoch [34/60], Step [10/61], Loss: 0.7997\nEpoch [34/60], Step [11/61], Loss: 0.7521\nEpoch [34/60], Step [12/61], Loss: 0.7264\nEpoch [34/60], Step [13/61], Loss: 0.7148\nEpoch [34/60], Step [14/61], Loss: 0.7059\nEpoch [34/60], Step [15/61], Loss: 1.1043\nEpoch [34/60], Step [16/61], Loss: 0.7110\nEpoch [34/60], Step [17/61], Loss: 0.7234\nEpoch [34/60], Step [18/61], Loss: 0.7675\nEpoch [34/60], Step [19/61], Loss: 0.7093\nEpoch [34/60], Step [20/61], Loss: 0.7864\nEpoch [34/60], Step [21/61], Loss: 0.7103\nEpoch [34/60], Step [22/61], Loss: 0.8721\nEpoch [34/60], Step [23/61], Loss: 0.7401\nEpoch [34/60], Step [24/61], Loss: 0.8574\nEpoch [34/60], Step [25/61], Loss: 0.7036\nEpoch [34/60], Step [26/61], Loss: 0.7203\nEpoch [34/60], Step [27/61], Loss: 0.7193\nEpoch [34/60], Step [28/61], Loss: 0.7100\nEpoch [34/60], Step [29/61], Loss: 0.7071\nEpoch [34/60], Step [30/61], Loss: 0.7155\nEpoch [34/60], Step [31/61], Loss: 0.7780\nEpoch [34/60], Step [32/61], Loss: 0.7544\nEpoch [34/60], Step [33/61], Loss: 0.9875\nEpoch [34/60], Step [34/61], Loss: 0.6997\nEpoch [34/60], Step [35/61], Loss: 0.7021\nEpoch [34/60], Step [36/61], Loss: 0.8390\nEpoch [34/60], Step [37/61], Loss: 0.6997\nEpoch [34/60], Step [38/61], Loss: 0.9408\nEpoch [34/60], Step [39/61], Loss: 0.7558\nEpoch [34/60], Step [40/61], Loss: 1.2428\nEpoch [34/60], Step [41/61], Loss: 0.7400\nEpoch [34/60], Step [42/61], Loss: 0.7109\nEpoch [34/60], Step [43/61], Loss: 0.8447\nEpoch [34/60], Step [44/61], Loss: 0.6996\nEpoch [34/60], Step [45/61], Loss: 0.6979\nEpoch [34/60], Step [46/61], Loss: 0.7677\nEpoch [34/60], Step [47/61], Loss: 1.0242\nEpoch [34/60], Step [48/61], Loss: 0.7253\nEpoch [34/60], Step [49/61], Loss: 1.7814\nEpoch [34/60], Step [50/61], Loss: 0.7096\nEpoch [34/60], Step [51/61], Loss: 0.9579\nEpoch [34/60], Step [52/61], Loss: 0.7593\nEpoch [34/60], Step [53/61], Loss: 0.7504\nEpoch [34/60], Step [54/61], Loss: 0.7993\nEpoch [34/60], Step [55/61], Loss: 0.8144\nEpoch [34/60], Step [56/61], Loss: 0.6971\nEpoch [34/60], Step [57/61], Loss: 1.0650\nEpoch [34/60], Step [58/61], Loss: 0.7166\nEpoch [34/60], Step [59/61], Loss: 0.7321\nEpoch [34/60], Step [60/61], Loss: 0.7110\nEpoch [34/60], Step [61/61], Loss: 0.8031\nTraining Loss: 0.8044\nTraining Accuracy: 0.9640\nValidation Accuracy: 0.8738\nPrecision: 0.8809, Recall: 0.8738\nEpoch [35/60], Step [1/61], Loss: 0.7030\nEpoch [35/60], Step [2/61], Loss: 0.7712\nEpoch [35/60], Step [3/61], Loss: 0.7395\nEpoch [35/60], Step [4/61], Loss: 1.0974\nEpoch [35/60], Step [5/61], Loss: 0.8652\nEpoch [35/60], Step [6/61], Loss: 0.7484\nEpoch [35/60], Step [7/61], Loss: 0.7247\nEpoch [35/60], Step [8/61], Loss: 0.7949\nEpoch [35/60], Step [9/61], Loss: 0.7312\nEpoch [35/60], Step [10/61], Loss: 0.8001\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 34\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}